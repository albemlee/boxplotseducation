{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gnWCGPLQ5ePG"
   },
   "source": [
    "# Setup Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8476,
     "status": "ok",
     "timestamp": 1535404790731,
     "user": {
      "displayName": "Albert Lee",
      "photoUrl": "//lh3.googleusercontent.com/-f0u8obCCk_w/AAAAAAAAAAI/AAAAAAAAAxs/83E22s4mtyM/s50-c-k-no/photo.jpg",
      "userId": "109969660738913262150"
     },
     "user_tz": 240
    },
    "id": "Nrg9OIDy5ePJ",
    "outputId": "311b66c1-cd20-4625-e2a9-65c25f1005eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# To load data\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For text processing\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import Imputer\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# To build model\n",
    "import keras\n",
    "from keras.layers import Dense, concatenate, Input, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "# To train model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# To evaluate model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# To track time elapsed\n",
    "import time\n",
    "\n",
    "# To save results\n",
    "import dill\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uPLEdpRv5ePM"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YF3nUD615ePM"
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Return pandas dataframe data_train: training data (features + labels)\n",
    "    Return pandas dataframe data_test: test data (only features)\n",
    "    \n",
    "    Required Libraries: zipfile, pandas\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load zipped folder with data files\n",
    "    resource_archive = zipfile.ZipFile('resources.zip', 'r')\n",
    "\n",
    "    # Load testing data\n",
    "    data_test = pd.read_csv(resource_archive.open('TestData.csv'), \n",
    "                            dtype={\n",
    "                                'Object_Description': str, \n",
    "                                'Program_Description': str, \n",
    "                                'SubFund_Description': str, \n",
    "                                'Job_Title_Description': str, \n",
    "                                'Facility_or_Department': str,\n",
    "                                'Sub_Object_Description': str, \n",
    "                                'Location_Description': str, \n",
    "                                'FTE': float,\n",
    "                                'Function_Description': str, \n",
    "                                'Position_Extra': str, \n",
    "                                'Text_4': str, \n",
    "                                'Total': float, \n",
    "                                'Text_2': str,\n",
    "                                'Text_3': str, \n",
    "                                'Fund_Description': str, \n",
    "                                'Text_1': str\n",
    "                            },\n",
    "                            index_col=0)\n",
    "\n",
    "    # Load training data\n",
    "    data_train = pd.read_csv(resource_archive.open('TrainingData.csv'), \n",
    "                            dtype={\n",
    "                                'Object_Description': str, \n",
    "                                'Program_Description': str, \n",
    "                                'SubFund_Description': str, \n",
    "                                'Job_Title_Description': str, \n",
    "                                'Facility_or_Department': str,\n",
    "                                'Sub_Object_Description': str, \n",
    "                                'Location_Description': str, \n",
    "                                'FTE': float,\n",
    "                                'Function_Description': str, \n",
    "                                'Position_Extra': str, \n",
    "                                'Text_4': str, \n",
    "                                'Total': float, \n",
    "                                'Text_2': str,\n",
    "                                'Text_3': str, \n",
    "                                'Fund_Description': str, \n",
    "                                'Text_1': str,\n",
    "                                'Function': 'category',\n",
    "                                'Object_Type': 'category',\n",
    "                                'Operating_Status': 'category',\n",
    "                                'Position_Type': 'category',\n",
    "                                'Pre_K': 'category',\n",
    "                                'Reporting': 'category',\n",
    "                                'Sharing': 'category',\n",
    "                                'Student_Type': 'category',\n",
    "                                'Use': 'category',\n",
    "                            },\n",
    "                             index_col=0)\n",
    "    \n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2859,
     "status": "ok",
     "timestamp": 1535404794111,
     "user": {
      "displayName": "Albert Lee",
      "photoUrl": "//lh3.googleusercontent.com/-f0u8obCCk_w/AAAAAAAAAAI/AAAAAAAAAxs/83E22s4mtyM/s50-c-k-no/photo.jpg",
      "userId": "109969660738913262150"
     },
     "user_tz": 240
    },
    "id": "GVz1rkaX5ePP",
    "outputId": "e9ff06e4-756d-4d1a-b0bb-6d9c404403a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_train shape: (400277, 25)\n",
      "data_test shape: (50064, 16)\n"
     ]
    }
   ],
   "source": [
    "data_train, data_test = load_data()\n",
    "print('data_train shape:', data_train.shape)\n",
    "print('data_test shape:', data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i203UxGy5ePT"
   },
   "outputs": [],
   "source": [
    "def load_features(data_train, data_test):\n",
    "    \"\"\"\n",
    "    Return pandas dataframe data_features: data in feature columns of data_train and data_test\n",
    "    \n",
    "    Param pandas dataframe data_train: training data (features + labels)\n",
    "    Param pandas dataframe data_test: test data (only features)\n",
    "    \n",
    "    Required Libraries: pandas\n",
    "    \"\"\"\n",
    "    \n",
    "    feature_columns = data_test.columns # data_test only contains features\n",
    "    \n",
    "    data_features = pd.concat([data_train[feature_columns], data_test])\n",
    "    \n",
    "    return data_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 458,
     "status": "ok",
     "timestamp": 1535404795089,
     "user": {
      "displayName": "Albert Lee",
      "photoUrl": "//lh3.googleusercontent.com/-f0u8obCCk_w/AAAAAAAAAAI/AAAAAAAAAxs/83E22s4mtyM/s50-c-k-no/photo.jpg",
      "userId": "109969660738913262150"
     },
     "user_tz": 240
    },
    "id": "vBZekpVd5ePV",
    "outputId": "efc0f579-b14f-4bd3-a946-55a0d6d0386f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_features shape: (450341, 16)\n"
     ]
    }
   ],
   "source": [
    "# Load Features\n",
    "data_features = load_features(data_train, data_test)\n",
    "print('data_features shape:', data_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ITNF9275ePY"
   },
   "source": [
    "# Prepare Data for Classification\n",
    "Run only if preprocessed data files are not available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ID0Uo8GT5ePY"
   },
   "outputs": [],
   "source": [
    "def text_processing(phrase):\n",
    "    \"\"\"\n",
    "    Return list processed_phrase: phrase tokens after processing has been completed\n",
    "    \n",
    "    param string phrase: phrase to be processed\n",
    "    \n",
    "    Required Libraries: re, nltk\n",
    "    \"\"\"\n",
    "    \n",
    "    # Case Normalization\n",
    "    processed_phrase = phrase.lower()\n",
    "    \n",
    "    # Remove Punctuations\n",
    "    processed_phrase = re.sub(r\"[^a-z0-9-]\", \" \", processed_phrase)\n",
    "    \n",
    "    # Tokenize Phrase\n",
    "    processed_phrase = processed_phrase.split()\n",
    "    \n",
    "    # Remove Stopwords\n",
    "    processed_phrase = [word for word in processed_phrase if word not in stopwords.words(\"english\") and word != '-']\n",
    "    \n",
    "    # Lemmatization\n",
    "    processed_phrase = [WordNetLemmatizer().lemmatize(word) for word in processed_phrase]\n",
    "    \n",
    "    # Recombine list into phrase\n",
    "    processed_phrase = ' '.join(processed_phrase)\n",
    "    \n",
    "    return processed_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_ZapkVld5ePb",
    "outputId": "8bf67ef2-f47a-4d30-84cf-7bb823a5cd59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "past patiently waitin passionately smashin every expectation every action act creation laughin face casualty sorrow first time thinkin past tomorrow\n",
      "0.033808231353759766\n"
     ]
    }
   ],
   "source": [
    "# test text_processing function (with quote from Hamilton: The Musical)\n",
    "start_time = time.time()\n",
    "print(text_processing(\n",
    "    \"I’m past patiently waitin’. I’m passionately smashin’ every expectation. \" + \n",
    "    \"Every action’s an act of creation! \" +\n",
    "    \"I’m laughin’ in the face of casualties and sorrow. \" +\n",
    "    \"For the first time, I’m thinkin’ past tomorrow\"))\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZFQgJuJ_5ePd"
   },
   "outputs": [],
   "source": [
    "def init_prep(data_train, data_test, data_features, label=None):\n",
    "    \"\"\"\n",
    "    Return numpy array X_numeric: numerical feature matrix of test set\n",
    "    Return numpy array X_text: text feature matrix for classification model fitting\n",
    "    Return numpy array X_numeric_test: numerical feature matrix of test set\n",
    "    Return numpy array X_text_test: text feature matrix for classification model fitting\n",
    "    Return numpy array y: labels matrix for classification model fitting\n",
    "    Return keras.Tokenizer() tokenize: contains word to token mapping\n",
    "    \n",
    "    Param pandas dataframe data_train: training data (features + labels)\n",
    "    Param pandas dataframe data_test: test data (features)\n",
    "    Param pandas dataframe data_features: data in feature columns of data_train and data_test\n",
    "    \n",
    "    Required Libraries: pandas, numpy, keras\n",
    "    Required helper functions: text_processing\n",
    "    \"\"\"\n",
    "    \n",
    "    # Combined and preprocess text columns\n",
    "    data_train['combined_text'] = (data_train[data_features.columns]\n",
    "                                       .drop(columns=['FTE', 'Total'])\n",
    "                                       .fillna(\"\")\n",
    "                                       .apply(lambda x: \" \".join(x), axis=1)\n",
    "                                       .apply(lambda x: text_processing(x))\n",
    "                                  )\n",
    "    data_test['combined_text'] = (data_test[data_features.columns]\n",
    "                                       .drop(columns=['FTE', 'Total'])\n",
    "                                       .fillna(\"\")\n",
    "                                       .apply(lambda x: \" \".join(x), axis=1)\n",
    "                                       .apply(lambda x: text_processing(x))\n",
    "                                 )\n",
    "    data_features['combined_text'] = (data_features\n",
    "                                          .drop(columns=['FTE', 'Total'])\n",
    "                                          .fillna(\"\")\n",
    "                                          .apply(lambda x: \" \".join(x), axis=1)\n",
    "                                          .apply(lambda x: text_processing(x))\n",
    "                                     )\n",
    "    \n",
    "    # Vectorizer text columns in training data\n",
    "    tokenize = Tokenizer()\n",
    "    tokenize.fit_on_texts(data_features['combined_text'])\n",
    "    \n",
    "    X_text = tokenize.texts_to_matrix(data_train['combined_text'])\n",
    "    X_text_test = tokenize.texts_to_matrix(data_test['combined_text'])\n",
    "\n",
    "    # Create new features based on whether fte is missing and whether total is missing\n",
    "    total_not_missing = pd.isnull(data_train['Total']).astype(int).values.reshape(-1, 1) # training data\n",
    "    fte_not_missing = pd.isnull(data_train['FTE']).astype(int).values.reshape(-1, 1) # training data\n",
    "    total_not_missing_test = pd.isnull(data_test['Total']).astype(int).values.reshape(-1, 1) # test data\n",
    "    fte_not_missing_test = pd.isnull(data_test['FTE']).astype(int).values.reshape(-1, 1) # test data\n",
    "    \n",
    "    # Impute total with median total value\n",
    "    imp_total = Imputer(strategy='median')\n",
    "    imp_total.fit(data_features['Total'].values.reshape(-1, 1))\n",
    "    total = imp_total.transform(data_train['Total'].values.reshape(-1, 1)) # training data\n",
    "    total_test = imp_total.transform(data_test['Total'].values.reshape(-1, 1)) # test data\n",
    "    \n",
    "    # Replace missing fte values with 0\n",
    "    fte = data_train['FTE'].fillna('0').values.reshape(-1, 1) # training data\n",
    "    fte_test = data_test['FTE'].fillna('0').values.reshape(-1, 1) # test data\n",
    "    \n",
    "    # Create feature matrix\n",
    "    X_numeric = np.concatenate([total, total_not_missing, fte, fte_not_missing], axis=1) # train\n",
    "    X_numeric_test = np.concatenate([total_test, total_not_missing_test, fte_test, fte_not_missing_test], axis=1) # test\n",
    "    \n",
    "    # Create labels matrix\n",
    "    if label:\n",
    "        y = pd.get_dummies(data_train[label]).values.astype('float64')\n",
    "    else:\n",
    "        label = ['Function',\n",
    "                 'Object_Type',\n",
    "                 'Operating_Status',\n",
    "                 'Position_Type',\n",
    "                 'Pre_K',\n",
    "                 'Reporting',\n",
    "                 'Sharing',\n",
    "                 'Student_Type',\n",
    "                 'Use']\n",
    "        y = pd.get_dummies(data_train[label]).values.astype('float64')\n",
    "    \n",
    "    return X_numeric, X_text, X_numeric_test, X_text_test, y, tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6IISwk-c5ePf"
   },
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "X_numeric, X_text, X_test_numeric, X_test_text, y, tokenize = init_prep(data_train, data_test, data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YeEwXBk75ePi"
   },
   "outputs": [],
   "source": [
    "# Convert X_test_text to sparse matrix\n",
    "X_test_text = sparse.csr_matrix(X_test_text)\n",
    "\n",
    "# Save preprocesses test data\n",
    "sparse.save_npz('X_test_text.npz', X_test_text)\n",
    "np.savez('X_test_numeric.npz', X_test_numeric)\n",
    "\n",
    "# Pickle tokenize\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenize, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g3wYrZqv5ePp"
   },
   "outputs": [],
   "source": [
    "def prep_for_classification(X_numeric, X_text, y, validation_size=50064):\n",
    "    \"\"\"\n",
    "    Split training data into training and validation sets\n",
    "    \n",
    "    Return pandas dataframe X_train_numeric: training data (features)\n",
    "    Return pandas dataframe X_train_text: training data (features)\n",
    "    Return pandas dataframe X_val_numeric: validation data (features)\n",
    "    Return pandas dataframe X_val_text: validation data (features)\n",
    "    Return pandas dataframe y_train: training data (labels)\n",
    "    Return pandas dataframe y_val: validation data (labels)\n",
    "    \n",
    "    param numpy array X_numeric: numerical feature matrix of test set\n",
    "    param numpy array X_text: text feature matrix for classification model fitting\n",
    "    param numpy array X_numeric_test: numerical feature matrix of test set\n",
    "    param numpy array X_text_test: text feature matrix for classification model fitting\n",
    "    param numpy array y: labels matrix for classification model fitting\n",
    "    \n",
    "    Required Libraries: pandas, sklearn.model_selection\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split into training and validation sets\n",
    "    X_train_numeric, X_val_numeric, X_train_text, X_val_text, y_train, y_val = train_test_split(X_numeric, X_text, y, test_size=validation_size, random_state=93)\n",
    "    \n",
    "    return X_train_numeric, X_val_numeric, X_train_text, X_val_text, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "drbaQuel5ePr",
    "outputId": "ee0f9a4d-d079-4e19-9a29-2cdede5e9b82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_numeric (350213, 4)\n",
      "X_train_text (350213, 3804)\n",
      "y_train (350213, 104)\n",
      "X_val_numeric (50064, 4)\n",
      "X_val_text (50064, 3804)\n",
      "y_val (50064, 104)\n"
     ]
    }
   ],
   "source": [
    "# Split trainin data into training and validation sets\n",
    "X_train_numeric, X_val_numeric, X_train_text, X_val_text, y_train, y_val = prep_for_classification(X_numeric, X_text, y)\n",
    "print('X_train_numeric', X_train_numeric.shape)\n",
    "print('X_train_text', X_train_text.shape)\n",
    "print('y_train', y_train.shape)\n",
    "print('X_val_numeric', X_val_numeric.shape)\n",
    "print('X_val_text', X_val_text.shape)\n",
    "print('y_val', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZhVYZEmx5ePv"
   },
   "outputs": [],
   "source": [
    "# Save preprocessed text data as sparse matrix\n",
    "X_train_text = sparse.csr_matrix(X_train_text)\n",
    "X_val_text = sparse.csr_matrix(X_val_text)\n",
    "sparse.save_npz('X_train_text.npz', X_train_text)\n",
    "sparse.save_npz('X_val_text.npz', X_val_text)\n",
    "\n",
    "# Save labels as sparse matrix\n",
    "y_train = sparse.csr_matrix(y_train)\n",
    "y_val = sparse.csr_matrix(y_val)\n",
    "sparse.save_npz('y_train.npz', y_train)\n",
    "sparse.save_npz('y_val.npz', y_val)\n",
    "\n",
    "# Save preprocessed numeric data\n",
    "np.savez('X_train_numeric.npz', X_train_numeric)\n",
    "np.savez('X_val_numeric.npz', X_val_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "Run this cell if preprocessed data files are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 67748,
     "status": "ok",
     "timestamp": 1535404876185,
     "user": {
      "displayName": "Albert Lee",
      "photoUrl": "//lh3.googleusercontent.com/-f0u8obCCk_w/AAAAAAAAAAI/AAAAAAAAAxs/83E22s4mtyM/s50-c-k-no/photo.jpg",
      "userId": "109969660738913262150"
     },
     "user_tz": 240
    },
    "id": "hGz1Dw0S5ePx",
    "outputId": "c35c4ea7-67ea-4fe3-c4fb-2a8fb5097c61"
   },
   "outputs": [],
   "source": [
    "X_train_text = sparse.load_npz('X_train_text.npz')\n",
    "X_val_text = sparse.load_npz('X_val_text.npz')\n",
    "X_test_text = sparse.load_npz('X_test_text.npz')\n",
    "X_train_numeric = np.load('X_train_numeric.npz')['arr_0']\n",
    "X_val_numeric = np.load('X_val_numeric.npz')['arr_0']\n",
    "X_test_numeric = np.load('X_test_numeric.npz')['arr_0']\n",
    "y_train = sparse.load_npz('y_train.npz')\n",
    "y_val = sparse.load_npz('y_val.npz')\n",
    "\n",
    "print('X_train_numeric', X_train_numeric.shape)\n",
    "print('X_train_text', X_train_text.shape)\n",
    "print('y_train', y_train.shape)\n",
    "print('X_val_numeric', X_val_numeric.shape)\n",
    "print('X_val_text', X_val_text.shape)\n",
    "print('y_val', y_val.shape)\n",
    "print('X_test_numeric', X_test_numeric.shape)\n",
    "print('X_test_text', X_test_text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UfppW7oi5eP0"
   },
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GtnM3_nE5eP0"
   },
   "outputs": [],
   "source": [
    "def build_network(X_numeric=X_train_numeric, X_text=X_train_text, y=y_train):\n",
    "    \"\"\"\n",
    "    Return compiled keras-model model\n",
    "    \n",
    "    param numpy array X_numeric: feature matrix for classification\n",
    "    param numpy array X_text: feature matrix for classification\n",
    "    param numpy array y: labels matrix for classification\n",
    "    \n",
    "    Required Libraries: keras\n",
    "    \"\"\"\n",
    "    \n",
    "    numeric_input = Input(shape=(X_numeric.shape[1],) , name='numeric_input') \n",
    "    text_input = Input(shape=(X_text.shape[1],) , name='text_input')\n",
    "    \n",
    "    dropout_value = 0.5\n",
    "    \n",
    "    # Function\n",
    "    text_function_hidden_layer = Dense(256, activation='relu')(text_input)\n",
    "    text_function_hidden_layer = Dropout(dropout_value)(text_function_hidden_layer)\n",
    "    text_function_hidden_layer = Dense(128, activation='relu')(text_function_hidden_layer)\n",
    "    text_function_hidden_layer = Dropout(dropout_value)(text_function_hidden_layer)\n",
    "    text_function_hidden_layer = Dense(64, activation='relu')(text_function_hidden_layer)\n",
    "    text_function_hidden_layer = Dropout(dropout_value)(text_function_hidden_layer)\n",
    "    numeric_function_hidden_layer = Dense(4, activation='relu')(numeric_input)\n",
    "    numeric_function_hidden_layer = Dropout(dropout_value)(numeric_function_hidden_layer)\n",
    "    combined_function_layer = concatenate([numeric_function_hidden_layer, text_function_hidden_layer])\n",
    "    function_output_layer = Dense(37, activation='softmax')(combined_function_layer)\n",
    "    \n",
    "    # Object_Type\n",
    "    text_object_type_hidden_layer = Dense(256, activation='relu')(text_input)\n",
    "    text_object_type_hidden_layer = Dropout(dropout_value)(text_object_type_hidden_layer)\n",
    "    text_object_type_hidden_layer = Dense(128, activation='relu')(text_object_type_hidden_layer)\n",
    "    text_object_type_hidden_layer = Dropout(dropout_value)(text_object_type_hidden_layer)\n",
    "    text_object_type_hidden_layer = Dense(64, activation='relu')(text_object_type_hidden_layer)\n",
    "    text_object_type_hidden_layer = Dropout(dropout_value)(text_object_type_hidden_layer)\n",
    "    text_object_type_hidden_layer = Dense(32, activation='relu')(text_object_type_hidden_layer)\n",
    "    text_object_type_hidden_layer = Dropout(dropout_value)(text_object_type_hidden_layer)\n",
    "    text_object_type_hidden_layer = Dense(16, activation='relu')(text_object_type_hidden_layer)\n",
    "    text_object_type_hidden_layer = Dropout(dropout_value)(text_object_type_hidden_layer)\n",
    "    numeric_object_type_hidden_layer = Dense(4, activation='relu')(numeric_input)\n",
    "    numeric_object_type_hidden_layer = Dropout(dropout_value)(numeric_object_type_hidden_layer)\n",
    "    combined_object_type_layer = concatenate([numeric_object_type_hidden_layer, text_object_type_hidden_layer])\n",
    "    object_type_output_layer = Dense(11, activation='softmax')(combined_object_type_layer)\n",
    "    \n",
    "    # Operating_Status\n",
    "    text_operating_status_hidden_layer = Dense(256, activation='relu')(text_input)\n",
    "    text_operating_status_hidden_layer = Dropout(dropout_value)(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dense(128, activation='relu')(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dropout(dropout_value)(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dense(64, activation='relu')(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dropout(dropout_value)(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dense(32, activation='relu')(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dropout(dropout_value)(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dense(16, activation='relu')(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dropout(dropout_value)(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dense(8, activation='relu')(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dropout(dropout_value)(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dense(4, activation='relu')(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dropout(dropout_value)(text_operating_status_hidden_layer)\n",
    "    numeric_operating_status_hidden_layer = Dense(4, activation='relu')(numeric_input)\n",
    "    numeric_operating_status_hidden_layer = Dropout(dropout_value)(numeric_operating_status_hidden_layer)\n",
    "    combined_operating_status_layer = concatenate([numeric_operating_status_hidden_layer, text_operating_status_hidden_layer])\n",
    "    operating_status_output_layer = Dense(3, activation='softmax')(combined_operating_status_layer)\n",
    "    \n",
    "    # Position_Type\n",
    "    text_position_type_hidden_layer = Dense(256, activation='relu')(text_input)\n",
    "    text_position_type_hidden_layer = Dropout(dropout_value)(text_position_type_hidden_layer)\n",
    "    text_position_type_hidden_layer = Dense(128, activation='relu')(text_position_type_hidden_layer)\n",
    "    text_position_type_hidden_layer = Dropout(dropout_value)(text_position_type_hidden_layer)\n",
    "    text_position_type_hidden_layer = Dense(64, activation='relu')(text_position_type_hidden_layer)\n",
    "    text_position_type_hidden_layer = Dropout(dropout_value)(text_position_type_hidden_layer)\n",
    "    text_position_type_hidden_layer = Dense(32, activation='relu')(text_position_type_hidden_layer)\n",
    "    text_position_type_hidden_layer = Dropout(dropout_value)(text_position_type_hidden_layer)\n",
    "    numeric_position_type_hidden_layer = Dense(4, activation='relu')(numeric_input)\n",
    "    numeric_position_type_hidden_layer = Dropout(dropout_value)(numeric_position_type_hidden_layer)\n",
    "    combined_position_type_layer = concatenate([numeric_position_type_hidden_layer, text_position_type_hidden_layer])\n",
    "    position_type_output_layer = Dense(25, activation='softmax')(combined_position_type_layer)\n",
    "    \n",
    "    # Pre_K\n",
    "    text_pre_k_hidden_layer = Dense(256, activation='relu')(text_input)\n",
    "    text_pre_k_hidden_layer = Dropout(dropout_value)(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dense(128, activation='relu')(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dropout(dropout_value)(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dense(64, activation='relu')(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dropout(dropout_value)(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dense(32, activation='relu')(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dropout(dropout_value)(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dense(16, activation='relu')(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dropout(dropout_value)(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dense(8, activation='relu')(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dropout(dropout_value)(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dense(4, activation='relu')(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dropout(dropout_value)(text_pre_k_hidden_layer)\n",
    "    numeric_pre_k_hidden_layer = Dense(4, activation='relu')(numeric_input)\n",
    "    numeric_pre_k_hidden_layer = Dropout(dropout_value)(numeric_pre_k_hidden_layer)\n",
    "    combined_pre_k_layer = concatenate([numeric_pre_k_hidden_layer, text_pre_k_hidden_layer])\n",
    "    pre_k_output_layer = Dense(3, activation='softmax')(combined_pre_k_layer)\n",
    "    \n",
    "    # Reporting\n",
    "    text_reporting_hidden_layer = Dense(256, activation='relu')(text_input)\n",
    "    text_reporting_hidden_layer = Dropout(dropout_value)(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dense(128, activation='relu')(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dropout(dropout_value)(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dense(64, activation='relu')(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dropout(dropout_value)(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dense(32, activation='relu')(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dropout(dropout_value)(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dense(16, activation='relu')(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dropout(dropout_value)(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dense(8, activation='relu')(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dropout(dropout_value)(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dense(4, activation='relu')(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dropout(dropout_value)(text_reporting_hidden_layer)\n",
    "    numeric_reporting_hidden_layer = Dense(4, activation='relu')(numeric_input)\n",
    "    numeric_reporting_hidden_layer = Dropout(dropout_value)(numeric_reporting_hidden_layer)\n",
    "    combined_reporting_layer = concatenate([numeric_reporting_hidden_layer, text_reporting_hidden_layer])\n",
    "    reporting_output_layer = Dense(3, activation='softmax')(combined_reporting_layer)\n",
    "    \n",
    "    # Sharing\n",
    "    text_sharing_hidden_layer = Dense(256, activation='relu')(text_input)\n",
    "    text_sharing_hidden_layer = Dropout(dropout_value)(text_sharing_hidden_layer)\n",
    "    text_sharing_hidden_layer = Dense(128, activation='relu')(text_sharing_hidden_layer)\n",
    "    text_sharing_hidden_layer = Dropout(dropout_value)(text_sharing_hidden_layer)\n",
    "    text_sharing_hidden_layer = Dense(64, activation='relu')(text_sharing_hidden_layer)\n",
    "    text_sharing_hidden_layer = Dropout(dropout_value)(text_sharing_hidden_layer)\n",
    "    text_sharing_hidden_layer = Dense(32, activation='relu')(text_sharing_hidden_layer)\n",
    "    text_sharing_hidden_layer = Dropout(dropout_value)(text_sharing_hidden_layer)\n",
    "    text_sharing_hidden_layer = Dense(16, activation='relu')(text_sharing_hidden_layer)\n",
    "    text_sharing_hidden_layer = Dropout(dropout_value)(text_sharing_hidden_layer)\n",
    "    text_sharing_hidden_layer = Dense(8, activation='relu')(text_sharing_hidden_layer)\n",
    "    text_sharing_hidden_layer = Dropout(dropout_value)(text_sharing_hidden_layer)\n",
    "    numeric_sharing_hidden_layer = Dense(4, activation='relu')(numeric_input)\n",
    "    numeric_sharing_hidden_layer = Dropout(dropout_value)(numeric_sharing_hidden_layer)\n",
    "    combined_sharing_layer = concatenate([numeric_sharing_hidden_layer, text_sharing_hidden_layer])\n",
    "    sharing_output_layer = Dense(5, activation='softmax')(combined_sharing_layer)\n",
    "    \n",
    "    # Student_Type\n",
    "    text_student_type_hidden_layer = Dense(256, activation='relu')(text_input)\n",
    "    text_student_type_hidden_layer = Dropout(dropout_value)(text_student_type_hidden_layer)\n",
    "    text_student_type_hidden_layer = Dense(128, activation='relu')(text_student_type_hidden_layer)\n",
    "    text_student_type_hidden_layer = Dropout(dropout_value)(text_student_type_hidden_layer)\n",
    "    text_student_type_hidden_layer = Dense(64, activation='relu')(text_student_type_hidden_layer)\n",
    "    text_student_type_hidden_layer = Dropout(dropout_value)(text_student_type_hidden_layer)\n",
    "    text_student_type_hidden_layer = Dense(32, activation='relu')(text_student_type_hidden_layer)\n",
    "    text_student_type_hidden_layer = Dropout(dropout_value)(text_student_type_hidden_layer)\n",
    "    text_student_type_hidden_layer = Dense(16, activation='relu')(text_student_type_hidden_layer)\n",
    "    text_student_type_hidden_layer = Dropout(dropout_value)(text_student_type_hidden_layer)\n",
    "    numeric_student_type_hidden_layer = Dense(4, activation='relu')(numeric_input)\n",
    "    numeric_student_type_hidden_layer = Dropout(dropout_value)(numeric_student_type_hidden_layer)\n",
    "    combined_student_type_layer = concatenate([numeric_student_type_hidden_layer, text_student_type_hidden_layer])\n",
    "    student_type_output_layer = Dense(9, activation='softmax')(combined_student_type_layer)\n",
    "    \n",
    "    # Use\n",
    "    text_use_hidden_layer = Dense(256, activation='relu')(text_input)\n",
    "    text_use_hidden_layer = Dropout(dropout_value)(text_use_hidden_layer)\n",
    "    text_use_hidden_layer = Dense(128, activation='relu')(text_use_hidden_layer)\n",
    "    text_use_hidden_layer = Dropout(dropout_value)(text_use_hidden_layer)\n",
    "    text_use_hidden_layer = Dense(64, activation='relu')(text_use_hidden_layer)\n",
    "    text_use_hidden_layer = Dropout(dropout_value)(text_use_hidden_layer)\n",
    "    text_use_hidden_layer = Dense(32, activation='relu')(text_use_hidden_layer)\n",
    "    text_use_hidden_layer = Dropout(dropout_value)(text_use_hidden_layer)\n",
    "    text_use_hidden_layer = Dense(16, activation='relu')(text_use_hidden_layer)\n",
    "    text_use_hidden_layer = Dropout(dropout_value)(text_use_hidden_layer)\n",
    "    numeric_use_hidden_layer = Dense(4, activation='relu')(numeric_input)\n",
    "    numeric_use_hidden_layer = Dropout(dropout_value)(numeric_use_hidden_layer)\n",
    "    combined_use_layer = concatenate([numeric_use_hidden_layer, text_use_hidden_layer])\n",
    "    use_output_layer = Dense(8, activation='softmax')(combined_use_layer)\n",
    "    \n",
    "    # Output\n",
    "    combined_output_layer = concatenate([function_output_layer, \n",
    "                                         object_type_output_layer,\n",
    "                                         operating_status_output_layer,\n",
    "                                         position_type_output_layer,\n",
    "                                         pre_k_output_layer,\n",
    "                                         reporting_output_layer,\n",
    "                                         sharing_output_layer,\n",
    "                                         student_type_output_layer,\n",
    "                                         use_output_layer])\n",
    "    \n",
    "    model = Model(inputs=[numeric_input, text_input], outputs=[combined_output_layer])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vkm6VVkF5eP2"
   },
   "outputs": [],
   "source": [
    "basic_NN = build_network()\n",
    "\n",
    "# Save model architexture\n",
    "model_json = basic_NN.to_json()\n",
    "with open('basic_NN_model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "INIHpqTn5eP4"
   },
   "source": [
    "# Train model and generate predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6834
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2556917,
     "status": "ok",
     "timestamp": 1535407446918,
     "user": {
      "displayName": "Albert Lee",
      "photoUrl": "//lh3.googleusercontent.com/-f0u8obCCk_w/AAAAAAAAAAI/AAAAAAAAAxs/83E22s4mtyM/s50-c-k-no/photo.jpg",
      "userId": "109969660738913262150"
     },
     "user_tz": 240
    },
    "id": "h2fWentL1If6",
    "outputId": "9841309a-368d-4e3d-db57-275705effe0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 315191 samples, validate on 35022 samples\n",
      "Epoch 1/100\n",
      "315191/315191 [==============================] - 33s 105us/step - loss: 69.0318 - acc: 0.0533 - val_loss: 57.6356 - val_acc: 0.1303\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 57.63565, saving model to basic_NN_model.h5\n",
      "Epoch 2/100\n",
      "315191/315191 [==============================] - 27s 86us/step - loss: 46.9497 - acc: 0.1016 - val_loss: 38.3025 - val_acc: 0.1550\n",
      "\n",
      "Epoch 00002: val_loss improved from 57.63565 to 38.30249, saving model to basic_NN_model.h5\n",
      "Epoch 3/100\n",
      "315191/315191 [==============================] - 26s 82us/step - loss: 38.9604 - acc: 0.1243 - val_loss: 31.9287 - val_acc: 0.2132\n",
      "\n",
      "Epoch 00003: val_loss improved from 38.30249 to 31.92871, saving model to basic_NN_model.h5\n",
      "Epoch 4/100\n",
      "315191/315191 [==============================] - 26s 83us/step - loss: 35.7136 - acc: 0.1454 - val_loss: 31.1295 - val_acc: 0.2287\n",
      "\n",
      "Epoch 00004: val_loss improved from 31.92871 to 31.12945, saving model to basic_NN_model.h5\n",
      "Epoch 5/100\n",
      "315191/315191 [==============================] - 26s 82us/step - loss: 34.1375 - acc: 0.1824 - val_loss: 29.6474 - val_acc: 0.2905\n",
      "\n",
      "Epoch 00005: val_loss improved from 31.12945 to 29.64735, saving model to basic_NN_model.h5\n",
      "Epoch 6/100\n",
      "315191/315191 [==============================] - 26s 81us/step - loss: 32.1330 - acc: 0.2171 - val_loss: 28.3948 - val_acc: 0.3332\n",
      "\n",
      "Epoch 00006: val_loss improved from 29.64735 to 28.39477, saving model to basic_NN_model.h5\n",
      "Epoch 7/100\n",
      "315191/315191 [==============================] - 25s 80us/step - loss: 30.7251 - acc: 0.2563 - val_loss: 27.8204 - val_acc: 0.3362\n",
      "\n",
      "Epoch 00007: val_loss improved from 28.39477 to 27.82040, saving model to basic_NN_model.h5\n",
      "Epoch 8/100\n",
      "315191/315191 [==============================] - 25s 80us/step - loss: 29.7898 - acc: 0.2723 - val_loss: 27.2732 - val_acc: 0.3949\n",
      "\n",
      "Epoch 00008: val_loss improved from 27.82040 to 27.27318, saving model to basic_NN_model.h5\n",
      "Epoch 9/100\n",
      "315191/315191 [==============================] - 25s 80us/step - loss: 29.2067 - acc: 0.2924 - val_loss: 27.1992 - val_acc: 0.4063\n",
      "\n",
      "Epoch 00009: val_loss improved from 27.27318 to 27.19919, saving model to basic_NN_model.h5\n",
      "Epoch 10/100\n",
      "315191/315191 [==============================] - 25s 80us/step - loss: 28.7009 - acc: 0.3017 - val_loss: 27.5297 - val_acc: 0.4170\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 27.19919\n",
      "Epoch 11/100\n",
      "315191/315191 [==============================] - 25s 78us/step - loss: 28.2329 - acc: 0.3124 - val_loss: 27.3623 - val_acc: 0.4203\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 27.19919\n",
      "Epoch 12/100\n",
      "315191/315191 [==============================] - 25s 80us/step - loss: 27.8402 - acc: 0.3227 - val_loss: 26.6406 - val_acc: 0.4366\n",
      "\n",
      "Epoch 00012: val_loss improved from 27.19919 to 26.64065, saving model to basic_NN_model.h5\n",
      "Epoch 13/100\n",
      "315191/315191 [==============================] - 25s 80us/step - loss: 27.6480 - acc: 0.3333 - val_loss: 27.4837 - val_acc: 0.4240\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 26.64065\n",
      "Epoch 14/100\n",
      "315191/315191 [==============================] - 25s 80us/step - loss: 27.5365 - acc: 0.3302 - val_loss: 25.7845 - val_acc: 0.4294\n",
      "\n",
      "Epoch 00014: val_loss improved from 26.64065 to 25.78452, saving model to basic_NN_model.h5\n",
      "Epoch 15/100\n",
      "315191/315191 [==============================] - 25s 80us/step - loss: 27.4182 - acc: 0.3289 - val_loss: 26.0990 - val_acc: 0.4475\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 25.78452\n",
      "Epoch 16/100\n",
      "315191/315191 [==============================] - 25s 78us/step - loss: 27.1136 - acc: 0.3344 - val_loss: 25.8979 - val_acc: 0.4285\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 25.78452\n",
      "Epoch 17/100\n",
      "315191/315191 [==============================] - 25s 78us/step - loss: 27.0734 - acc: 0.3330 - val_loss: 25.6079 - val_acc: 0.4368\n",
      "\n",
      "Epoch 00017: val_loss improved from 25.78452 to 25.60787, saving model to basic_NN_model.h5\n",
      "Epoch 18/100\n",
      "315191/315191 [==============================] - 25s 79us/step - loss: 27.1356 - acc: 0.3206 - val_loss: 26.0821 - val_acc: 0.4205\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 25.60787\n",
      "Epoch 19/100\n",
      "315191/315191 [==============================] - 25s 79us/step - loss: 26.9115 - acc: 0.3130 - val_loss: 25.6919 - val_acc: 0.4240\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 25.60787\n",
      "Epoch 20/100\n",
      "315191/315191 [==============================] - 25s 79us/step - loss: 26.7687 - acc: 0.3149 - val_loss: 25.8545 - val_acc: 0.4124\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 25.60787\n",
      "Epoch 21/100\n",
      "315191/315191 [==============================] - 25s 79us/step - loss: 26.7681 - acc: 0.3159 - val_loss: 26.1791 - val_acc: 0.4254\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 25.60787\n",
      "Epoch 22/100\n",
      "315191/315191 [==============================] - 25s 78us/step - loss: 26.7833 - acc: 0.3160 - val_loss: 25.8057 - val_acc: 0.4213\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 25.60787\n",
      "Epoch 23/100\n",
      "315191/315191 [==============================] - 25s 78us/step - loss: 26.8127 - acc: 0.3198 - val_loss: 25.7069 - val_acc: 0.4299\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 25.60787\n",
      "Epoch 24/100\n",
      "315191/315191 [==============================] - 24s 78us/step - loss: 26.7617 - acc: 0.3220 - val_loss: 25.5579 - val_acc: 0.4255\n",
      "\n",
      "Epoch 00024: val_loss improved from 25.60787 to 25.55792, saving model to basic_NN_model.h5\n",
      "Epoch 25/100\n",
      "315191/315191 [==============================] - 24s 78us/step - loss: 26.5844 - acc: 0.3268 - val_loss: 25.2871 - val_acc: 0.4440\n",
      "\n",
      "Epoch 00025: val_loss improved from 25.55792 to 25.28708, saving model to basic_NN_model.h5\n",
      "Epoch 26/100\n",
      "315191/315191 [==============================] - 24s 78us/step - loss: 26.4189 - acc: 0.3258 - val_loss: 25.3768 - val_acc: 0.4445\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 25.28708\n",
      "Epoch 27/100\n",
      "315191/315191 [==============================] - 25s 79us/step - loss: 26.5746 - acc: 0.3224 - val_loss: 25.5230 - val_acc: 0.4318\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 25.28708\n",
      "Epoch 28/100\n",
      "315191/315191 [==============================] - 25s 79us/step - loss: 26.4629 - acc: 0.3250 - val_loss: 25.5064 - val_acc: 0.4405\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 25.28708\n",
      "Epoch 29/100\n",
      "315191/315191 [==============================] - 25s 79us/step - loss: 26.3995 - acc: 0.3293 - val_loss: 25.3655 - val_acc: 0.4616\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 25.28708\n",
      "Epoch 30/100\n",
      "315191/315191 [==============================] - 25s 79us/step - loss: 26.1987 - acc: 0.3301 - val_loss: 24.7734 - val_acc: 0.4468\n",
      "\n",
      "Epoch 00030: val_loss improved from 25.28708 to 24.77336, saving model to basic_NN_model.h5\n",
      "Epoch 31/100\n",
      "315191/315191 [==============================] - 25s 79us/step - loss: 26.1656 - acc: 0.3326 - val_loss: 25.1151 - val_acc: 0.4545\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 24.77336\n",
      "Epoch 32/100\n",
      "315191/315191 [==============================] - 25s 79us/step - loss: 26.1605 - acc: 0.3384 - val_loss: 24.9967 - val_acc: 0.4565\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 24.77336\n",
      "Epoch 33/100\n",
      "315191/315191 [==============================] - 25s 79us/step - loss: 26.0912 - acc: 0.3360 - val_loss: 24.9276 - val_acc: 0.4560\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 24.77336\n",
      "Epoch 34/100\n",
      "315191/315191 [==============================] - 25s 79us/step - loss: 26.0643 - acc: 0.3391 - val_loss: 24.9528 - val_acc: 0.4603\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 24.77336\n",
      "Epoch 35/100\n",
      "315191/315191 [==============================] - 25s 79us/step - loss: 26.0185 - acc: 0.3413 - val_loss: 24.9064 - val_acc: 0.4615\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 24.77336\n",
      "Epoch 36/100\n",
      "315191/315191 [==============================] - 24s 77us/step - loss: 25.8426 - acc: 0.3529 - val_loss: 23.9120 - val_acc: 0.4855\n",
      "\n",
      "Epoch 00036: val_loss improved from 24.77336 to 23.91197, saving model to basic_NN_model.h5\n",
      "Epoch 37/100\n",
      "315191/315191 [==============================] - 25s 79us/step - loss: 25.8165 - acc: 0.3507 - val_loss: 23.8764 - val_acc: 0.4728\n",
      "\n",
      "Epoch 00037: val_loss improved from 23.91197 to 23.87636, saving model to basic_NN_model.h5\n",
      "Epoch 38/100\n",
      "315191/315191 [==============================] - 25s 78us/step - loss: 25.8669 - acc: 0.3473 - val_loss: 23.7460 - val_acc: 0.4676\n",
      "\n",
      "Epoch 00038: val_loss improved from 23.87636 to 23.74596, saving model to basic_NN_model.h5\n",
      "Epoch 39/100\n",
      "315191/315191 [==============================] - 25s 81us/step - loss: 25.7573 - acc: 0.3488 - val_loss: 24.0224 - val_acc: 0.4667\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 23.74596\n",
      "Epoch 40/100\n",
      "315191/315191 [==============================] - 25s 80us/step - loss: 25.7233 - acc: 0.3470 - val_loss: 24.0784 - val_acc: 0.4680\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 23.74596\n",
      "Epoch 41/100\n",
      "315191/315191 [==============================] - 25s 81us/step - loss: 25.6848 - acc: 0.3445 - val_loss: 24.1271 - val_acc: 0.4596\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 23.74596\n",
      "Epoch 42/100\n",
      "315191/315191 [==============================] - 25s 80us/step - loss: 25.6117 - acc: 0.3412 - val_loss: 24.0344 - val_acc: 0.4635\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 23.74596\n",
      "Epoch 43/100\n",
      "315191/315191 [==============================] - 25s 80us/step - loss: 25.7440 - acc: 0.3480 - val_loss: 23.8088 - val_acc: 0.4555\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 23.74596\n",
      "Epoch 44/100\n",
      "315191/315191 [==============================] - 25s 80us/step - loss: 25.5606 - acc: 0.3421 - val_loss: 24.0826 - val_acc: 0.4489\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 23.74596\n",
      "Epoch 45/100\n",
      "315191/315191 [==============================] - 25s 80us/step - loss: 25.5530 - acc: 0.3371 - val_loss: 23.7910 - val_acc: 0.4457\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 23.74596\n",
      "Epoch 46/100\n",
      "315191/315191 [==============================] - 25s 80us/step - loss: 25.4245 - acc: 0.3356 - val_loss: 23.7245 - val_acc: 0.4552\n",
      "\n",
      "Epoch 00046: val_loss improved from 23.74596 to 23.72451, saving model to basic_NN_model.h5\n",
      "Epoch 47/100\n",
      "315191/315191 [==============================] - 25s 80us/step - loss: 25.4066 - acc: 0.3342 - val_loss: 23.7979 - val_acc: 0.4417\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 23.72451\n",
      "Epoch 48/100\n",
      "315191/315191 [==============================] - 25s 79us/step - loss: 25.2089 - acc: 0.3358 - val_loss: 23.8424 - val_acc: 0.4558\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 23.72451\n",
      "Epoch 49/100\n",
      "315191/315191 [==============================] - 25s 80us/step - loss: 25.0855 - acc: 0.3376 - val_loss: 23.9154 - val_acc: 0.4554\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 23.72451\n",
      "Epoch 50/100\n",
      "315191/315191 [==============================] - 25s 80us/step - loss: 25.1180 - acc: 0.3370 - val_loss: 23.7876 - val_acc: 0.4492\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 23.72451\n",
      "Epoch 51/100\n",
      "315191/315191 [==============================] - 25s 80us/step - loss: 24.9147 - acc: 0.3357 - val_loss: 23.7232 - val_acc: 0.4541\n",
      "\n",
      "Epoch 00051: val_loss improved from 23.72451 to 23.72321, saving model to basic_NN_model.h5\n",
      "Epoch 52/100\n",
      "315191/315191 [==============================] - 25s 80us/step - loss: 24.8797 - acc: 0.3402 - val_loss: 23.9931 - val_acc: 0.4418\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 23.72321\n",
      "Epoch 53/100\n",
      "315191/315191 [==============================] - 25s 80us/step - loss: 25.0244 - acc: 0.3358 - val_loss: 23.7053 - val_acc: 0.4352\n",
      "\n",
      "Epoch 00053: val_loss improved from 23.72321 to 23.70534, saving model to basic_NN_model.h5\n",
      "Epoch 54/100\n",
      "315191/315191 [==============================] - 25s 80us/step - loss: 24.9153 - acc: 0.3351 - val_loss: 23.9712 - val_acc: 0.4410\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 23.70534\n",
      "Epoch 55/100\n",
      "315191/315191 [==============================] - 25s 80us/step - loss: 24.9874 - acc: 0.3323 - val_loss: 23.7526 - val_acc: 0.4207\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 23.70534\n",
      "Epoch 56/100\n",
      "315191/315191 [==============================] - 25s 80us/step - loss: 24.9989 - acc: 0.3286 - val_loss: 23.4751 - val_acc: 0.4297\n",
      "\n",
      "Epoch 00056: val_loss improved from 23.70534 to 23.47510, saving model to basic_NN_model.h5\n",
      "Epoch 57/100\n",
      "315191/315191 [==============================] - 25s 79us/step - loss: 24.8413 - acc: 0.3359 - val_loss: 23.5852 - val_acc: 0.4301\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 23.47510\n",
      "Epoch 58/100\n",
      "315191/315191 [==============================] - 25s 79us/step - loss: 24.7361 - acc: 0.3346 - val_loss: 23.5237 - val_acc: 0.4297\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 23.47510\n",
      "Epoch 59/100\n",
      "315191/315191 [==============================] - 25s 80us/step - loss: 24.7897 - acc: 0.3362 - val_loss: 23.4365 - val_acc: 0.4258\n",
      "\n",
      "Epoch 00059: val_loss improved from 23.47510 to 23.43651, saving model to basic_NN_model.h5\n",
      "Epoch 60/100\n",
      "315191/315191 [==============================] - 25s 79us/step - loss: 24.6578 - acc: 0.3441 - val_loss: 23.4508 - val_acc: 0.4421\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 23.43651\n",
      "Epoch 61/100\n",
      "315191/315191 [==============================] - 25s 78us/step - loss: 24.5580 - acc: 0.3354 - val_loss: 23.2512 - val_acc: 0.4230\n",
      "\n",
      "Epoch 00061: val_loss improved from 23.43651 to 23.25117, saving model to basic_NN_model.h5\n",
      "Epoch 62/100\n",
      "315191/315191 [==============================] - 26s 82us/step - loss: 24.5056 - acc: 0.3362 - val_loss: 23.3454 - val_acc: 0.4309\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 23.25117\n",
      "Epoch 63/100\n",
      "315191/315191 [==============================] - 26s 83us/step - loss: 24.6779 - acc: 0.3372 - val_loss: 23.3842 - val_acc: 0.4225\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 23.25117\n",
      "Epoch 64/100\n",
      "315191/315191 [==============================] - 25s 80us/step - loss: 24.5305 - acc: 0.3400 - val_loss: 23.2027 - val_acc: 0.4419\n",
      "\n",
      "Epoch 00064: val_loss improved from 23.25117 to 23.20269, saving model to basic_NN_model.h5\n",
      "Epoch 65/100\n",
      "315191/315191 [==============================] - 25s 80us/step - loss: 24.4791 - acc: 0.3440 - val_loss: 23.2763 - val_acc: 0.4497\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 23.20269\n",
      "Epoch 66/100\n",
      "315191/315191 [==============================] - 25s 81us/step - loss: 24.4432 - acc: 0.3464 - val_loss: 23.1099 - val_acc: 0.4508\n",
      "\n",
      "Epoch 00066: val_loss improved from 23.20269 to 23.10992, saving model to basic_NN_model.h5\n",
      "Epoch 67/100\n",
      "315191/315191 [==============================] - 27s 85us/step - loss: 24.4465 - acc: 0.3460 - val_loss: 23.1550 - val_acc: 0.4367\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 23.10992\n",
      "Epoch 68/100\n",
      "315191/315191 [==============================] - 27s 84us/step - loss: 24.4295 - acc: 0.3438 - val_loss: 22.8735 - val_acc: 0.4557\n",
      "\n",
      "Epoch 00068: val_loss improved from 23.10992 to 22.87348, saving model to basic_NN_model.h5\n",
      "Epoch 69/100\n",
      "315191/315191 [==============================] - 26s 84us/step - loss: 24.4212 - acc: 0.3472 - val_loss: 23.3370 - val_acc: 0.4455\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 22.87348\n",
      "Epoch 70/100\n",
      "315191/315191 [==============================] - 26s 82us/step - loss: 24.3553 - acc: 0.3492 - val_loss: 22.8929 - val_acc: 0.4381\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 22.87348\n",
      "Epoch 71/100\n",
      "315191/315191 [==============================] - 26s 82us/step - loss: 24.4195 - acc: 0.3458 - val_loss: 23.1304 - val_acc: 0.4490\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 22.87348\n",
      "Epoch 72/100\n",
      "315191/315191 [==============================] - 25s 81us/step - loss: 24.3331 - acc: 0.3567 - val_loss: 23.1049 - val_acc: 0.4552\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 22.87348\n",
      "Epoch 73/100\n",
      "315191/315191 [==============================] - 25s 80us/step - loss: 24.4877 - acc: 0.3513 - val_loss: 23.0972 - val_acc: 0.4528\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 22.87348\n",
      "Epoch 74/100\n",
      "315191/315191 [==============================] - 25s 81us/step - loss: 24.3657 - acc: 0.3476 - val_loss: 22.8001 - val_acc: 0.4580\n",
      "\n",
      "Epoch 00074: val_loss improved from 22.87348 to 22.80012, saving model to basic_NN_model.h5\n",
      "Epoch 75/100\n",
      "315191/315191 [==============================] - 26s 81us/step - loss: 24.2915 - acc: 0.3481 - val_loss: 23.0396 - val_acc: 0.4434\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 22.80012\n",
      "Epoch 76/100\n",
      "315191/315191 [==============================] - 26s 81us/step - loss: 24.2758 - acc: 0.3497 - val_loss: 23.1215 - val_acc: 0.4459\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 22.80012\n",
      "Epoch 77/100\n",
      "315191/315191 [==============================] - 26s 81us/step - loss: 24.3878 - acc: 0.3505 - val_loss: 23.2180 - val_acc: 0.4574\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 22.80012\n",
      "Epoch 78/100\n",
      "315191/315191 [==============================] - 25s 80us/step - loss: 24.2187 - acc: 0.3527 - val_loss: 22.8871 - val_acc: 0.4574\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 22.80012\n",
      "Epoch 79/100\n",
      "315191/315191 [==============================] - 25s 80us/step - loss: 24.3750 - acc: 0.3492 - val_loss: 23.1236 - val_acc: 0.4288\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 22.80012\n",
      "Epoch 80/100\n",
      "315191/315191 [==============================] - 26s 81us/step - loss: 24.2720 - acc: 0.3502 - val_loss: 23.0134 - val_acc: 0.4559\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 22.80012\n",
      "Epoch 81/100\n",
      "315191/315191 [==============================] - 25s 81us/step - loss: 24.2161 - acc: 0.3504 - val_loss: 22.9006 - val_acc: 0.4390\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 22.80012\n",
      "Epoch 82/100\n",
      "315191/315191 [==============================] - 25s 80us/step - loss: 24.1667 - acc: 0.3499 - val_loss: 22.7909 - val_acc: 0.4493\n",
      "\n",
      "Epoch 00082: val_loss improved from 22.80012 to 22.79094, saving model to basic_NN_model.h5\n",
      "Epoch 83/100\n",
      "315191/315191 [==============================] - 25s 79us/step - loss: 24.2081 - acc: 0.3521 - val_loss: 22.8770 - val_acc: 0.4392\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 22.79094\n",
      "Epoch 84/100\n",
      "315191/315191 [==============================] - 25s 78us/step - loss: 24.1301 - acc: 0.3530 - val_loss: 22.6922 - val_acc: 0.4465\n",
      "\n",
      "Epoch 00084: val_loss improved from 22.79094 to 22.69218, saving model to basic_NN_model.h5\n",
      "Epoch 85/100\n",
      "315191/315191 [==============================] - 24s 78us/step - loss: 24.1248 - acc: 0.3569 - val_loss: 22.7594 - val_acc: 0.4386\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 22.69218\n",
      "Epoch 86/100\n",
      "315191/315191 [==============================] - 25s 78us/step - loss: 24.0799 - acc: 0.3539 - val_loss: 22.7088 - val_acc: 0.4489\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 22.69218\n",
      "Epoch 87/100\n",
      "315191/315191 [==============================] - 25s 78us/step - loss: 24.0508 - acc: 0.3583 - val_loss: 22.9096 - val_acc: 0.4553\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 22.69218\n",
      "Epoch 88/100\n",
      "315191/315191 [==============================] - 25s 78us/step - loss: 24.1788 - acc: 0.3601 - val_loss: 22.7386 - val_acc: 0.4552\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 22.69218\n",
      "Epoch 89/100\n",
      "315191/315191 [==============================] - 25s 78us/step - loss: 24.0624 - acc: 0.3561 - val_loss: 22.6491 - val_acc: 0.4585\n",
      "\n",
      "Epoch 00089: val_loss improved from 22.69218 to 22.64912, saving model to basic_NN_model.h5\n",
      "Epoch 90/100\n",
      "315191/315191 [==============================] - 25s 78us/step - loss: 24.1064 - acc: 0.3590 - val_loss: 22.7578 - val_acc: 0.4694\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 22.64912\n",
      "Epoch 91/100\n",
      "315191/315191 [==============================] - 25s 78us/step - loss: 24.0815 - acc: 0.3647 - val_loss: 23.0082 - val_acc: 0.4619\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 22.64912\n",
      "Epoch 92/100\n",
      "315191/315191 [==============================] - 25s 78us/step - loss: 24.0815 - acc: 0.3596 - val_loss: 22.6915 - val_acc: 0.4524\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 22.64912\n",
      "Epoch 93/100\n",
      "315191/315191 [==============================] - 25s 78us/step - loss: 23.9640 - acc: 0.3618 - val_loss: 22.4781 - val_acc: 0.4560\n",
      "\n",
      "Epoch 00093: val_loss improved from 22.64912 to 22.47809, saving model to basic_NN_model.h5\n",
      "Epoch 94/100\n",
      "315191/315191 [==============================] - 25s 81us/step - loss: 23.9682 - acc: 0.3645 - val_loss: 22.8621 - val_acc: 0.4832\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 22.47809\n",
      "Epoch 95/100\n",
      "315191/315191 [==============================] - 25s 80us/step - loss: 23.9497 - acc: 0.3702 - val_loss: 22.5114 - val_acc: 0.4755\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 22.47809\n",
      "Epoch 96/100\n",
      "315191/315191 [==============================] - 25s 80us/step - loss: 23.9913 - acc: 0.3698 - val_loss: 22.6515 - val_acc: 0.4845\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 22.47809\n",
      "Epoch 97/100\n",
      "315191/315191 [==============================] - 25s 78us/step - loss: 24.0836 - acc: 0.3703 - val_loss: 22.3655 - val_acc: 0.4869\n",
      "\n",
      "Epoch 00097: val_loss improved from 22.47809 to 22.36555, saving model to basic_NN_model.h5\n",
      "Epoch 98/100\n",
      "315191/315191 [==============================] - 25s 80us/step - loss: 23.9520 - acc: 0.3702 - val_loss: 22.5179 - val_acc: 0.4830\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 22.36555\n",
      "Epoch 99/100\n",
      "315191/315191 [==============================] - 25s 79us/step - loss: 24.0765 - acc: 0.3744 - val_loss: 22.9177 - val_acc: 0.4875\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 22.36555\n",
      "Epoch 100/100\n",
      "315191/315191 [==============================] - 25s 79us/step - loss: 24.0162 - acc: 0.3770 - val_loss: 22.3559 - val_acc: 0.4770\n",
      "\n",
      "Epoch 00100: val_loss improved from 22.36555 to 22.35589, saving model to basic_NN_model.h5\n"
     ]
    }
   ],
   "source": [
    "early_stopping_monitor = EarlyStopping(patience=10)\n",
    "checkpointer = ModelCheckpoint(filepath=\"basic_NN_model.h5\", verbose=1, save_best_only=True)\n",
    "\n",
    "history_basic_NN = basic_NN.fit([X_train_numeric, X_train_text], y_train, epochs=100, batch_size=2048, validation_split=0.1, callbacks=[early_stopping_monitor, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 476,
     "status": "ok",
     "timestamp": 1535407573848,
     "user": {
      "displayName": "Albert Lee",
      "photoUrl": "//lh3.googleusercontent.com/-f0u8obCCk_w/AAAAAAAAAAI/AAAAAAAAAxs/83E22s4mtyM/s50-c-k-no/photo.jpg",
      "userId": "109969660738913262150"
     },
     "user_tz": 240
    },
    "id": "Aqfq_Q9VDf5B",
    "outputId": "05e5f981-9ebe-4118-e938-9a09319c518d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAFnCAYAAACLnxFFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niEiIUNEWEREJESraIiIiIeJ3szd8+YbG3/AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd278619588>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training and validation loss\n",
    "plt.plot(history_basic_NN.history['loss'])\n",
    "plt.plot(history_basic_NN.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss is lower on validation data because of dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MzyY-6uQNQCT"
   },
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "# run this cell if saved model and weights are available\n",
    "with open('basic_NN_model.json', 'r') as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "\n",
    "basic_NN = model_from_json(loaded_model_json)\n",
    "basic_NN.load_weights('basic_NN_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AlXYzPrd5eP4"
   },
   "outputs": [],
   "source": [
    "label_class_counts = {\n",
    "    'Function': [0, 37], \n",
    "    'Object_Type': [37, 48], \n",
    "    'Operating_Status': [48, 51], \n",
    "    'Position_Type': [51, 76], \n",
    "    'Pre_K': [76, 79], \n",
    "    'Reporting': [79, 82], \n",
    "    'Sharing': [82, 87], \n",
    "    'Student_Type': [87, 96], \n",
    "    'Use': [96, 104]\n",
    "}\n",
    "\n",
    "scores = {}\n",
    "val_predictions = basic_NN.predict([X_val_numeric, X_val_text])\n",
    "\n",
    "# Calculate validation logloss for each label\n",
    "for label, indices in label_class_counts.items():\n",
    "    # Get values for specific label\n",
    "    start_idx = indices[0]\n",
    "    end_idx = indices[1]\n",
    "    y_val_label = y_val[:, start_idx:end_idx]\n",
    "    val_predictions_label = val_predictions[:, start_idx:end_idx]\n",
    "    \n",
    "    \n",
    "    # Get logloss score of mode\n",
    "    scores[label] = log_loss(y_val_label, val_predictions_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c6d9EwoxGX3z"
   },
   "outputs": [],
   "source": [
    "# Generate predictions for test set\n",
    "predictions = basic_NN.predict([X_test_numeric, X_test_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_-pJMVIr5eP7"
   },
   "source": [
    "# Save score and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ngyDlM8A5eP7"
   },
   "outputs": [],
   "source": [
    "# Save predictions for test set\n",
    "label = ['Function',\n",
    "         'Object_Type',\n",
    "         'Operating_Status',\n",
    "         'Position_Type',\n",
    "         'Pre_K',\n",
    "         'Reporting',\n",
    "         'Sharing',\n",
    "         'Student_Type',\n",
    "         'Use']\n",
    "y = pd.get_dummies(data_train[label])\n",
    "\n",
    "submission = pd.DataFrame(predictions, index=data_test.index, columns=y.columns)\n",
    "submission.to_csv('basic_NN_model.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 282,
     "status": "ok",
     "timestamp": 1535409243925,
     "user": {
      "displayName": "Albert Lee",
      "photoUrl": "//lh3.googleusercontent.com/-f0u8obCCk_w/AAAAAAAAAAI/AAAAAAAAAxs/83E22s4mtyM/s50-c-k-no/photo.jpg",
      "userId": "109969660738913262150"
     },
     "user_tz": 240
    },
    "id": "uRHjIso15eP8",
    "outputId": "0aa7ff65-ec71-495c-a0f5-428ffb8d2cdd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Function': 0.20235726074366506,\n",
       " 'Object_Type': 0.2084070009335452,\n",
       " 'Operating_Status': 0.20732444779513906,\n",
       " 'Position_Type': 0.14738533642239618,\n",
       " 'Pre_K': 0.1738836264962506,\n",
       " 'Reporting': 1.7706314793889721,\n",
       " 'Sharing': 0.3757578466487278,\n",
       " 'Student_Type': 0.7058452184809189,\n",
       " 'Use': 0.2704541813227247}"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 509,
     "status": "ok",
     "timestamp": 1535409248887,
     "user": {
      "displayName": "Albert Lee",
      "photoUrl": "//lh3.googleusercontent.com/-f0u8obCCk_w/AAAAAAAAAAI/AAAAAAAAAxs/83E22s4mtyM/s50-c-k-no/photo.jpg",
      "userId": "109969660738913262150"
     },
     "user_tz": 240
    },
    "id": "yQd8s_q85eP_",
    "outputId": "bfa37583-2850-4742-c0e4-1019be408acd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4513384886924821"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([score for score in scores.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DQxW3L5o5eQC"
   },
   "outputs": [],
   "source": [
    "# Save scores\n",
    "with open('basic_NN_score.json', 'w') as file:\n",
    "     file.write(json.dumps(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A2xKwd0vMz7t"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "basic_NN_model.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
