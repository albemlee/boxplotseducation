{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gnWCGPLQ5ePG"
   },
   "source": [
    "# Setup Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nrg9OIDy5ePJ"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# To load data\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For text processing\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import Imputer\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# To build model\n",
    "import keras\n",
    "from keras.layers import Dense, concatenate, Input, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "# To train model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# To evaluate model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# To track time elapsed\n",
    "import time\n",
    "\n",
    "# To save results\n",
    "import dill\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uPLEdpRv5ePM"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YF3nUD615ePM"
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Return pandas dataframe data_train: training data (features + labels)\n",
    "    Return pandas dataframe data_test: test data (only features)\n",
    "    \n",
    "    Required Libraries: zipfile, pandas\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load zipped folder with data files\n",
    "    resource_archive = zipfile.ZipFile('resources.zip', 'r')\n",
    "\n",
    "    # Load testing data\n",
    "    data_test = pd.read_csv(resource_archive.open('TestData.csv'), \n",
    "                            dtype={\n",
    "                                'Object_Description': str, \n",
    "                                'Program_Description': str, \n",
    "                                'SubFund_Description': str, \n",
    "                                'Job_Title_Description': str, \n",
    "                                'Facility_or_Department': str,\n",
    "                                'Sub_Object_Description': str, \n",
    "                                'Location_Description': str, \n",
    "                                'FTE': float,\n",
    "                                'Function_Description': str, \n",
    "                                'Position_Extra': str, \n",
    "                                'Text_4': str, \n",
    "                                'Total': float, \n",
    "                                'Text_2': str,\n",
    "                                'Text_3': str, \n",
    "                                'Fund_Description': str, \n",
    "                                'Text_1': str\n",
    "                            },\n",
    "                            index_col=0)\n",
    "\n",
    "    # Load training data\n",
    "    data_train = pd.read_csv(resource_archive.open('TrainingData.csv'), \n",
    "                            dtype={\n",
    "                                'Object_Description': str, \n",
    "                                'Program_Description': str, \n",
    "                                'SubFund_Description': str, \n",
    "                                'Job_Title_Description': str, \n",
    "                                'Facility_or_Department': str,\n",
    "                                'Sub_Object_Description': str, \n",
    "                                'Location_Description': str, \n",
    "                                'FTE': float,\n",
    "                                'Function_Description': str, \n",
    "                                'Position_Extra': str, \n",
    "                                'Text_4': str, \n",
    "                                'Total': float, \n",
    "                                'Text_2': str,\n",
    "                                'Text_3': str, \n",
    "                                'Fund_Description': str, \n",
    "                                'Text_1': str,\n",
    "                                'Function': 'category',\n",
    "                                'Object_Type': 'category',\n",
    "                                'Operating_Status': 'category',\n",
    "                                'Position_Type': 'category',\n",
    "                                'Pre_K': 'category',\n",
    "                                'Reporting': 'category',\n",
    "                                'Sharing': 'category',\n",
    "                                'Student_Type': 'category',\n",
    "                                'Use': 'category',\n",
    "                            },\n",
    "                             index_col=0)\n",
    "    \n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "GVz1rkaX5ePP",
    "outputId": "fa1d8aae-4a4f-446b-e080-12d8be1679c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_train shape: (400277, 25)\n",
      "data_test shape: (50064, 16)\n"
     ]
    }
   ],
   "source": [
    "data_train, data_test = load_data()\n",
    "print('data_train shape:', data_train.shape)\n",
    "print('data_test shape:', data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i203UxGy5ePT"
   },
   "outputs": [],
   "source": [
    "def load_features(data_train, data_test):\n",
    "    \"\"\"\n",
    "    Return pandas dataframe data_features: data in feature columns of data_train and data_test\n",
    "    \n",
    "    Param pandas dataframe data_train: training data (features + labels)\n",
    "    Param pandas dataframe data_test: test data (only features)\n",
    "    \n",
    "    Required Libraries: pandas\n",
    "    \"\"\"\n",
    "    \n",
    "    feature_columns = data_test.columns # data_test only contains features\n",
    "    \n",
    "    data_features = pd.concat([data_train[feature_columns], data_test])\n",
    "    \n",
    "    return data_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vBZekpVd5ePV",
    "outputId": "af724da7-ff07-49b6-93a2-2beb6e94bcb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_features shape: (450341, 16)\n"
     ]
    }
   ],
   "source": [
    "# Load Features\n",
    "data_features = load_features(data_train, data_test)\n",
    "print('data_features shape:', data_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ITNF9275ePY"
   },
   "source": [
    "# Prepare Data for Classification\n",
    "Run the cells below if prepped data files have not been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ID0Uo8GT5ePY"
   },
   "outputs": [],
   "source": [
    "def text_processing(phrase):\n",
    "    \"\"\"\n",
    "    Return list processed_phrase: phrase tokens after processing has been completed\n",
    "    \n",
    "    param string phrase: phrase to be processed\n",
    "    \n",
    "    Required Libraries: re, nltk\n",
    "    \"\"\"\n",
    "    \n",
    "    # Case Normalization\n",
    "    processed_phrase = phrase.lower()\n",
    "    \n",
    "    # Remove Punctuations\n",
    "    processed_phrase = re.sub(r\"[^a-z0-9-]\", \" \", processed_phrase)\n",
    "    \n",
    "    # Tokenize Phrase\n",
    "    processed_phrase = processed_phrase.split()\n",
    "    \n",
    "    # Remove Stopwords\n",
    "    processed_phrase = [word for word in processed_phrase if word not in stopwords.words(\"english\") and word != '-']\n",
    "    \n",
    "    # Lemmatization\n",
    "    processed_phrase = [WordNetLemmatizer().lemmatize(word) for word in processed_phrase]\n",
    "    \n",
    "    # Recombine list into phrase\n",
    "    processed_phrase = ' '.join(processed_phrase)\n",
    "    \n",
    "    return processed_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_ZapkVld5ePb",
    "outputId": "8bf67ef2-f47a-4d30-84cf-7bb823a5cd59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "past patiently waitin passionately smashin every expectation every action act creation laughin face casualty sorrow first time thinkin past tomorrow\n",
      "0.033808231353759766\n"
     ]
    }
   ],
   "source": [
    "# test text_processing function (with quote from Hamilton: The Musical)\n",
    "start_time = time.time()\n",
    "print(text_processing(\n",
    "    \"I’m past patiently waitin’. I’m passionately smashin’ every expectation. \" + \n",
    "    \"Every action’s an act of creation! \" +\n",
    "    \"I’m laughin’ in the face of casualties and sorrow. \" +\n",
    "    \"For the first time, I’m thinkin’ past tomorrow\"))\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZFQgJuJ_5ePd"
   },
   "outputs": [],
   "source": [
    "def init_prep(data_train, data_test, data_features, label=None):\n",
    "    \"\"\"\n",
    "    Return numpy array X_numeric: numerical feature matrix of test set\n",
    "    Return numpy array X_text: text feature matrix for classification model fitting\n",
    "    Return numpy array X_numeric_test: numerical feature matrix of test set\n",
    "    Return numpy array X_text_test: text feature matrix for classification model fitting\n",
    "    Return numpy array y: labels matrix for classification model fitting\n",
    "    Return keras.Tokenizer() tokenize: contains word to token mapping\n",
    "    \n",
    "    Param pandas dataframe data_train: training data (features + labels)\n",
    "    Param pandas dataframe data_test: test data (features)\n",
    "    Param pandas dataframe data_features: data in feature columns of data_train and data_test\n",
    "    \n",
    "    Required Libraries: pandas, numpy, keras\n",
    "    Required helper functions: text_processing\n",
    "    \"\"\"\n",
    "    \n",
    "    # Combined and preprocess text columns\n",
    "    data_train['combined_text'] = (data_train[data_features.columns]\n",
    "                                       .drop(columns=['FTE', 'Total'])\n",
    "                                       .fillna(\"\")\n",
    "                                       .apply(lambda x: \" \".join(x), axis=1)\n",
    "                                       .apply(lambda x: text_processing(x))\n",
    "                                  )\n",
    "    data_test['combined_text'] = (data_test[data_features.columns]\n",
    "                                       .drop(columns=['FTE', 'Total'])\n",
    "                                       .fillna(\"\")\n",
    "                                       .apply(lambda x: \" \".join(x), axis=1)\n",
    "                                       .apply(lambda x: text_processing(x))\n",
    "                                 )\n",
    "    data_features['combined_text'] = (data_features\n",
    "                                          .drop(columns=['FTE', 'Total'])\n",
    "                                          .fillna(\"\")\n",
    "                                          .apply(lambda x: \" \".join(x), axis=1)\n",
    "                                          .apply(lambda x: text_processing(x))\n",
    "                                     )\n",
    "    \n",
    "    # Vectorizer text columns in training data\n",
    "    tokenize = Tokenizer()\n",
    "    tokenize.fit_on_texts(data_features['combined_text'])\n",
    "    \n",
    "    X_text = tokenize.texts_to_matrix(data_train['combined_text'])\n",
    "    X_text_test = tokenize.texts_to_matrix(data_test['combined_text'])\n",
    "    \n",
    "    # Impute missing numerical data\n",
    "    imp_total = Imputer(strategy='median')\n",
    "    imp_total.fit(data_features['Total'].values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "    total_not_missing = pd.isnull(data_train['Total']).astype(int).values.reshape(-1, 1)\n",
    "    fte_not_missing = pd.isnull(data_train['FTE']).astype(int).values.reshape(-1, 1)\n",
    "    total = imp_total.transform(data_train['Total'].values.reshape(-1, 1))\n",
    "    fte = data_train['FTE'].fillna('0').values.reshape(-1, 1)\n",
    "\n",
    "    total_not_missing_test = pd.isnull(data_test['Total']).astype(int).values.reshape(-1, 1)\n",
    "    fte_not_missing_test = pd.isnull(data_test['FTE']).astype(int).values.reshape(-1, 1)\n",
    "    total_test = imp_total.transform(data_test['Total'].values.reshape(-1, 1))\n",
    "    fte_test = data_test['FTE'].fillna('0').values.reshape(-1, 1)\n",
    "    \n",
    "    # Create feature matrix\n",
    "    X_numeric = np.concatenate([total, total_not_missing, fte, fte_not_missing], axis=1)\n",
    "    X_numeric_test = np.concatenate([total_test, total_not_missing_test, fte_test, fte_not_missing_test], axis=1)\n",
    "    \n",
    "    # Create labels matrix\n",
    "    if label:\n",
    "        y = pd.get_dummies(data_train[label]).values.astype('float64')\n",
    "    else:\n",
    "        label = ['Function',\n",
    "                 'Object_Type',\n",
    "                 'Operating_Status',\n",
    "                 'Position_Type',\n",
    "                 'Pre_K',\n",
    "                 'Reporting',\n",
    "                 'Sharing',\n",
    "                 'Student_Type',\n",
    "                 'Use']\n",
    "        y = pd.get_dummies(data_train[label]).values.astype('float64')\n",
    "    \n",
    "    return X_numeric, X_text, X_numeric_test, X_text_test, y, tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6IISwk-c5ePf"
   },
   "outputs": [],
   "source": [
    "X_numeric, X_text, X_test_numeric, X_test_text, y, tokenize = init_prep(data_train, data_test, data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YeEwXBk75ePi"
   },
   "outputs": [],
   "source": [
    "# Save X_test_text and X_test_numeric\n",
    "X_test_text = sparse.csr_matrix(X_test_text)\n",
    "\n",
    "sparse.save_npz('X_test_text.npz', X_test_text)\n",
    "np.savez('X_test_numeric.npz', X_test_numeric)\n",
    "\n",
    "# Pickle tokenize\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenize, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g3wYrZqv5ePp"
   },
   "outputs": [],
   "source": [
    "def prep_for_classification(X_numeric, X_text, y, validation_size=50064):\n",
    "    \"\"\"\n",
    "    Split training data into training and validation sets\n",
    "    \n",
    "    Return pandas dataframe X_train_numeric: training data (features)\n",
    "    Return pandas dataframe X_train_text: training data (features)\n",
    "    Return pandas dataframe X_val_numeric: validation data (features)\n",
    "    Return pandas dataframe X_val_text: validation data (features)\n",
    "    Return pandas dataframe y_train: training data (labels)\n",
    "    Return pandas dataframe y_val: validation data (labels)\n",
    "    \n",
    "    param numpy array X_numeric: numerical feature matrix of test set\n",
    "    param numpy array X_text: text feature matrix for classification model fitting\n",
    "    param numpy array X_numeric_test: numerical feature matrix of test set\n",
    "    param numpy array X_text_test: text feature matrix for classification model fitting\n",
    "    param numpy array y: labels matrix for classification model fitting\n",
    "    \n",
    "    Required Libraries: pandas, sklearn.model_selection\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split into training and development sets\n",
    "    X_train_numeric, X_val_numeric, X_train_text, X_val_text, y_train, y_val = train_test_split(X_numeric, X_text, y, test_size=validation_size, random_state=93)\n",
    "    \n",
    "    return X_train_numeric, X_val_numeric, X_train_text, X_val_text, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "drbaQuel5ePr",
    "outputId": "ee0f9a4d-d079-4e19-9a29-2cdede5e9b82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_numeric (350213, 4)\n",
      "X_train_text (350213, 3804)\n",
      "y_train (350213, 104)\n",
      "X_val_numeric (50064, 4)\n",
      "X_val_text (50064, 3804)\n",
      "y_val (50064, 104)\n"
     ]
    }
   ],
   "source": [
    "X_train_numeric, X_val_numeric, X_train_text, X_val_text, y_train, y_val = prep_for_classification(X_numeric, X_text, y)\n",
    "print('X_train_numeric', X_train_numeric.shape)\n",
    "print('X_train_text', X_train_text.shape)\n",
    "print('y_train', y_train.shape)\n",
    "print('X_val_numeric', X_val_numeric.shape)\n",
    "print('X_val_text', X_val_text.shape)\n",
    "print('y_val', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZhVYZEmx5ePv"
   },
   "outputs": [],
   "source": [
    "# Save prepped data\n",
    "X_train_text = sparse.csr_matrix(X_train_text)\n",
    "X_val_text = sparse.csr_matrix(X_val_text)\n",
    "y_train = sparse.csr_matrix(y_train)\n",
    "y_val = sparse.csr_matrix(y_val)\n",
    "\n",
    "sparse.save_npz('X_train_text.npz', X_train_text)\n",
    "sparse.save_npz('X_val_text.npz', X_val_text)\n",
    "np.savez('X_train_numeric.npz', X_train_numeric)\n",
    "np.savez('X_val_numeric.npz', X_val_numeric)\n",
    "sparse.save_npz('y_train.npz', y_train)\n",
    "sparse.save_npz('y_val.npz', y_val)\n",
    "\n",
    "if google_colab:\n",
    "    files.download('X_train_text.npz')\n",
    "    files.download('X_val_text.npz')\n",
    "    files.download('X_train_numeric.npz')\n",
    "    files.download('X_val_numeric.npz')\n",
    "    files.download('y_train.npz')\n",
    "    files.download('y_val.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Prepped Data\n",
    "Run these cells below if prepped data files are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "hGz1Dw0S5ePx",
    "outputId": "1f2112f6-f12c-4545-cd12-52a1f2b06571"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_numeric (350213, 4)\n",
      "X_train_text (350213, 3804)\n",
      "y_train (350213, 104)\n",
      "X_val_numeric (50064, 4)\n",
      "X_val_text (50064, 3804)\n",
      "y_val (50064, 104)\n",
      "X_test_numeric (50064, 4)\n",
      "X_test_text (50064, 3804)\n"
     ]
    }
   ],
   "source": [
    "X_train_text = sparse.load_npz('X_train_text.npz')\n",
    "X_val_text = sparse.load_npz('X_val_text.npz')\n",
    "X_test_text = sparse.load_npz('X_test_text.npz')\n",
    "X_train_numeric = np.load('X_train_numeric.npz')['arr_0']\n",
    "X_val_numeric = np.load('X_val_numeric.npz')['arr_0']\n",
    "X_test_numeric = np.load('X_test_numeric.npz')['arr_0']\n",
    "y_train = sparse.load_npz('y_train.npz')\n",
    "y_val = sparse.load_npz('y_val.npz')\n",
    "\n",
    "print('X_train_numeric', X_train_numeric.shape)\n",
    "print('X_train_text', X_train_text.shape)\n",
    "print('y_train', y_train.shape)\n",
    "print('X_val_numeric', X_val_numeric.shape)\n",
    "print('X_val_text', X_val_text.shape)\n",
    "print('y_val', y_val.shape)\n",
    "print('X_test_numeric', X_test_numeric.shape)\n",
    "print('X_test_text', X_test_text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UfppW7oi5eP0"
   },
   "source": [
    "# Build Model\n",
    "Run cells below if model has not been fitted yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GtnM3_nE5eP0"
   },
   "outputs": [],
   "source": [
    "def build_network(X_numeric=X_train_numeric, X_text=X_train_text, y=y_train):\n",
    "    \"\"\"\n",
    "    Return compiled keras-model model\n",
    "    \n",
    "    param numpy array X_numeric: feature matrix for classification\n",
    "    param numpy array X_text: feature matrix for classification\n",
    "    param numpy array y: labels matrix for classification\n",
    "    \n",
    "    Required Libraries: keras\n",
    "    \"\"\"\n",
    "    \n",
    "    numeric_input = Input(shape=(X_numeric.shape[1],) , name='numeric_input') \n",
    "    text_input = Input(shape=(X_text.shape[1],) , name='text_input')\n",
    "    \n",
    "    dropout_value = 0.5\n",
    "    \n",
    "    # Function\n",
    "    text_function_hidden_layer = Dense(512, activation='relu')(text_input)\n",
    "    text_function_hidden_layer = Dropout(dropout_value)(text_function_hidden_layer)\n",
    "    text_function_hidden_layer = Dense(256, activation='relu')(text_function_hidden_layer)\n",
    "    text_function_hidden_layer = Dropout(dropout_value)(text_function_hidden_layer)\n",
    "    text_function_hidden_layer = Dense(128, activation='relu')(text_function_hidden_layer)\n",
    "    text_function_hidden_layer = Dropout(dropout_value)(text_function_hidden_layer)\n",
    "    text_function_hidden_layer = Dense(64, activation='relu')(text_function_hidden_layer)\n",
    "    text_function_hidden_layer = Dropout(dropout_value)(text_function_hidden_layer)\n",
    "    numeric_function_hidden_layer = Dense(4, activation='relu')(numeric_input)\n",
    "    numeric_function_hidden_layer = Dropout(dropout_value)(numeric_function_hidden_layer)\n",
    "    combined_function_layer = concatenate([numeric_function_hidden_layer, text_function_hidden_layer])\n",
    "    function_output_layer = Dense(37, activation='softmax')(combined_function_layer)\n",
    "    \n",
    "    # Object_Type\n",
    "    text_object_type_hidden_layer = Dense(512, activation='relu')(text_input)\n",
    "    text_object_type_hidden_layer = Dropout(dropout_value)(text_object_type_hidden_layer)\n",
    "    text_object_type_hidden_layer = Dense(256, activation='relu')(text_object_type_hidden_layer)\n",
    "    text_object_type_hidden_layer = Dropout(dropout_value)(text_object_type_hidden_layer)\n",
    "    text_object_type_hidden_layer = Dense(128, activation='relu')(text_object_type_hidden_layer)\n",
    "    text_object_type_hidden_layer = Dropout(dropout_value)(text_object_type_hidden_layer)\n",
    "    text_object_type_hidden_layer = Dense(64, activation='relu')(text_object_type_hidden_layer)\n",
    "    text_object_type_hidden_layer = Dropout(dropout_value)(text_object_type_hidden_layer)\n",
    "    text_object_type_hidden_layer = Dense(32, activation='relu')(text_object_type_hidden_layer)\n",
    "    text_object_type_hidden_layer = Dropout(dropout_value)(text_object_type_hidden_layer)\n",
    "    text_object_type_hidden_layer = Dense(16, activation='relu')(text_object_type_hidden_layer)\n",
    "    text_object_type_hidden_layer = Dropout(dropout_value)(text_object_type_hidden_layer)\n",
    "    numeric_object_type_hidden_layer = Dense(4, activation='relu')(numeric_input)\n",
    "    numeric_object_type_hidden_layer = Dropout(dropout_value)(numeric_object_type_hidden_layer)\n",
    "    combined_object_type_layer = concatenate([numeric_object_type_hidden_layer, text_object_type_hidden_layer])\n",
    "    object_type_output_layer = Dense(11, activation='softmax')(combined_object_type_layer)\n",
    "    \n",
    "    # Operating_Status\n",
    "    text_operating_status_hidden_layer = Dense(512, activation='relu')(text_input)\n",
    "    text_operating_status_hidden_layer = Dropout(dropout_value)(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dense(256, activation='relu')(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dropout(dropout_value)(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dense(128, activation='relu')(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dropout(dropout_value)(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dense(64, activation='relu')(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dropout(dropout_value)(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dense(32, activation='relu')(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dropout(dropout_value)(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dense(16, activation='relu')(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dropout(dropout_value)(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dense(8, activation='relu')(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dropout(dropout_value)(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dense(4, activation='relu')(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dropout(dropout_value)(text_operating_status_hidden_layer)\n",
    "    numeric_operating_status_hidden_layer = Dense(4, activation='relu')(numeric_input)\n",
    "    numeric_operating_status_hidden_layer = Dropout(dropout_value)(numeric_operating_status_hidden_layer)\n",
    "    combined_operating_status_layer = concatenate([numeric_operating_status_hidden_layer, text_operating_status_hidden_layer])\n",
    "    operating_status_output_layer = Dense(3, activation='softmax')(combined_operating_status_layer)\n",
    "    \n",
    "    # Position_Type\n",
    "    text_position_type_hidden_layer = Dense(512, activation='relu')(text_input)\n",
    "    text_position_type_hidden_layer = Dropout(dropout_value)(text_position_type_hidden_layer)\n",
    "    text_position_type_hidden_layer = Dense(256, activation='relu')(text_position_type_hidden_layer)\n",
    "    text_position_type_hidden_layer = Dropout(dropout_value)(text_position_type_hidden_layer)\n",
    "    text_position_type_hidden_layer = Dense(128, activation='relu')(text_position_type_hidden_layer)\n",
    "    text_position_type_hidden_layer = Dropout(dropout_value)(text_position_type_hidden_layer)\n",
    "    text_position_type_hidden_layer = Dense(64, activation='relu')(text_position_type_hidden_layer)\n",
    "    text_position_type_hidden_layer = Dropout(dropout_value)(text_position_type_hidden_layer)\n",
    "    text_position_type_hidden_layer = Dense(32, activation='relu')(text_position_type_hidden_layer)\n",
    "    text_position_type_hidden_layer = Dropout(dropout_value)(text_position_type_hidden_layer)\n",
    "    numeric_position_type_hidden_layer = Dense(4, activation='relu')(numeric_input)\n",
    "    numeric_position_type_hidden_layer = Dropout(dropout_value)(numeric_position_type_hidden_layer)\n",
    "    combined_position_type_layer = concatenate([numeric_position_type_hidden_layer, text_position_type_hidden_layer])\n",
    "    position_type_output_layer = Dense(25, activation='softmax')(combined_position_type_layer)\n",
    "    \n",
    "    # Pre_K\n",
    "    text_pre_k_hidden_layer = Dense(512, activation='relu')(text_input)\n",
    "    text_pre_k_hidden_layer = Dropout(dropout_value)(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dense(256, activation='relu')(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dropout(dropout_value)(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dense(128, activation='relu')(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dropout(dropout_value)(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dense(64, activation='relu')(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dropout(dropout_value)(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dense(32, activation='relu')(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dropout(dropout_value)(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dense(16, activation='relu')(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dropout(dropout_value)(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dense(8, activation='relu')(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dropout(dropout_value)(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dense(4, activation='relu')(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dropout(dropout_value)(text_pre_k_hidden_layer)\n",
    "    numeric_pre_k_hidden_layer = Dense(4, activation='relu')(numeric_input)\n",
    "    numeric_pre_k_hidden_layer = Dropout(dropout_value)(numeric_pre_k_hidden_layer)\n",
    "    combined_pre_k_layer = concatenate([numeric_pre_k_hidden_layer, text_pre_k_hidden_layer])\n",
    "    pre_k_output_layer = Dense(3, activation='softmax')(combined_pre_k_layer)\n",
    "    \n",
    "    # Reporting\n",
    "    text_reporting_hidden_layer = Dense(512, activation='relu')(text_input)\n",
    "    text_reporting_hidden_layer = Dropout(dropout_value)(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dense(256, activation='relu')(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dropout(dropout_value)(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dense(128, activation='relu')(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dropout(dropout_value)(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dense(64, activation='relu')(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dropout(dropout_value)(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dense(32, activation='relu')(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dropout(dropout_value)(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dense(16, activation='relu')(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dropout(dropout_value)(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dense(8, activation='relu')(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dropout(dropout_value)(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dense(4, activation='relu')(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dropout(dropout_value)(text_reporting_hidden_layer)\n",
    "    numeric_reporting_hidden_layer = Dense(4, activation='relu')(numeric_input)\n",
    "    numeric_reporting_hidden_layer = Dropout(dropout_value)(numeric_reporting_hidden_layer)\n",
    "    combined_reporting_layer = concatenate([numeric_reporting_hidden_layer, text_reporting_hidden_layer])\n",
    "    reporting_output_layer = Dense(3, activation='softmax')(combined_reporting_layer)\n",
    "    \n",
    "    # Sharing\n",
    "    text_sharing_hidden_layer = Dense(512, activation='relu')(text_input)\n",
    "    text_sharing_hidden_layer = Dropout(dropout_value)(text_sharing_hidden_layer)\n",
    "    text_sharing_hidden_layer = Dense(256, activation='relu')(text_sharing_hidden_layer)\n",
    "    text_sharing_hidden_layer = Dropout(dropout_value)(text_sharing_hidden_layer)\n",
    "    text_sharing_hidden_layer = Dense(128, activation='relu')(text_sharing_hidden_layer)\n",
    "    text_sharing_hidden_layer = Dropout(dropout_value)(text_sharing_hidden_layer)\n",
    "    text_sharing_hidden_layer = Dense(64, activation='relu')(text_sharing_hidden_layer)\n",
    "    text_sharing_hidden_layer = Dropout(dropout_value)(text_sharing_hidden_layer)\n",
    "    text_sharing_hidden_layer = Dense(32, activation='relu')(text_sharing_hidden_layer)\n",
    "    text_sharing_hidden_layer = Dropout(dropout_value)(text_sharing_hidden_layer)\n",
    "    text_sharing_hidden_layer = Dense(16, activation='relu')(text_sharing_hidden_layer)\n",
    "    text_sharing_hidden_layer = Dropout(dropout_value)(text_sharing_hidden_layer)\n",
    "    text_sharing_hidden_layer = Dense(8, activation='relu')(text_sharing_hidden_layer)\n",
    "    text_sharing_hidden_layer = Dropout(dropout_value)(text_sharing_hidden_layer)\n",
    "    numeric_sharing_hidden_layer = Dense(4, activation='relu')(numeric_input)\n",
    "    numeric_sharing_hidden_layer = Dropout(dropout_value)(numeric_sharing_hidden_layer)\n",
    "    combined_sharing_layer = concatenate([numeric_sharing_hidden_layer, text_sharing_hidden_layer])\n",
    "    sharing_output_layer = Dense(5, activation='softmax')(combined_sharing_layer)\n",
    "    \n",
    "    # Student_Type\n",
    "    text_student_type_hidden_layer = Dense(512, activation='relu')(text_input)\n",
    "    text_student_type_hidden_layer = Dropout(dropout_value)(text_student_type_hidden_layer)\n",
    "    text_student_type_hidden_layer = Dense(256, activation='relu')(text_student_type_hidden_layer)\n",
    "    text_student_type_hidden_layer = Dropout(dropout_value)(text_student_type_hidden_layer)\n",
    "    text_student_type_hidden_layer = Dense(128, activation='relu')(text_student_type_hidden_layer)\n",
    "    text_student_type_hidden_layer = Dropout(dropout_value)(text_student_type_hidden_layer)\n",
    "    text_student_type_hidden_layer = Dense(64, activation='relu')(text_student_type_hidden_layer)\n",
    "    text_student_type_hidden_layer = Dropout(dropout_value)(text_student_type_hidden_layer)\n",
    "    text_student_type_hidden_layer = Dense(32, activation='relu')(text_student_type_hidden_layer)\n",
    "    text_student_type_hidden_layer = Dropout(dropout_value)(text_student_type_hidden_layer)\n",
    "    text_student_type_hidden_layer = Dense(16, activation='relu')(text_student_type_hidden_layer)\n",
    "    text_student_type_hidden_layer = Dropout(dropout_value)(text_student_type_hidden_layer)\n",
    "    numeric_student_type_hidden_layer = Dense(4, activation='relu')(numeric_input)\n",
    "    numeric_student_type_hidden_layer = Dropout(dropout_value)(numeric_student_type_hidden_layer)\n",
    "    combined_student_type_layer = concatenate([numeric_student_type_hidden_layer, text_student_type_hidden_layer])\n",
    "    student_type_output_layer = Dense(9, activation='softmax')(combined_student_type_layer)\n",
    "    \n",
    "    # Use\n",
    "    text_use_hidden_layer = Dense(512, activation='relu')(text_input)\n",
    "    text_use_hidden_layer = Dropout(dropout_value)(text_use_hidden_layer)\n",
    "    text_use_hidden_layer = Dense(256, activation='relu')(text_use_hidden_layer)\n",
    "    text_use_hidden_layer = Dropout(dropout_value)(text_use_hidden_layer)\n",
    "    text_use_hidden_layer = Dense(128, activation='relu')(text_use_hidden_layer)\n",
    "    text_use_hidden_layer = Dropout(dropout_value)(text_use_hidden_layer)\n",
    "    text_use_hidden_layer = Dense(64, activation='relu')(text_use_hidden_layer)\n",
    "    text_use_hidden_layer = Dropout(dropout_value)(text_use_hidden_layer)\n",
    "    text_use_hidden_layer = Dense(32, activation='relu')(text_use_hidden_layer)\n",
    "    text_use_hidden_layer = Dropout(dropout_value)(text_use_hidden_layer)\n",
    "    text_use_hidden_layer = Dense(16, activation='relu')(text_use_hidden_layer)\n",
    "    text_use_hidden_layer = Dropout(dropout_value)(text_use_hidden_layer)\n",
    "    numeric_use_hidden_layer = Dense(4, activation='relu')(numeric_input)\n",
    "    numeric_use_hidden_layer = Dropout(dropout_value)(numeric_use_hidden_layer)\n",
    "    combined_use_layer = concatenate([numeric_use_hidden_layer, text_use_hidden_layer])\n",
    "    use_output_layer = Dense(8, activation='softmax')(combined_use_layer)\n",
    "    \n",
    "    # Output\n",
    "    combined_output_layer = concatenate([function_output_layer, \n",
    "                                         object_type_output_layer,\n",
    "                                         operating_status_output_layer,\n",
    "                                         position_type_output_layer,\n",
    "                                         pre_k_output_layer,\n",
    "                                         reporting_output_layer,\n",
    "                                         sharing_output_layer,\n",
    "                                         student_type_output_layer,\n",
    "                                         use_output_layer])\n",
    "    \n",
    "    model = Model(inputs=[numeric_input, text_input], outputs=[combined_output_layer])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vkm6VVkF5eP2"
   },
   "outputs": [],
   "source": [
    "basic_NN = build_network()\n",
    "\n",
    "# Save model architecture\n",
    "model_json = basic_NN.to_json()\n",
    "with open('basic_NN_model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "INIHpqTn5eP4"
   },
   "source": [
    "# Train model and generate predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6834
    },
    "colab_type": "code",
    "id": "h2fWentL1If6",
    "outputId": "b06299a6-9196-4381-9136-d15f87ee7b15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 315191 samples, validate on 35022 samples\n",
      "Epoch 1/100\n",
      "315191/315191 [==============================] - 40s 128us/step - loss: 77.1327 - acc: 0.2301 - val_loss: 70.7497 - val_acc: 0.4936\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 70.74975, saving model to basic_NN_model.h5\n",
      "Epoch 2/100\n",
      "315191/315191 [==============================] - 32s 101us/step - loss: 50.2581 - acc: 0.5303 - val_loss: 45.6662 - val_acc: 0.6183\n",
      "\n",
      "Epoch 00002: val_loss improved from 70.74975 to 45.66624, saving model to basic_NN_model.h5\n",
      "Epoch 3/100\n",
      "315191/315191 [==============================] - 32s 100us/step - loss: 40.2172 - acc: 0.5931 - val_loss: 37.5293 - val_acc: 0.6463\n",
      "\n",
      "Epoch 00003: val_loss improved from 45.66624 to 37.52931, saving model to basic_NN_model.h5\n",
      "Epoch 4/100\n",
      "315191/315191 [==============================] - 32s 101us/step - loss: 36.0730 - acc: 0.6235 - val_loss: 33.4429 - val_acc: 0.6602\n",
      "\n",
      "Epoch 00004: val_loss improved from 37.52931 to 33.44286, saving model to basic_NN_model.h5\n",
      "Epoch 5/100\n",
      "315191/315191 [==============================] - 32s 100us/step - loss: 32.6547 - acc: 0.6307 - val_loss: 30.7365 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00005: val_loss improved from 33.44286 to 30.73650, saving model to basic_NN_model.h5\n",
      "Epoch 6/100\n",
      "315191/315191 [==============================] - 32s 101us/step - loss: 30.5739 - acc: 0.6353 - val_loss: 29.7128 - val_acc: 0.6738\n",
      "\n",
      "Epoch 00006: val_loss improved from 30.73650 to 29.71276, saving model to basic_NN_model.h5\n",
      "Epoch 7/100\n",
      "315191/315191 [==============================] - 32s 100us/step - loss: 29.6550 - acc: 0.6381 - val_loss: 28.6791 - val_acc: 0.6796\n",
      "\n",
      "Epoch 00007: val_loss improved from 29.71276 to 28.67910, saving model to basic_NN_model.h5\n",
      "Epoch 8/100\n",
      "315191/315191 [==============================] - 32s 101us/step - loss: 28.3369 - acc: 0.6397 - val_loss: 27.5250 - val_acc: 0.6827\n",
      "\n",
      "Epoch 00008: val_loss improved from 28.67910 to 27.52499, saving model to basic_NN_model.h5\n",
      "Epoch 9/100\n",
      "315191/315191 [==============================] - 32s 100us/step - loss: 27.7703 - acc: 0.6439 - val_loss: 27.1453 - val_acc: 0.6833\n",
      "\n",
      "Epoch 00009: val_loss improved from 27.52499 to 27.14525, saving model to basic_NN_model.h5\n",
      "Epoch 10/100\n",
      "315191/315191 [==============================] - 32s 101us/step - loss: 27.3685 - acc: 0.6445 - val_loss: 26.8512 - val_acc: 0.6852\n",
      "\n",
      "Epoch 00010: val_loss improved from 27.14525 to 26.85125, saving model to basic_NN_model.h5\n",
      "Epoch 11/100\n",
      "315191/315191 [==============================] - 32s 101us/step - loss: 27.0994 - acc: 0.6419 - val_loss: 26.5535 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00011: val_loss improved from 26.85125 to 26.55351, saving model to basic_NN_model.h5\n",
      "Epoch 12/100\n",
      "315191/315191 [==============================] - 32s 100us/step - loss: 26.8655 - acc: 0.6318 - val_loss: 26.3068 - val_acc: 0.6629\n",
      "\n",
      "Epoch 00012: val_loss improved from 26.55351 to 26.30681, saving model to basic_NN_model.h5\n",
      "Epoch 13/100\n",
      "315191/315191 [==============================] - 32s 101us/step - loss: 26.5700 - acc: 0.6202 - val_loss: 25.9439 - val_acc: 0.6477\n",
      "\n",
      "Epoch 00013: val_loss improved from 26.30681 to 25.94393, saving model to basic_NN_model.h5\n",
      "Epoch 14/100\n",
      "315191/315191 [==============================] - 32s 100us/step - loss: 26.2566 - acc: 0.5817 - val_loss: 25.7357 - val_acc: 0.5849\n",
      "\n",
      "Epoch 00014: val_loss improved from 25.94393 to 25.73571, saving model to basic_NN_model.h5\n",
      "Epoch 15/100\n",
      "315191/315191 [==============================] - 32s 100us/step - loss: 26.0946 - acc: 0.5386 - val_loss: 25.4489 - val_acc: 0.5668\n",
      "\n",
      "Epoch 00015: val_loss improved from 25.73571 to 25.44888, saving model to basic_NN_model.h5\n",
      "Epoch 16/100\n",
      "315191/315191 [==============================] - 32s 101us/step - loss: 25.8783 - acc: 0.5174 - val_loss: 25.3579 - val_acc: 0.5525\n",
      "\n",
      "Epoch 00016: val_loss improved from 25.44888 to 25.35794, saving model to basic_NN_model.h5\n",
      "Epoch 17/100\n",
      "315191/315191 [==============================] - 31s 100us/step - loss: 25.8133 - acc: 0.5042 - val_loss: 25.2744 - val_acc: 0.5410\n",
      "\n",
      "Epoch 00017: val_loss improved from 25.35794 to 25.27437, saving model to basic_NN_model.h5\n",
      "Epoch 18/100\n",
      "315191/315191 [==============================] - 32s 101us/step - loss: 25.6408 - acc: 0.4947 - val_loss: 25.1197 - val_acc: 0.5390\n",
      "\n",
      "Epoch 00018: val_loss improved from 25.27437 to 25.11970, saving model to basic_NN_model.h5\n",
      "Epoch 19/100\n",
      "315191/315191 [==============================] - 32s 100us/step - loss: 25.4881 - acc: 0.4769 - val_loss: 25.0495 - val_acc: 0.5016\n",
      "\n",
      "Epoch 00019: val_loss improved from 25.11970 to 25.04946, saving model to basic_NN_model.h5\n",
      "Epoch 20/100\n",
      "315191/315191 [==============================] - 32s 101us/step - loss: 25.4287 - acc: 0.4619 - val_loss: 24.8153 - val_acc: 0.4904\n",
      "\n",
      "Epoch 00020: val_loss improved from 25.04946 to 24.81532, saving model to basic_NN_model.h5\n",
      "Epoch 21/100\n",
      "315191/315191 [==============================] - 32s 100us/step - loss: 25.2072 - acc: 0.4557 - val_loss: 24.7783 - val_acc: 0.4906\n",
      "\n",
      "Epoch 00021: val_loss improved from 24.81532 to 24.77826, saving model to basic_NN_model.h5\n",
      "Epoch 22/100\n",
      "315191/315191 [==============================] - 32s 101us/step - loss: 25.0420 - acc: 0.4465 - val_loss: 24.6784 - val_acc: 0.4588\n",
      "\n",
      "Epoch 00022: val_loss improved from 24.77826 to 24.67844, saving model to basic_NN_model.h5\n",
      "Epoch 23/100\n",
      "315191/315191 [==============================] - 32s 100us/step - loss: 24.9550 - acc: 0.4347 - val_loss: 24.5369 - val_acc: 0.4546\n",
      "\n",
      "Epoch 00023: val_loss improved from 24.67844 to 24.53694, saving model to basic_NN_model.h5\n",
      "Epoch 24/100\n",
      "315191/315191 [==============================] - 32s 100us/step - loss: 24.8772 - acc: 0.4246 - val_loss: 24.5969 - val_acc: 0.4508\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 24.53694\n",
      "Epoch 25/100\n",
      "315191/315191 [==============================] - 32s 101us/step - loss: 24.8487 - acc: 0.4184 - val_loss: 24.4626 - val_acc: 0.4426\n",
      "\n",
      "Epoch 00025: val_loss improved from 24.53694 to 24.46256, saving model to basic_NN_model.h5\n",
      "Epoch 26/100\n",
      "315191/315191 [==============================] - 31s 100us/step - loss: 24.7607 - acc: 0.4147 - val_loss: 24.3308 - val_acc: 0.4409\n",
      "\n",
      "Epoch 00026: val_loss improved from 24.46256 to 24.33079, saving model to basic_NN_model.h5\n",
      "Epoch 27/100\n",
      "315191/315191 [==============================] - 32s 101us/step - loss: 24.6469 - acc: 0.4088 - val_loss: 24.3367 - val_acc: 0.4296\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 24.33079\n",
      "Epoch 28/100\n",
      "315191/315191 [==============================] - 32s 100us/step - loss: 24.5412 - acc: 0.4009 - val_loss: 24.2262 - val_acc: 0.4450\n",
      "\n",
      "Epoch 00028: val_loss improved from 24.33079 to 24.22618, saving model to basic_NN_model.h5\n",
      "Epoch 29/100\n",
      "315191/315191 [==============================] - 32s 101us/step - loss: 24.6227 - acc: 0.4001 - val_loss: 24.1082 - val_acc: 0.4367\n",
      "\n",
      "Epoch 00029: val_loss improved from 24.22618 to 24.10819, saving model to basic_NN_model.h5\n",
      "Epoch 30/100\n",
      "315191/315191 [==============================] - 32s 100us/step - loss: 24.7664 - acc: 0.3969 - val_loss: 24.0759 - val_acc: 0.4354\n",
      "\n",
      "Epoch 00030: val_loss improved from 24.10819 to 24.07586, saving model to basic_NN_model.h5\n",
      "Epoch 31/100\n",
      "315191/315191 [==============================] - 32s 100us/step - loss: 24.5384 - acc: 0.3924 - val_loss: 23.8102 - val_acc: 0.4255\n",
      "\n",
      "Epoch 00031: val_loss improved from 24.07586 to 23.81016, saving model to basic_NN_model.h5\n",
      "Epoch 32/100\n",
      "315191/315191 [==============================] - 32s 100us/step - loss: 24.5594 - acc: 0.3881 - val_loss: 23.7440 - val_acc: 0.4531\n",
      "\n",
      "Epoch 00032: val_loss improved from 23.81016 to 23.74405, saving model to basic_NN_model.h5\n",
      "Epoch 33/100\n",
      "315191/315191 [==============================] - 32s 100us/step - loss: 24.4608 - acc: 0.3870 - val_loss: 23.5890 - val_acc: 0.4479\n",
      "\n",
      "Epoch 00033: val_loss improved from 23.74405 to 23.58904, saving model to basic_NN_model.h5\n",
      "Epoch 34/100\n",
      "315191/315191 [==============================] - 32s 100us/step - loss: 24.3867 - acc: 0.3895 - val_loss: 23.5800 - val_acc: 0.4214\n",
      "\n",
      "Epoch 00034: val_loss improved from 23.58904 to 23.58001, saving model to basic_NN_model.h5\n",
      "Epoch 35/100\n",
      "315191/315191 [==============================] - 31s 100us/step - loss: 24.3646 - acc: 0.3859 - val_loss: 23.5492 - val_acc: 0.4289\n",
      "\n",
      "Epoch 00035: val_loss improved from 23.58001 to 23.54921, saving model to basic_NN_model.h5\n",
      "Epoch 36/100\n",
      "315191/315191 [==============================] - 32s 101us/step - loss: 24.3077 - acc: 0.3838 - val_loss: 23.4815 - val_acc: 0.4186\n",
      "\n",
      "Epoch 00036: val_loss improved from 23.54921 to 23.48151, saving model to basic_NN_model.h5\n",
      "Epoch 37/100\n",
      "315191/315191 [==============================] - 31s 100us/step - loss: 24.2742 - acc: 0.3821 - val_loss: 23.3549 - val_acc: 0.4334\n",
      "\n",
      "Epoch 00037: val_loss improved from 23.48151 to 23.35487, saving model to basic_NN_model.h5\n",
      "Epoch 38/100\n",
      "315191/315191 [==============================] - 32s 100us/step - loss: 24.1726 - acc: 0.3782 - val_loss: 22.5470 - val_acc: 0.4404\n",
      "\n",
      "Epoch 00038: val_loss improved from 23.35487 to 22.54696, saving model to basic_NN_model.h5\n",
      "Epoch 39/100\n",
      "315191/315191 [==============================] - 31s 100us/step - loss: 24.1462 - acc: 0.3752 - val_loss: 22.9996 - val_acc: 0.4533\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 22.54696\n",
      "Epoch 40/100\n",
      "315191/315191 [==============================] - 32s 100us/step - loss: 24.1018 - acc: 0.3771 - val_loss: 22.7359 - val_acc: 0.4472\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 22.54696\n",
      "Epoch 41/100\n",
      "315191/315191 [==============================] - 32s 100us/step - loss: 23.9665 - acc: 0.3741 - val_loss: 22.7370 - val_acc: 0.4214\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 22.54696\n",
      "Epoch 42/100\n",
      "315191/315191 [==============================] - 32s 100us/step - loss: 23.9464 - acc: 0.3735 - val_loss: 22.6753 - val_acc: 0.4533\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 22.54696\n",
      "Epoch 43/100\n",
      "315191/315191 [==============================] - 32s 100us/step - loss: 23.8671 - acc: 0.3820 - val_loss: 22.4790 - val_acc: 0.4396\n",
      "\n",
      "Epoch 00043: val_loss improved from 22.54696 to 22.47895, saving model to basic_NN_model.h5\n",
      "Epoch 44/100\n",
      "315191/315191 [==============================] - 32s 100us/step - loss: 23.8190 - acc: 0.3746 - val_loss: 22.9732 - val_acc: 0.4400\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 22.47895\n",
      "Epoch 45/100\n",
      "315191/315191 [==============================] - 31s 100us/step - loss: 23.7843 - acc: 0.3775 - val_loss: 22.4063 - val_acc: 0.4344\n",
      "\n",
      "Epoch 00045: val_loss improved from 22.47895 to 22.40629, saving model to basic_NN_model.h5\n",
      "Epoch 46/100\n",
      "315191/315191 [==============================] - 32s 101us/step - loss: 23.6711 - acc: 0.3726 - val_loss: 22.3260 - val_acc: 0.4320\n",
      "\n",
      "Epoch 00046: val_loss improved from 22.40629 to 22.32601, saving model to basic_NN_model.h5\n",
      "Epoch 47/100\n",
      "315191/315191 [==============================] - 31s 99us/step - loss: 23.6073 - acc: 0.3647 - val_loss: 22.2766 - val_acc: 0.4239\n",
      "\n",
      "Epoch 00047: val_loss improved from 22.32601 to 22.27659, saving model to basic_NN_model.h5\n",
      "Epoch 48/100\n",
      "315191/315191 [==============================] - 31s 99us/step - loss: 23.5342 - acc: 0.3697 - val_loss: 22.3015 - val_acc: 0.4216\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 22.27659\n",
      "Epoch 49/100\n",
      "315191/315191 [==============================] - 32s 100us/step - loss: 23.5029 - acc: 0.3756 - val_loss: 22.3647 - val_acc: 0.4118\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 22.27659\n",
      "Epoch 50/100\n",
      "315191/315191 [==============================] - 31s 100us/step - loss: 23.4231 - acc: 0.3618 - val_loss: 22.2370 - val_acc: 0.3904\n",
      "\n",
      "Epoch 00050: val_loss improved from 22.27659 to 22.23698, saving model to basic_NN_model.h5\n",
      "Epoch 51/100\n",
      "315191/315191 [==============================] - 31s 100us/step - loss: 23.3418 - acc: 0.3656 - val_loss: 22.2482 - val_acc: 0.3972\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 22.23698\n",
      "Epoch 52/100\n",
      "315191/315191 [==============================] - 31s 100us/step - loss: 23.1714 - acc: 0.3654 - val_loss: 22.1735 - val_acc: 0.4167\n",
      "\n",
      "Epoch 00052: val_loss improved from 22.23698 to 22.17347, saving model to basic_NN_model.h5\n",
      "Epoch 53/100\n",
      "315191/315191 [==============================] - 31s 100us/step - loss: 22.9023 - acc: 0.3615 - val_loss: 22.1266 - val_acc: 0.3837\n",
      "\n",
      "Epoch 00053: val_loss improved from 22.17347 to 22.12656, saving model to basic_NN_model.h5\n",
      "Epoch 54/100\n",
      "315191/315191 [==============================] - 31s 100us/step - loss: 22.7528 - acc: 0.3672 - val_loss: 22.1520 - val_acc: 0.4272\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 22.12656\n",
      "Epoch 55/100\n",
      "315191/315191 [==============================] - 31s 100us/step - loss: 22.6826 - acc: 0.3642 - val_loss: 22.1135 - val_acc: 0.3964\n",
      "\n",
      "Epoch 00055: val_loss improved from 22.12656 to 22.11347, saving model to basic_NN_model.h5\n",
      "Epoch 56/100\n",
      "315191/315191 [==============================] - 31s 99us/step - loss: 22.6647 - acc: 0.3649 - val_loss: 21.9982 - val_acc: 0.3830\n",
      "\n",
      "Epoch 00056: val_loss improved from 22.11347 to 21.99824, saving model to basic_NN_model.h5\n",
      "Epoch 57/100\n",
      "315191/315191 [==============================] - 32s 100us/step - loss: 22.6092 - acc: 0.3610 - val_loss: 21.9392 - val_acc: 0.4085\n",
      "\n",
      "Epoch 00057: val_loss improved from 21.99824 to 21.93916, saving model to basic_NN_model.h5\n",
      "Epoch 58/100\n",
      "315191/315191 [==============================] - 31s 99us/step - loss: 22.6034 - acc: 0.3657 - val_loss: 21.9915 - val_acc: 0.4154\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 21.93916\n",
      "Epoch 59/100\n",
      "315191/315191 [==============================] - 31s 100us/step - loss: 22.5899 - acc: 0.3659 - val_loss: 21.9288 - val_acc: 0.3835\n",
      "\n",
      "Epoch 00059: val_loss improved from 21.93916 to 21.92883, saving model to basic_NN_model.h5\n",
      "Epoch 60/100\n",
      "315191/315191 [==============================] - 31s 100us/step - loss: 22.5033 - acc: 0.3604 - val_loss: 21.8631 - val_acc: 0.3919\n",
      "\n",
      "Epoch 00060: val_loss improved from 21.92883 to 21.86313, saving model to basic_NN_model.h5\n",
      "Epoch 61/100\n",
      "315191/315191 [==============================] - 31s 100us/step - loss: 22.4682 - acc: 0.3623 - val_loss: 21.8572 - val_acc: 0.3918\n",
      "\n",
      "Epoch 00061: val_loss improved from 21.86313 to 21.85724, saving model to basic_NN_model.h5\n",
      "Epoch 62/100\n",
      "315191/315191 [==============================] - 31s 100us/step - loss: 22.4816 - acc: 0.3586 - val_loss: 21.8461 - val_acc: 0.3661\n",
      "\n",
      "Epoch 00062: val_loss improved from 21.85724 to 21.84607, saving model to basic_NN_model.h5\n",
      "Epoch 63/100\n",
      "315191/315191 [==============================] - 32s 102us/step - loss: 22.4307 - acc: 0.3571 - val_loss: 21.8262 - val_acc: 0.3733\n",
      "\n",
      "Epoch 00063: val_loss improved from 21.84607 to 21.82618, saving model to basic_NN_model.h5\n",
      "Epoch 64/100\n",
      "315191/315191 [==============================] - 32s 100us/step - loss: 22.4223 - acc: 0.3579 - val_loss: 21.8381 - val_acc: 0.4065\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 21.82618\n",
      "Epoch 65/100\n",
      "315191/315191 [==============================] - 31s 99us/step - loss: 22.4094 - acc: 0.3643 - val_loss: 21.8335 - val_acc: 0.3888\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 21.82618\n",
      "Epoch 66/100\n",
      "315191/315191 [==============================] - 32s 100us/step - loss: 22.3922 - acc: 0.3646 - val_loss: 21.8144 - val_acc: 0.3811\n",
      "\n",
      "Epoch 00066: val_loss improved from 21.82618 to 21.81444, saving model to basic_NN_model.h5\n",
      "Epoch 67/100\n",
      "315191/315191 [==============================] - 31s 100us/step - loss: 22.3744 - acc: 0.3649 - val_loss: 21.8015 - val_acc: 0.3910\n",
      "\n",
      "Epoch 00067: val_loss improved from 21.81444 to 21.80155, saving model to basic_NN_model.h5\n",
      "Epoch 68/100\n",
      "315191/315191 [==============================] - 31s 100us/step - loss: 22.3679 - acc: 0.3613 - val_loss: 21.8003 - val_acc: 0.3783\n",
      "\n",
      "Epoch 00068: val_loss improved from 21.80155 to 21.80030, saving model to basic_NN_model.h5\n",
      "Epoch 69/100\n",
      "315191/315191 [==============================] - 32s 100us/step - loss: 22.4138 - acc: 0.3717 - val_loss: 21.8024 - val_acc: 0.3865\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 21.80030\n",
      "Epoch 70/100\n",
      "315191/315191 [==============================] - 31s 99us/step - loss: 22.3627 - acc: 0.3675 - val_loss: 21.7894 - val_acc: 0.3850\n",
      "\n",
      "Epoch 00070: val_loss improved from 21.80030 to 21.78938, saving model to basic_NN_model.h5\n",
      "Epoch 71/100\n",
      "315191/315191 [==============================] - 31s 100us/step - loss: 22.3620 - acc: 0.3632 - val_loss: 21.8077 - val_acc: 0.3745\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 21.78938\n",
      "Epoch 72/100\n",
      "315191/315191 [==============================] - 31s 99us/step - loss: 22.3634 - acc: 0.3747 - val_loss: 21.7858 - val_acc: 0.3973\n",
      "\n",
      "Epoch 00072: val_loss improved from 21.78938 to 21.78580, saving model to basic_NN_model.h5\n",
      "Epoch 73/100\n",
      "315191/315191 [==============================] - 31s 100us/step - loss: 22.3329 - acc: 0.3725 - val_loss: 21.7887 - val_acc: 0.3980\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 21.78580\n",
      "Epoch 74/100\n",
      "315191/315191 [==============================] - 32s 100us/step - loss: 22.3358 - acc: 0.3681 - val_loss: 21.7801 - val_acc: 0.3819\n",
      "\n",
      "Epoch 00074: val_loss improved from 21.78580 to 21.78005, saving model to basic_NN_model.h5\n",
      "Epoch 75/100\n",
      "315191/315191 [==============================] - 31s 99us/step - loss: 22.3502 - acc: 0.3720 - val_loss: 21.7702 - val_acc: 0.3979\n",
      "\n",
      "Epoch 00075: val_loss improved from 21.78005 to 21.77015, saving model to basic_NN_model.h5\n",
      "Epoch 76/100\n",
      "315191/315191 [==============================] - 32s 100us/step - loss: 22.3116 - acc: 0.3701 - val_loss: 21.7736 - val_acc: 0.4027\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 21.77015\n",
      "Epoch 77/100\n",
      "315191/315191 [==============================] - 31s 100us/step - loss: 22.3028 - acc: 0.3694 - val_loss: 21.7584 - val_acc: 0.3950\n",
      "\n",
      "Epoch 00077: val_loss improved from 21.77015 to 21.75839, saving model to basic_NN_model.h5\n",
      "Epoch 78/100\n",
      "315191/315191 [==============================] - 31s 100us/step - loss: 22.2839 - acc: 0.3648 - val_loss: 21.7473 - val_acc: 0.3735\n",
      "\n",
      "Epoch 00078: val_loss improved from 21.75839 to 21.74730, saving model to basic_NN_model.h5\n",
      "Epoch 79/100\n",
      "315191/315191 [==============================] - 31s 100us/step - loss: 22.2706 - acc: 0.3678 - val_loss: 21.7536 - val_acc: 0.3924\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 21.74730\n",
      "Epoch 80/100\n",
      "315191/315191 [==============================] - 31s 99us/step - loss: 22.2686 - acc: 0.3706 - val_loss: 21.7329 - val_acc: 0.3754\n",
      "\n",
      "Epoch 00080: val_loss improved from 21.74730 to 21.73288, saving model to basic_NN_model.h5\n",
      "Epoch 81/100\n",
      "315191/315191 [==============================] - 31s 100us/step - loss: 22.2547 - acc: 0.3697 - val_loss: 21.7186 - val_acc: 0.3907\n",
      "\n",
      "Epoch 00081: val_loss improved from 21.73288 to 21.71864, saving model to basic_NN_model.h5\n",
      "Epoch 82/100\n",
      "315191/315191 [==============================] - 31s 99us/step - loss: 22.2449 - acc: 0.3760 - val_loss: 21.7278 - val_acc: 0.3910\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 21.71864\n",
      "Epoch 83/100\n",
      "315191/315191 [==============================] - 31s 100us/step - loss: 22.2406 - acc: 0.3683 - val_loss: 21.7414 - val_acc: 0.3791\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 21.71864\n",
      "Epoch 84/100\n",
      "315191/315191 [==============================] - 31s 99us/step - loss: 22.2272 - acc: 0.3728 - val_loss: 21.7144 - val_acc: 0.4004\n",
      "\n",
      "Epoch 00084: val_loss improved from 21.71864 to 21.71440, saving model to basic_NN_model.h5\n",
      "Epoch 85/100\n",
      "315191/315191 [==============================] - 31s 100us/step - loss: 22.2133 - acc: 0.3772 - val_loss: 21.7028 - val_acc: 0.3990\n",
      "\n",
      "Epoch 00085: val_loss improved from 21.71440 to 21.70280, saving model to basic_NN_model.h5\n",
      "Epoch 86/100\n",
      "315191/315191 [==============================] - 31s 99us/step - loss: 22.2010 - acc: 0.3788 - val_loss: 21.6965 - val_acc: 0.3831\n",
      "\n",
      "Epoch 00086: val_loss improved from 21.70280 to 21.69651, saving model to basic_NN_model.h5\n",
      "Epoch 87/100\n",
      "315191/315191 [==============================] - 32s 100us/step - loss: 22.2045 - acc: 0.3745 - val_loss: 21.7061 - val_acc: 0.4033\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 21.69651\n",
      "Epoch 88/100\n",
      "315191/315191 [==============================] - 31s 99us/step - loss: 22.2526 - acc: 0.3855 - val_loss: 21.6996 - val_acc: 0.3902\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 21.69651\n",
      "Epoch 89/100\n",
      "315191/315191 [==============================] - 31s 100us/step - loss: 22.2499 - acc: 0.3778 - val_loss: 21.6987 - val_acc: 0.4015\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 21.69651\n",
      "Epoch 90/100\n",
      "315191/315191 [==============================] - 31s 100us/step - loss: 22.1982 - acc: 0.3794 - val_loss: 21.7001 - val_acc: 0.4228\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 21.69651\n",
      "Epoch 91/100\n",
      "315191/315191 [==============================] - 31s 100us/step - loss: 22.1955 - acc: 0.3802 - val_loss: 21.6974 - val_acc: 0.3995\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 21.69651\n",
      "Epoch 92/100\n",
      "315191/315191 [==============================] - 31s 100us/step - loss: 22.1811 - acc: 0.3727 - val_loss: 21.6771 - val_acc: 0.3910\n",
      "\n",
      "Epoch 00092: val_loss improved from 21.69651 to 21.67705, saving model to basic_NN_model.h5\n",
      "Epoch 93/100\n",
      "315191/315191 [==============================] - 31s 100us/step - loss: 22.1874 - acc: 0.3762 - val_loss: 21.6704 - val_acc: 0.3962\n",
      "\n",
      "Epoch 00093: val_loss improved from 21.67705 to 21.67041, saving model to basic_NN_model.h5\n",
      "Epoch 94/100\n",
      "315191/315191 [==============================] - 31s 100us/step - loss: 22.1817 - acc: 0.3772 - val_loss: 21.6707 - val_acc: 0.4083\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 21.67041\n",
      "Epoch 95/100\n",
      "315191/315191 [==============================] - 31s 99us/step - loss: 22.1727 - acc: 0.3786 - val_loss: 21.6831 - val_acc: 0.3976\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 21.67041\n",
      "Epoch 96/100\n",
      "315191/315191 [==============================] - 32s 100us/step - loss: 22.1730 - acc: 0.3825 - val_loss: 21.6794 - val_acc: 0.4210\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 21.67041\n",
      "Epoch 97/100\n",
      "315191/315191 [==============================] - 31s 99us/step - loss: 22.1675 - acc: 0.3830 - val_loss: 21.6713 - val_acc: 0.3980\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 21.67041\n",
      "Epoch 98/100\n",
      "315191/315191 [==============================] - 31s 100us/step - loss: 22.1592 - acc: 0.3803 - val_loss: 21.6618 - val_acc: 0.4013\n",
      "\n",
      "Epoch 00098: val_loss improved from 21.67041 to 21.66180, saving model to basic_NN_model.h5\n",
      "Epoch 99/100\n",
      "315191/315191 [==============================] - 32s 100us/step - loss: 22.1571 - acc: 0.3823 - val_loss: 21.6833 - val_acc: 0.3885\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 21.66180\n",
      "Epoch 100/100\n",
      "315191/315191 [==============================] - 31s 99us/step - loss: 22.1571 - acc: 0.3844 - val_loss: 21.6788 - val_acc: 0.4132\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 21.66180\n"
     ]
    }
   ],
   "source": [
    "early_stopping_monitor = EarlyStopping(patience=10)\n",
    "checkpointer = ModelCheckpoint(filepath=\"basic_NN_model.h5\", verbose=1, save_best_only=True)\n",
    "\n",
    "history_basic_NN = basic_NN.fit([X_train_numeric, X_train_text], y_train, epochs=100, batch_size=2048, validation_split=0.1, callbacks=[early_stopping_monitor, checkpointer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
    "id": "Aqfq_Q9VDf5B",
    "outputId": "af249c43-e642-41a1-a4f0-0a685f53cacc"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAFnCAYAAACLnxFFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3ac11eb668>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_basic_NN.history['loss'])\n",
    "plt.plot(history_basic_NN.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pre-trained model\n",
    "Run cell below if model has already been trained, and json and h5 files are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MzyY-6uQNQCT"
   },
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "with open('basic_NN_model.json', 'r') as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "\n",
    "basic_NN = model_from_json(loaded_model_json)\n",
    "basic_NN.load_weights('basic_NN_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create predictions on test and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AlXYzPrd5eP4"
   },
   "outputs": [],
   "source": [
    "label_class_counts = {\n",
    "    'Function': [0, 37], \n",
    "    'Object_Type': [37, 48], \n",
    "    'Operating_Status': [48, 51], \n",
    "    'Position_Type': [51, 76], \n",
    "    'Pre_K': [76, 79], \n",
    "    'Reporting': [79, 82], \n",
    "    'Sharing': [82, 87], \n",
    "    'Student_Type': [87, 96], \n",
    "    'Use': [96, 104]\n",
    "}\n",
    "\n",
    "scores = {}\n",
    "val_predictions = basic_NN.predict([X_val_numeric, X_val_text])\n",
    "\n",
    "# Calculate validation logloss for each label\n",
    "for label, indices in label_class_counts.items():\n",
    "    # Get values for specific label\n",
    "    start_idx = indices[0]\n",
    "    end_idx = indices[1]\n",
    "    y_val_label = y_val[:, start_idx:end_idx]\n",
    "    val_predictions_label = val_predictions[:, start_idx:end_idx]\n",
    "    \n",
    "    \n",
    "    # Get logloss score of mode\n",
    "    scores[label] = log_loss(y_val_label, val_predictions_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c6d9EwoxGX3z"
   },
   "outputs": [],
   "source": [
    "# Generate predictions for test set\n",
    "predictions = basic_NN.predict([X_test_numeric, X_test_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_-pJMVIr5eP7"
   },
   "source": [
    "# Save score and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ngyDlM8A5eP7"
   },
   "outputs": [],
   "source": [
    "# Save predictions for test set\n",
    "label = ['Function',\n",
    "         'Object_Type',\n",
    "         'Operating_Status',\n",
    "         'Position_Type',\n",
    "         'Pre_K',\n",
    "         'Reporting',\n",
    "         'Sharing',\n",
    "         'Student_Type',\n",
    "         'Use']\n",
    "y = pd.get_dummies(data_train[label])\n",
    "\n",
    "submission = pd.DataFrame(predictions, index=data_test.index, columns=y.columns)\n",
    "submission.to_csv('basic_NN_model.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "uRHjIso15eP8",
    "outputId": "eb327b5e-bc0e-4293-ddec-3a030d5b4e9f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Function': 0.2301983517988105,\n",
       " 'Object_Type': 0.3767337669434889,\n",
       " 'Operating_Status': 0.17604704140009167,\n",
       " 'Position_Type': 0.21400684640244078,\n",
       " 'Pre_K': 0.16854484993480728,\n",
       " 'Reporting': 0.3701935306624163,\n",
       " 'Sharing': 0.4806175221214185,\n",
       " 'Student_Type': 0.28148916380670513,\n",
       " 'Use': 0.3049840409094727}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yQd8s_q85eP_",
    "outputId": "e5603b74-3363-4031-be7c-821b4927e72f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28920167933107244"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([score for score in scores.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DQxW3L5o5eQC"
   },
   "outputs": [],
   "source": [
    "# Save scores\n",
    "with open('basic_NN_score.json', 'w') as file:\n",
    "     file.write(json.dumps(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A2xKwd0vMz7t"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "basic_NN_model.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
