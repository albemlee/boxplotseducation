{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Working Directory\n",
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10037,
     "status": "ok",
     "timestamp": 1533388300661,
     "user": {
      "displayName": "Albert Lee",
      "photoUrl": "//lh3.googleusercontent.com/-f0u8obCCk_w/AAAAAAAAAAI/AAAAAAAAAxs/83E22s4mtyM/s50-c-k-no/photo.jpg",
      "userId": "109969660738913262150"
     },
     "user_tz": 240
    },
    "id": "OwY7ZriLi3yM",
    "outputId": "3fbf41b0-f3f3-4456-c663-92e9cecad01d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /content/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /content/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import Imputer\n",
    "from scipy import sparse\n",
    "\n",
    "import zipfile\n",
    "import re, nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, concatenate, Input, Dropout, Embedding, Flatten\n",
    "from keras.models import Model, model_from_json\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "IaOt20--iRlQ"
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Return pandas dataframe data_train: training data (features + labels)\n",
    "    Return pandas dataframe data_test: test data (only features)\n",
    "    \n",
    "    Required Libraries: zipfile, pandas\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load zipped folder with data files\n",
    "    resource_archive = zipfile.ZipFile('resources.zip', 'r')\n",
    "\n",
    "    # Load testing data\n",
    "    data_test = pd.read_csv(resource_archive.open('TestData.csv'), \n",
    "                            dtype={\n",
    "                                'Object_Description': str, \n",
    "                                'Program_Description': str, \n",
    "                                'SubFund_Description': str, \n",
    "                                'Job_Title_Description': str, \n",
    "                                'Facility_or_Department': str,\n",
    "                                'Sub_Object_Description': str, \n",
    "                                'Location_Description': str, \n",
    "                                'FTE': float,\n",
    "                                'Function_Description': str, \n",
    "                                'Position_Extra': str, \n",
    "                                'Text_4': str, \n",
    "                                'Total': float, \n",
    "                                'Text_2': str,\n",
    "                                'Text_3': str, \n",
    "                                'Fund_Description': str, \n",
    "                                'Text_1': str\n",
    "                            },\n",
    "                            index_col=0)\n",
    "\n",
    "    # Load training data\n",
    "    data_train = pd.read_csv(resource_archive.open('TrainingData.csv'), \n",
    "                            dtype={\n",
    "                                'Object_Description': str, \n",
    "                                'Program_Description': str, \n",
    "                                'SubFund_Description': str, \n",
    "                                'Job_Title_Description': str, \n",
    "                                'Facility_or_Department': str,\n",
    "                                'Sub_Object_Description': str, \n",
    "                                'Location_Description': str, \n",
    "                                'FTE': float,\n",
    "                                'Function_Description': str, \n",
    "                                'Position_Extra': str, \n",
    "                                'Text_4': str, \n",
    "                                'Total': float, \n",
    "                                'Text_2': str,\n",
    "                                'Text_3': str, \n",
    "                                'Fund_Description': str, \n",
    "                                'Text_1': str,\n",
    "                                'Function': 'category',\n",
    "                                'Object_Type': 'category',\n",
    "                                'Operating_Status': 'category',\n",
    "                                'Position_Type': 'category',\n",
    "                                'Pre_K': 'category',\n",
    "                                'Reporting': 'category',\n",
    "                                'Sharing': 'category',\n",
    "                                'Student_Type': 'category',\n",
    "                                'Use': 'category',\n",
    "                            },\n",
    "                             index_col=0)\n",
    "    \n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3041,
     "status": "ok",
     "timestamp": 1533388364127,
     "user": {
      "displayName": "Albert Lee",
      "photoUrl": "//lh3.googleusercontent.com/-f0u8obCCk_w/AAAAAAAAAAI/AAAAAAAAAxs/83E22s4mtyM/s50-c-k-no/photo.jpg",
      "userId": "109969660738913262150"
     },
     "user_tz": 240
    },
    "id": "s4O_q-AJirvy",
    "outputId": "e88425e3-9c92-4749-e957-314e74cf02f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_train shape: (400277, 25)\n",
      "data_test shape: (50064, 16)\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "data_train, data_test = load_data()\n",
    "print('data_train shape:', data_train.shape)\n",
    "print('data_test shape:', data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "aaMvEQaZivGl"
   },
   "outputs": [],
   "source": [
    "def load_features(data_train, data_test):\n",
    "    \"\"\"\n",
    "    Return pandas dataframe data_features: data in feature columns of data_train and data_test\n",
    "    \n",
    "    Param pandas dataframe data_train: training data (features + labels)\n",
    "    Param pandas dataframe data_test: test data (only features)\n",
    "    \n",
    "    Required Libraries: pandas\n",
    "    \"\"\"\n",
    "    \n",
    "    feature_columns = data_test.columns # data_test only contains features\n",
    "    \n",
    "    data_features = pd.concat([data_train[feature_columns], data_test])\n",
    "    \n",
    "    return data_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 543,
     "status": "ok",
     "timestamp": 1533388365009,
     "user": {
      "displayName": "Albert Lee",
      "photoUrl": "//lh3.googleusercontent.com/-f0u8obCCk_w/AAAAAAAAAAI/AAAAAAAAAxs/83E22s4mtyM/s50-c-k-no/photo.jpg",
      "userId": "109969660738913262150"
     },
     "user_tz": 240
    },
    "id": "0veNH0MTizav",
    "outputId": "366ef012-a310-4ee5-8631-ebba585e8b6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_features shape: (450341, 16)\n"
     ]
    }
   ],
   "source": [
    "# Load Features\n",
    "data_features = load_features(data_train, data_test)\n",
    "print('data_features shape:', data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "IS6tKdpJi_LF"
   },
   "outputs": [],
   "source": [
    "def text_processing(phrase):\n",
    "    \"\"\"\n",
    "    Return list processed_phrase: phrase tokens after processing has been completed\n",
    "    \n",
    "    param string phrase: phrase to be processed\n",
    "    \n",
    "    Required Libraries: re, nltk\n",
    "    \"\"\"\n",
    "    \n",
    "    # Case Normalization\n",
    "    processed_phrase = phrase.lower()\n",
    "    \n",
    "    # Remove Punctuations\n",
    "    processed_phrase = re.sub(r\"[^a-z0-9-]\", \" \", processed_phrase)\n",
    "    \n",
    "    # Tokenize Phrase\n",
    "    processed_phrase = processed_phrase.split()\n",
    "    \n",
    "    # Remove Stopwords\n",
    "    processed_phrase = [word for word in processed_phrase if word not in stopwords.words(\"english\") and word != '-']\n",
    "    \n",
    "    # Lemmatization\n",
    "    processed_phrase = [WordNetLemmatizer().lemmatize(word) for word in processed_phrase]\n",
    "    \n",
    "    # Recombine list into phrase\n",
    "    processed_phrase = ' '.join(processed_phrase)\n",
    "    \n",
    "    return processed_phrase\n",
    "\n",
    "def init_prep(data_train, data_test, data_features, label=None):\n",
    "    \"\"\"\n",
    "    Return numpy array X: feature matrix for classification model fitting\n",
    "    Return numpy array y: labels matrix for classification model fitting\n",
    "    Return numpy array X_test: feature matrix of test set\n",
    "    \n",
    "    Param pandas dataframe data_train: training data (features + labels)\n",
    "    Param pandas dataframe data_test: test data (features)\n",
    "    Param pandas dataframe data_features: data in feature columns of data_train and data_test\n",
    "    \n",
    "    Required Libraries: pandas, numpy, keras\n",
    "    Required helper functions: text_processing\n",
    "    \"\"\"\n",
    "    \n",
    "    # Combined and preprocess text columns\n",
    "    data_train['combined_text'] = (data_train[data_features.columns]\n",
    "                                       .drop(columns=['FTE', 'Total'])\n",
    "                                       .fillna(\"\")\n",
    "                                       .apply(lambda x: \" \".join(x), axis=1)\n",
    "                                       .apply(lambda x: text_processing(x))\n",
    "                                  )\n",
    "    data_test['combined_text'] = (data_test[data_features.columns]\n",
    "                                       .drop(columns=['FTE', 'Total'])\n",
    "                                       .fillna(\"\")\n",
    "                                       .apply(lambda x: \" \".join(x), axis=1)\n",
    "                                       .apply(lambda x: text_processing(x))\n",
    "                                 )\n",
    "    data_features['combined_text'] = (data_features\n",
    "                                          .drop(columns=['FTE', 'Total'])\n",
    "                                          .fillna(\"\")\n",
    "                                          .apply(lambda x: \" \".join(x), axis=1)\n",
    "                                          .apply(lambda x: text_processing(x))\n",
    "                                     )\n",
    "    \n",
    "    # Vectorizer text columns in training data\n",
    "    tokenize = Tokenizer()\n",
    "    tokenize.fit_on_texts(data_features['combined_text'])\n",
    "    \n",
    "    X_text = tokenize.texts_to_sequences(data_train['combined_text'])\n",
    "    X_text_test = tokenize.texts_to_sequences(data_test['combined_text'])\n",
    "    \n",
    "    X_text = pad_sequences(X_text, padding='post', maxlen=50, truncating='post')\n",
    "    X_text_test = pad_sequences(X_text_test, padding='post', maxlen=50, truncating='post')\n",
    "    \n",
    "    # Impute missing numerical data\n",
    "    imp_total = Imputer(strategy='median')\n",
    "    imp_total.fit(data_features['Total'].values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "    total_not_missing = pd.isnull(data_train['Total']).astype(int).values.reshape(-1, 1)\n",
    "    fte_not_missing = pd.isnull(data_train['FTE']).astype(int).values.reshape(-1, 1)\n",
    "    total = imp_total.transform(data_train['Total'].values.reshape(-1, 1))\n",
    "    fte = data_train['FTE'].fillna('0').values.reshape(-1, 1)\n",
    "\n",
    "    total_not_missing_test = pd.isnull(data_test['Total']).astype(int).values.reshape(-1, 1)\n",
    "    fte_not_missing_test = pd.isnull(data_test['FTE']).astype(int).values.reshape(-1, 1)\n",
    "    total_test = imp_total.transform(data_test['Total'].values.reshape(-1, 1))\n",
    "    fte_test = data_test['FTE'].fillna('0').values.reshape(-1, 1)\n",
    "    \n",
    "    # Create feature matrix\n",
    "    X_numeric = np.concatenate([total, total_not_missing, fte, fte_not_missing], axis=1)\n",
    "    X_numeric_test = np.concatenate([total_test, total_not_missing_test, fte_test, fte_not_missing_test], axis=1)\n",
    "    \n",
    "    # Create labels matrix\n",
    "    if label:\n",
    "        y = pd.get_dummies(data_train[label]).values.astype('float64')\n",
    "    else:\n",
    "        label = ['Function',\n",
    "                 'Object_Type',\n",
    "                 'Operating_Status',\n",
    "                 'Position_Type',\n",
    "                 'Pre_K',\n",
    "                 'Reporting',\n",
    "                 'Sharing',\n",
    "                 'Student_Type',\n",
    "                 'Use']\n",
    "        y = pd.get_dummies(data_train[label]).values.astype('float64')\n",
    "    \n",
    "    return X_numeric, X_text, X_numeric_test, X_text_test, y, tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "fmI7saJKjJKl"
   },
   "outputs": [],
   "source": [
    "X_numeric, X_text, X_numeric_test, X_text_test, y, tokenize = init_prep(data_train, data_test, data_features, label=None)\n",
    "print('X_numeric shape:', X_numeric.shape)\n",
    "print('X_numeric_test shape:', X_numeric_test.shape)\n",
    "print('X_text shape:', X_text.shape)\n",
    "print('X_text_test shape:', X_text_test.shape)\n",
    "print('y shape:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "4pHkrZ__lzHt"
   },
   "outputs": [],
   "source": [
    "np.savetxt('X_numeric.csv', X_numeric, fmt='%5s', delimiter=\",\")\n",
    "np.savetxt('X_numeric_test.csv', X_numeric_test, fmt='%5s', delimiter=\",\")\n",
    "np.savetxt('X_text.csv', X_text, fmt='%5s', delimiter=\",\")\n",
    "np.savetxt('X_text_test.csv', X_text_test, fmt='%5s', delimiter=\",\")\n",
    "np.savetxt('y.csv', y, fmt='%5s', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 324,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 315069,
     "status": "ok",
     "timestamp": 1533388712336,
     "user": {
      "displayName": "Albert Lee",
      "photoUrl": "//lh3.googleusercontent.com/-f0u8obCCk_w/AAAAAAAAAAI/AAAAAAAAAxs/83E22s4mtyM/s50-c-k-no/photo.jpg",
      "userId": "109969660738913262150"
     },
     "user_tz": 240
    },
    "id": "R8RriV9EMuMm",
    "outputId": "7ca56f16-e899-4b0c-c50f-1d7d2d0e657f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-795750d4-9345-49cb-b2bb-64976bdbd2a5\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-795750d4-9345-49cb-b2bb-64976bdbd2a5\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving X_text_test.csv to X_text_test.csv\n",
      "Saving X_text.csv to X_text.csv\n",
      "Saving X_numeric_test.csv to X_numeric_test.csv\n",
      "Saving X_numeric.csv to X_numeric.csv\n",
      "User uploaded file \"X_text_test.csv\" with length 15019200 bytes\n",
      "User uploaded file \"X_text.csv\" with length 120083100 bytes\n",
      "User uploaded file \"X_numeric_test.csv\" with length 1513417 bytes\n",
      "User uploaded file \"X_numeric.csv\" with length 11961469 bytes\n",
      "X_numeric shape: (400277, 4)\n",
      "X_numeric_test shape: (50064, 4)\n",
      "X_text shape: (400277, 50)\n",
      "X_text_test shape: (50064, 50)\n",
      "y shape: (400277, 104)\n"
     ]
    }
   ],
   "source": [
    "label = ['Function',\n",
    "         'Object_Type',\n",
    "         'Operating_Status',\n",
    "         'Position_Type',\n",
    "         'Pre_K',\n",
    "         'Reporting',\n",
    "         'Sharing',\n",
    "         'Student_Type',\n",
    "         'Use']\n",
    "y = pd.get_dummies(data_train[label]).values.astype('float64')\n",
    "\n",
    "X_numeric = pd.read_csv('X_numeric.csv', header=None).values\n",
    "X_numeric_test = pd.read_csv('X_numeric_test.csv', header=None).values\n",
    "X_text = pd.read_csv('X_text.csv', header=None).values\n",
    "X_text_test = pd.read_csv('X_text_test.csv', header=None).values\n",
    "\n",
    "print('X_numeric shape:', X_numeric.shape)\n",
    "print('X_numeric_test shape:', X_numeric_test.shape)\n",
    "print('X_text shape:', X_text.shape)\n",
    "print('X_text_test shape:', X_text_test.shape)\n",
    "print('y shape:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "FiqgQCnLjiul"
   },
   "outputs": [],
   "source": [
    "def build_network(X_numeric, X_text, X_numeric_test, X_text_test, y):\n",
    "    \"\"\"\n",
    "    Return compiled keras-model model\n",
    "    \n",
    "    param numpy array X: feature matrix for classification\n",
    "    param numpy array y: labels matrix for classification\n",
    "    \n",
    "    Required Libraries: keras\n",
    "    \"\"\"\n",
    "    \n",
    "    dropout_value = 0.5\n",
    "    embedding_vector_length = 8\n",
    "    \n",
    "    numeric_input = Input(shape=(X_numeric.shape[1],) , name='numeric_input') \n",
    "    text_input = Input(shape=(X_text.shape[1],) , name='text_input')\n",
    "    \n",
    "    # Function\n",
    "    word_embedding_function = Embedding(input_dim=3804, output_dim=embedding_vector_length, mask_zero=False, input_length=50)(text_input)\n",
    "    word_embedding_function = Flatten()(word_embedding_function)\n",
    "    text_function_hidden_layer_1 = Dense(256, activation='relu')(word_embedding_function)\n",
    "    text_function_hidden_layer_1 = Dropout(dropout_value)(text_function_hidden_layer_1)\n",
    "    text_function_hidden_layer_2 = Dense(128, activation='relu')(text_function_hidden_layer_1)\n",
    "    text_function_hidden_layer_2 = Dropout(dropout_value)(text_function_hidden_layer_2)\n",
    "    text_function_hidden_layer_3 = Dense(64, activation='relu')(text_function_hidden_layer_2)\n",
    "    text_function_hidden_layer_3 = Dropout(dropout_value)(text_function_hidden_layer_3)\n",
    "    numeric_function_hidden_layer_1 = Dense(4, activation='relu')(numeric_input)\n",
    "    numeric_function_hidden_layer_1 = Dropout(dropout_value)(numeric_function_hidden_layer_1)\n",
    "    combined_function_layer = concatenate([numeric_function_hidden_layer_1, text_function_hidden_layer_2])\n",
    "    function_output_layer = Dense(37, activation='softmax')(combined_function_layer)\n",
    "    \n",
    "    # Object_Type\n",
    "    word_embedding_object_type = Embedding(input_dim=3804, output_dim=embedding_vector_length, mask_zero=False, input_length=50)(text_input)\n",
    "    word_embedding_object_type = Flatten()(word_embedding_object_type)\n",
    "    text_object_type_hidden_layer_1 = Dense(256, activation='relu')(word_embedding_object_type)\n",
    "    text_object_type_hidden_layer_1 = Dropout(dropout_value)(text_object_type_hidden_layer_1)\n",
    "    text_object_type_hidden_layer_2 = Dense(128, activation='relu')(text_object_type_hidden_layer_1)\n",
    "    text_object_type_hidden_layer_2 = Dropout(dropout_value)(text_object_type_hidden_layer_2)\n",
    "    text_object_type_hidden_layer_3 = Dense(64, activation='relu')(text_object_type_hidden_layer_2)\n",
    "    text_object_type_hidden_layer_3 = Dropout(dropout_value)(text_object_type_hidden_layer_3)\n",
    "    text_object_type_hidden_layer_4 = Dense(32, activation='relu')(text_object_type_hidden_layer_3)\n",
    "    text_object_type_hidden_layer_4 = Dropout(dropout_value)(text_object_type_hidden_layer_4)\n",
    "    numeric_object_type_hidden_layer_1 = Dense(4, activation='relu')(numeric_input)\n",
    "    numeric_object_type_hidden_layer_1 = Dropout(dropout_value)(numeric_object_type_hidden_layer_1)\n",
    "    combined_object_type_layer = concatenate([numeric_object_type_hidden_layer_1, text_object_type_hidden_layer_4])\n",
    "    object_type_output_layer = Dense(11, activation='softmax')(combined_object_type_layer)\n",
    "    \n",
    "    # Operating_Status\n",
    "    word_embedding_operating_status = Embedding(input_dim=3804, output_dim=embedding_vector_length, mask_zero=False, input_length=50)(text_input)\n",
    "    word_embedding_operating_status = Flatten()(word_embedding_operating_status)\n",
    "    text_operating_status_hidden_layer_1 = Dense(256, activation='relu')(word_embedding_operating_status)\n",
    "    text_operating_status_hidden_layer_1 = Dropout(dropout_value)(text_operating_status_hidden_layer_1)\n",
    "    text_operating_status_hidden_layer_2 = Dense(128, activation='relu')(text_operating_status_hidden_layer_1)\n",
    "    text_operating_status_hidden_layer_2 = Dropout(dropout_value)(text_operating_status_hidden_layer_2)\n",
    "    text_operating_status_hidden_layer_3 = Dense(64, activation='relu')(text_operating_status_hidden_layer_2)\n",
    "    text_operating_status_hidden_layer_3 = Dropout(dropout_value)(text_operating_status_hidden_layer_3)\n",
    "    text_operating_status_hidden_layer_4 = Dense(32, activation='relu')(text_operating_status_hidden_layer_3)\n",
    "    text_operating_status_hidden_layer_4 = Dropout(dropout_value)(text_operating_status_hidden_layer_4)\n",
    "    text_operating_status_hidden_layer_5 = Dense(16, activation='relu')(text_operating_status_hidden_layer_4)\n",
    "    text_operating_status_hidden_layer_5 = Dropout(dropout_value)(text_operating_status_hidden_layer_5)\n",
    "    text_operating_status_hidden_layer_6 = Dense(9, activation='relu')(text_operating_status_hidden_layer_5)\n",
    "    text_operating_status_hidden_layer_6 = Dropout(dropout_value)(text_operating_status_hidden_layer_6)\n",
    "    numeric_operating_status_hidden_layer_1 = Dense(4, activation='relu')(numeric_input)\n",
    "    numeric_operating_status_hidden_layer_1 = Dropout(dropout_value)(numeric_operating_status_hidden_layer_1)\n",
    "    combined_operating_status_layer = concatenate([numeric_operating_status_hidden_layer_1, text_operating_status_hidden_layer_6])\n",
    "    operating_status_output_layer = Dense(3, activation='softmax')(combined_operating_status_layer)\n",
    "    \n",
    "    # Position_Type\n",
    "    word_embedding_position_type = Embedding(input_dim=3804, output_dim=embedding_vector_length, mask_zero=False, input_length=50)(text_input)\n",
    "    word_embedding_position_type = Flatten()(word_embedding_position_type)\n",
    "    text_position_type_hidden_layer_1 = Dense(256, activation='relu')(word_embedding_operating_status)\n",
    "    text_position_type_hidden_layer_1 = Dropout(dropout_value)(text_position_type_hidden_layer_1)\n",
    "    text_position_type_hidden_layer_2 = Dense(128, activation='relu')(text_position_type_hidden_layer_1)\n",
    "    text_position_type_hidden_layer_2 = Dropout(dropout_value)(text_position_type_hidden_layer_2)\n",
    "    text_position_type_hidden_layer_3 = Dense(64, activation='relu')(text_position_type_hidden_layer_2)\n",
    "    text_position_type_hidden_layer_3 = Dropout(dropout_value)(text_position_type_hidden_layer_3)\n",
    "    numeric_position_type_hidden_layer_1 = Dense(4, activation='relu')(numeric_input)\n",
    "    numeric_position_type_hidden_layer_1 = Dropout(dropout_value)(numeric_position_type_hidden_layer_1)\n",
    "    combined_position_type_layer = concatenate([numeric_position_type_hidden_layer_1, text_position_type_hidden_layer_3])\n",
    "    position_type_output_layer = Dense(25, activation='softmax')(combined_position_type_layer)\n",
    "    \n",
    "    # Pre_K\n",
    "    word_embedding_pre_k = Embedding(input_dim=3804, output_dim=embedding_vector_length, mask_zero=False, input_length=50)(text_input)\n",
    "    word_embedding_pre_k = Flatten()(word_embedding_pre_k)\n",
    "    text_pre_k_hidden_layer_1 = Dense(256, activation='relu')(word_embedding_pre_k)\n",
    "    text_pre_k_hidden_layer_1 = Dropout(dropout_value)(text_pre_k_hidden_layer_1)\n",
    "    text_pre_k_hidden_layer_2 = Dense(128, activation='relu')(text_pre_k_hidden_layer_1)\n",
    "    text_pre_k_hidden_layer_2 = Dropout(dropout_value)(text_pre_k_hidden_layer_2)\n",
    "    text_pre_k_hidden_layer_3 = Dense(64, activation='relu')(text_pre_k_hidden_layer_2)\n",
    "    text_pre_k_hidden_layer_3 = Dropout(dropout_value)(text_pre_k_hidden_layer_3)\n",
    "    text_pre_k_hidden_layer_4 = Dense(32, activation='relu')(text_pre_k_hidden_layer_3)\n",
    "    text_pre_k_hidden_layer_4 = Dropout(dropout_value)(text_pre_k_hidden_layer_4)\n",
    "    text_pre_k_hidden_layer_5 = Dense(16, activation='relu')(text_pre_k_hidden_layer_4)\n",
    "    text_pre_k_hidden_layer_5 = Dropout(dropout_value)(text_pre_k_hidden_layer_5)\n",
    "    text_pre_k_hidden_layer_6 = Dense(9, activation='relu')(text_pre_k_hidden_layer_5)\n",
    "    text_pre_k_hidden_layer_6 = Dropout(dropout_value)(text_pre_k_hidden_layer_6)\n",
    "    numeric_pre_k_hidden_layer_1 = Dense(4, activation='relu')(numeric_input)\n",
    "    numeric_pre_k_hidden_layer_1 = Dropout(dropout_value)(numeric_pre_k_hidden_layer_1)\n",
    "    combined_pre_k_layer = concatenate([numeric_pre_k_hidden_layer_1, text_pre_k_hidden_layer_6])\n",
    "    pre_k_output_layer = Dense(3, activation='softmax')(combined_pre_k_layer)\n",
    "    \n",
    "    # Reporting\n",
    "    word_embedding_reporting = Embedding(input_dim=3804, output_dim=embedding_vector_length, mask_zero=False, input_length=50)(text_input)\n",
    "    word_embedding_reporting = Flatten()(word_embedding_reporting)\n",
    "    text_reporting_hidden_layer_1 = Dense(256, activation='relu')(word_embedding_reporting)\n",
    "    text_reporting_hidden_layer_1 = Dropout(dropout_value)(text_reporting_hidden_layer_1)\n",
    "    text_reporting_hidden_layer_2 = Dense(128, activation='relu')(text_reporting_hidden_layer_1)\n",
    "    text_reporting_hidden_layer_2 = Dropout(dropout_value)(text_reporting_hidden_layer_2)\n",
    "    text_reporting_hidden_layer_3 = Dense(64, activation='relu')(text_reporting_hidden_layer_2)\n",
    "    text_reporting_hidden_layer_3 = Dropout(dropout_value)(text_reporting_hidden_layer_3)\n",
    "    text_reporting_hidden_layer_4 = Dense(32, activation='relu')(text_reporting_hidden_layer_3)\n",
    "    text_reporting_hidden_layer_4 = Dropout(dropout_value)(text_reporting_hidden_layer_4)\n",
    "    text_reporting_hidden_layer_5 = Dense(16, activation='relu')(text_reporting_hidden_layer_4)\n",
    "    text_reporting_hidden_layer_5 = Dropout(dropout_value)(text_reporting_hidden_layer_5)\n",
    "    text_reporting_hidden_layer_6 = Dense(8, activation='relu')(text_reporting_hidden_layer_5)\n",
    "    text_reporting_hidden_layer_6 = Dropout(dropout_value)(text_reporting_hidden_layer_6)\n",
    "    numeric_reporting_hidden_layer_1 = Dense(4, activation='relu')(numeric_input)\n",
    "    numeric_reporting_hidden_layer_1 = Dropout(dropout_value)(numeric_reporting_hidden_layer_1)\n",
    "    combined_reporting_layer = concatenate([numeric_reporting_hidden_layer_1, text_reporting_hidden_layer_6])\n",
    "    reporting_output_layer = Dense(3, activation='softmax')(combined_reporting_layer)\n",
    "    \n",
    "    # Sharing\n",
    "    word_embedding_sharing = Embedding(input_dim=3804, output_dim=embedding_vector_length, mask_zero=False, input_length=50)(text_input)\n",
    "    word_embedding_sharing = Flatten()(word_embedding_sharing)\n",
    "    text_sharing_hidden_layer_1 = Dense(256, activation='relu')(word_embedding_sharing)\n",
    "    text_sharing_hidden_layer_1 = Dropout(dropout_value)(text_sharing_hidden_layer_1)\n",
    "    text_sharing_hidden_layer_2 = Dense(128, activation='relu')(text_sharing_hidden_layer_1)\n",
    "    text_sharing_hidden_layer_2 = Dropout(dropout_value)(text_sharing_hidden_layer_2)\n",
    "    text_sharing_hidden_layer_3 = Dense(64, activation='relu')(text_sharing_hidden_layer_2)\n",
    "    text_sharing_hidden_layer_3 = Dropout(dropout_value)(text_sharing_hidden_layer_3)\n",
    "    text_sharing_hidden_layer_4 = Dense(32, activation='relu')(text_sharing_hidden_layer_3)\n",
    "    text_sharing_hidden_layer_4 = Dropout(dropout_value)(text_sharing_hidden_layer_4)\n",
    "    text_sharing_hidden_layer_5 = Dense(16, activation='relu')(text_sharing_hidden_layer_4)\n",
    "    text_sharing_hidden_layer_5 = Dropout(dropout_value)(text_sharing_hidden_layer_5)\n",
    "    numeric_sharing_hidden_layer_1 = Dense(4, activation='relu')(numeric_input)\n",
    "    numeric_sharing_hidden_layer_1 = Dropout(dropout_value)(numeric_sharing_hidden_layer_1)\n",
    "    combined_sharing_layer = concatenate([numeric_sharing_hidden_layer_1, text_sharing_hidden_layer_5])\n",
    "    sharing_output_layer = Dense(5, activation='softmax')(combined_sharing_layer)\n",
    "    \n",
    "    # Student_Type\n",
    "    word_embedding_student_type = Embedding(input_dim=3804, output_dim=embedding_vector_length, mask_zero=False, input_length=50)(text_input)\n",
    "    word_embedding_student_type = Flatten()(word_embedding_student_type)\n",
    "    text_student_type_hidden_layer_1 = Dense(256, activation='relu')(word_embedding_student_type)\n",
    "    text_student_type_hidden_layer_1 = Dropout(dropout_value)(text_student_type_hidden_layer_1)\n",
    "    text_student_type_hidden_layer_2 = Dense(128, activation='relu')(text_student_type_hidden_layer_1)\n",
    "    text_student_type_hidden_layer_2 = Dropout(dropout_value)(text_student_type_hidden_layer_2)\n",
    "    text_student_type_hidden_layer_3 = Dense(64, activation='relu')(text_student_type_hidden_layer_2)\n",
    "    text_student_type_hidden_layer_3 = Dropout(dropout_value)(text_student_type_hidden_layer_3)\n",
    "    text_student_type_hidden_layer_4 = Dense(32, activation='relu')(text_student_type_hidden_layer_3)\n",
    "    text_student_type_hidden_layer_4 = Dropout(dropout_value)(text_student_type_hidden_layer_4)\n",
    "    numeric_student_type_hidden_layer_1 = Dense(4, activation='relu')(numeric_input)\n",
    "    numeric_student_type_hidden_layer_1 = Dropout(dropout_value)(numeric_student_type_hidden_layer_1)\n",
    "    combined_student_type_layer = concatenate([numeric_student_type_hidden_layer_1, text_student_type_hidden_layer_4])\n",
    "    student_type_output_layer = Dense(9, activation='softmax')(combined_student_type_layer)\n",
    "    \n",
    "    # Use\n",
    "    word_embedding_use = Embedding(input_dim=3804, output_dim=embedding_vector_length, mask_zero=False, input_length=50)(text_input)\n",
    "    word_embedding_use = Flatten()(word_embedding_use)\n",
    "    text_use_hidden_layer_1 = Dense(256, activation='relu')(word_embedding_use)\n",
    "    text_use_hidden_layer_1 = Dropout(dropout_value)(text_use_hidden_layer_1)\n",
    "    text_use_hidden_layer_2 = Dense(128, activation='relu')(text_use_hidden_layer_1)\n",
    "    text_use_hidden_layer_2 = Dropout(dropout_value)(text_use_hidden_layer_2)\n",
    "    text_use_hidden_layer_3 = Dense(64, activation='relu')(text_use_hidden_layer_2)\n",
    "    text_use_hidden_layer_3 = Dropout(dropout_value)(text_use_hidden_layer_3)\n",
    "    text_use_hidden_layer_4 = Dense(32, activation='relu')(text_use_hidden_layer_3)\n",
    "    text_use_hidden_layer_4 = Dropout(dropout_value)(text_use_hidden_layer_4)\n",
    "    numeric_use_hidden_layer_1 = Dense(4, activation='relu')(numeric_input)\n",
    "    numeric_use_hidden_layer_1 = Dropout(dropout_value)(numeric_use_hidden_layer_1)\n",
    "    combined_use_layer = concatenate([numeric_use_hidden_layer_1, text_use_hidden_layer_4])\n",
    "    use_output_layer = Dense(8, activation='softmax')(combined_use_layer)\n",
    "    \n",
    "    # Output\n",
    "    combined_output_layer = concatenate([function_output_layer, \n",
    "                                         object_type_output_layer,\n",
    "                                         operating_status_output_layer,\n",
    "                                         position_type_output_layer,\n",
    "                                         pre_k_output_layer,\n",
    "                                         reporting_output_layer,\n",
    "                                         sharing_output_layer,\n",
    "                                         student_type_output_layer,\n",
    "                                         use_output_layer])\n",
    "    \n",
    "    model = Model(inputs=[numeric_input, text_input], outputs=[combined_output_layer])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 6698
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9629436,
     "status": "ok",
     "timestamp": 1533398381112,
     "user": {
      "displayName": "Albert Lee",
      "photoUrl": "//lh3.googleusercontent.com/-f0u8obCCk_w/AAAAAAAAAAI/AAAAAAAAAxs/83E22s4mtyM/s50-c-k-no/photo.jpg",
      "userId": "109969660738913262150"
     },
     "user_tz": 240
    },
    "id": "1wWQdejZlOPD",
    "outputId": "6f965b69-7213-4656-9ee9-f698f915c386"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 360249 samples, validate on 40028 samples\n",
      "Epoch 1/150\n",
      "360249/360249 [==============================] - 104s 288us/step - loss: 54.7598 - acc: 0.3340 - val_loss: 38.5948 - val_acc: 0.5436\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 38.59477, saving model to embedding_model.h5\n",
      "Epoch 2/150\n",
      "122880/360249 [=========>....................] - ETA: 1:01 - loss: 38.0413 - acc: 0.4269360249/360249 [==============================] - 96s 266us/step - loss: 35.2765 - acc: 0.4033 - val_loss: 29.2334 - val_acc: 0.4525\n",
      "\n",
      "Epoch 00002: val_loss improved from 38.59477 to 29.23340, saving model to embedding_model.h5\n",
      "Epoch 3/150\n",
      "248832/360249 [===================>..........] - ETA: 28s - loss: 31.9202 - acc: 0.3598360249/360249 [==============================] - 96s 266us/step - loss: 31.6163 - acc: 0.3551 - val_loss: 28.2199 - val_acc: 0.4347\n",
      "\n",
      "Epoch 00003: val_loss improved from 29.23340 to 28.21986, saving model to embedding_model.h5\n",
      "Epoch 4/150\n",
      "282624/360249 [======================>.......] - ETA: 19s - loss: 29.8411 - acc: 0.3345360249/360249 [==============================] - 96s 266us/step - loss: 29.6490 - acc: 0.3332 - val_loss: 26.4416 - val_acc: 0.3975\n",
      "\n",
      "Epoch 00004: val_loss improved from 28.21986 to 26.44160, saving model to embedding_model.h5\n",
      "Epoch 5/150\n",
      "291840/360249 [=======================>......] - ETA: 17s - loss: 28.4076 - acc: 0.3192360249/360249 [==============================] - 96s 266us/step - loss: 28.3205 - acc: 0.3188 - val_loss: 25.7445 - val_acc: 0.3971\n",
      "\n",
      "Epoch 00005: val_loss improved from 26.44160 to 25.74446, saving model to embedding_model.h5\n",
      "Epoch 6/150\n",
      "293888/360249 [=======================>......] - ETA: 17s - loss: 27.5923 - acc: 0.3132360249/360249 [==============================] - 96s 266us/step - loss: 27.5340 - acc: 0.3145 - val_loss: 25.3093 - val_acc: 0.4038\n",
      "\n",
      "Epoch 00006: val_loss improved from 25.74446 to 25.30931, saving model to embedding_model.h5\n",
      "Epoch 7/150\n",
      "294912/360249 [=======================>......] - ETA: 16s - loss: 27.0668 - acc: 0.3164360249/360249 [==============================] - 96s 266us/step - loss: 27.0376 - acc: 0.3141 - val_loss: 25.4742 - val_acc: 0.3948\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 25.30931\n",
      "Epoch 8/150\n",
      "338944/360249 [===========================>..] - ETA: 5s - loss: 26.8497 - acc: 0.3121360249/360249 [==============================] - 96s 266us/step - loss: 26.8373 - acc: 0.3120 - val_loss: 25.2428 - val_acc: 0.4028\n",
      "\n",
      "Epoch 00008: val_loss improved from 25.30931 to 25.24284, saving model to embedding_model.h5\n",
      "Epoch 9/150\n",
      "306176/360249 [========================>.....] - ETA: 13s - loss: 26.4942 - acc: 0.3164360249/360249 [==============================] - 96s 266us/step - loss: 26.4589 - acc: 0.3156 - val_loss: 24.9254 - val_acc: 0.4169\n",
      "\n",
      "Epoch 00009: val_loss improved from 25.24284 to 24.92543, saving model to embedding_model.h5\n",
      "Epoch 10/150\n",
      "296960/360249 [=======================>......] - ETA: 16s - loss: 26.1718 - acc: 0.3194360249/360249 [==============================] - 96s 266us/step - loss: 26.1337 - acc: 0.3184 - val_loss: 24.6066 - val_acc: 0.3866\n",
      "\n",
      "Epoch 00010: val_loss improved from 24.92543 to 24.60657, saving model to embedding_model.h5\n",
      "Epoch 11/150\n",
      "294912/360249 [=======================>......] - ETA: 16s - loss: 25.7859 - acc: 0.3196360249/360249 [==============================] - 96s 267us/step - loss: 25.7664 - acc: 0.3216 - val_loss: 24.2864 - val_acc: 0.4241\n",
      "\n",
      "Epoch 00011: val_loss improved from 24.60657 to 24.28645, saving model to embedding_model.h5\n",
      "Epoch 12/150\n",
      "293888/360249 [=======================>......] - ETA: 17s - loss: 25.4975 - acc: 0.3393360249/360249 [==============================] - 96s 268us/step - loss: 25.4581 - acc: 0.3366 - val_loss: 24.0626 - val_acc: 0.4180\n",
      "\n",
      "Epoch 00012: val_loss improved from 24.28645 to 24.06262, saving model to embedding_model.h5\n",
      "Epoch 13/150\n",
      "293888/360249 [=======================>......] - ETA: 17s - loss: 25.0341 - acc: 0.3344360249/360249 [==============================] - 96s 267us/step - loss: 25.0169 - acc: 0.3336 - val_loss: 23.8048 - val_acc: 0.3796\n",
      "\n",
      "Epoch 00013: val_loss improved from 24.06262 to 23.80480, saving model to embedding_model.h5\n",
      "Epoch 14/150\n",
      "293888/360249 [=======================>......] - ETA: 17s - loss: 24.7926 - acc: 0.3425360249/360249 [==============================] - 96s 267us/step - loss: 24.8182 - acc: 0.3405 - val_loss: 23.8773 - val_acc: 0.4154\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 23.80480\n",
      "Epoch 15/150\n",
      "336896/360249 [===========================>..] - ETA: 6s - loss: 24.7847 - acc: 0.3585360249/360249 [==============================] - 96s 268us/step - loss: 24.7741 - acc: 0.3575 - val_loss: 23.6480 - val_acc: 0.4392\n",
      "\n",
      "Epoch 00015: val_loss improved from 23.80480 to 23.64800, saving model to embedding_model.h5\n",
      "Epoch 16/150\n",
      "305152/360249 [========================>.....] - ETA: 14s - loss: 24.5596 - acc: 0.3634360249/360249 [==============================] - 96s 267us/step - loss: 24.5455 - acc: 0.3671 - val_loss: 23.3051 - val_acc: 0.4680\n",
      "\n",
      "Epoch 00016: val_loss improved from 23.64800 to 23.30507, saving model to embedding_model.h5\n",
      "Epoch 17/150\n",
      "296960/360249 [=======================>......] - ETA: 16s - loss: 24.1913 - acc: 0.3865360249/360249 [==============================] - 96s 268us/step - loss: 24.1672 - acc: 0.3864 - val_loss: 23.0413 - val_acc: 0.4621\n",
      "\n",
      "Epoch 00017: val_loss improved from 23.30507 to 23.04126, saving model to embedding_model.h5\n",
      "Epoch 18/150\n",
      "294912/360249 [=======================>......] - ETA: 16s - loss: 23.9451 - acc: 0.3921360249/360249 [==============================] - 96s 268us/step - loss: 23.9325 - acc: 0.3915 - val_loss: 22.8989 - val_acc: 0.4663\n",
      "\n",
      "Epoch 00018: val_loss improved from 23.04126 to 22.89886, saving model to embedding_model.h5\n",
      "Epoch 19/150\n",
      "293888/360249 [=======================>......] - ETA: 17s - loss: 23.8864 - acc: 0.4087360249/360249 [==============================] - 96s 267us/step - loss: 23.8881 - acc: 0.4090 - val_loss: 22.8252 - val_acc: 0.5084\n",
      "\n",
      "Epoch 00019: val_loss improved from 22.89886 to 22.82522, saving model to embedding_model.h5\n",
      "Epoch 20/150\n",
      "293888/360249 [=======================>......] - ETA: 17s - loss: 23.8811 - acc: 0.4072360249/360249 [==============================] - 97s 268us/step - loss: 23.8690 - acc: 0.4061 - val_loss: 22.7709 - val_acc: 0.4870\n",
      "\n",
      "Epoch 00020: val_loss improved from 22.82522 to 22.77090, saving model to embedding_model.h5\n",
      "Epoch 21/150\n",
      "293888/360249 [=======================>......] - ETA: 17s - loss: 23.8452 - acc: 0.4078360249/360249 [==============================] - 96s 268us/step - loss: 23.8517 - acc: 0.4110 - val_loss: 22.8118 - val_acc: 0.4959\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 22.77090\n",
      "Epoch 22/150\n",
      "336896/360249 [===========================>..] - ETA: 6s - loss: 23.8687 - acc: 0.4152360249/360249 [==============================] - 96s 267us/step - loss: 23.8679 - acc: 0.4140 - val_loss: 22.7112 - val_acc: 0.4627\n",
      "\n",
      "Epoch 00022: val_loss improved from 22.77090 to 22.71118, saving model to embedding_model.h5\n",
      "Epoch 23/150\n",
      "305152/360249 [========================>.....] - ETA: 14s - loss: 23.8019 - acc: 0.4110360249/360249 [==============================] - 96s 268us/step - loss: 23.8006 - acc: 0.4101 - val_loss: 22.6710 - val_acc: 0.4980\n",
      "\n",
      "Epoch 00023: val_loss improved from 22.71118 to 22.67097, saving model to embedding_model.h5\n",
      "Epoch 24/150\n",
      "296960/360249 [=======================>......] - ETA: 16s - loss: 23.7504 - acc: 0.4175360249/360249 [==============================] - 97s 269us/step - loss: 23.7497 - acc: 0.4160 - val_loss: 22.6084 - val_acc: 0.5169\n",
      "\n",
      "Epoch 00024: val_loss improved from 22.67097 to 22.60836, saving model to embedding_model.h5\n",
      "Epoch 25/150\n",
      "294912/360249 [=======================>......] - ETA: 16s - loss: 23.7340 - acc: 0.4183360249/360249 [==============================] - 96s 268us/step - loss: 23.7958 - acc: 0.4182 - val_loss: 22.9922 - val_acc: 0.5176\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 22.60836\n",
      "Epoch 26/150\n",
      "336896/360249 [===========================>..] - ETA: 6s - loss: 24.0070 - acc: 0.4236360249/360249 [==============================] - 96s 268us/step - loss: 23.9814 - acc: 0.4226 - val_loss: 22.4869 - val_acc: 0.5111\n",
      "\n",
      "Epoch 00026: val_loss improved from 22.60836 to 22.48687, saving model to embedding_model.h5\n",
      "Epoch 27/150\n",
      "305152/360249 [========================>.....] - ETA: 14s - loss: 23.5521 - acc: 0.4331360249/360249 [==============================] - 97s 269us/step - loss: 23.5290 - acc: 0.4345 - val_loss: 22.4502 - val_acc: 0.5210\n",
      "\n",
      "Epoch 00027: val_loss improved from 22.48687 to 22.45017, saving model to embedding_model.h5\n",
      "Epoch 28/150\n",
      "296960/360249 [=======================>......] - ETA: 16s - loss: 23.3494 - acc: 0.4372360249/360249 [==============================] - 97s 269us/step - loss: 23.3402 - acc: 0.4360 - val_loss: 22.3916 - val_acc: 0.5231\n",
      "\n",
      "Epoch 00028: val_loss improved from 22.45017 to 22.39159, saving model to embedding_model.h5\n",
      "Epoch 29/150\n",
      "294912/360249 [=======================>......] - ETA: 16s - loss: 23.3153 - acc: 0.4360360249/360249 [==============================] - 97s 268us/step - loss: 23.2995 - acc: 0.4367 - val_loss: 22.3249 - val_acc: 0.5382\n",
      "\n",
      "Epoch 00029: val_loss improved from 22.39159 to 22.32488, saving model to embedding_model.h5\n",
      "Epoch 30/150\n",
      "293888/360249 [=======================>......] - ETA: 17s - loss: 23.3131 - acc: 0.4399360249/360249 [==============================] - 97s 268us/step - loss: 23.3026 - acc: 0.4401 - val_loss: 22.4127 - val_acc: 0.5456\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 22.32488\n",
      "Epoch 31/150\n",
      "336896/360249 [===========================>..] - ETA: 6s - loss: 23.2421 - acc: 0.4431360249/360249 [==============================] - 96s 268us/step - loss: 23.2406 - acc: 0.4435 - val_loss: 22.4365 - val_acc: 0.5532\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 22.32488\n",
      "Epoch 32/150\n",
      "350208/360249 [============================>.] - ETA: 2s - loss: 23.0508 - acc: 0.4498360249/360249 [==============================] - 98s 271us/step - loss: 23.0465 - acc: 0.4494 - val_loss: 22.3872 - val_acc: 0.5534\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 22.32488\n",
      "Epoch 33/150\n",
      "354304/360249 [============================>.] - ETA: 1s - loss: 22.9256 - acc: 0.4564360249/360249 [==============================] - 97s 270us/step - loss: 22.9251 - acc: 0.4566 - val_loss: 22.2793 - val_acc: 0.5726\n",
      "\n",
      "Epoch 00033: val_loss improved from 22.32488 to 22.27928, saving model to embedding_model.h5\n",
      "Epoch 34/150\n",
      "309248/360249 [========================>.....] - ETA: 13s - loss: 22.8610 - acc: 0.4596360249/360249 [==============================] - 97s 270us/step - loss: 22.8628 - acc: 0.4598 - val_loss: 22.2444 - val_acc: 0.5687\n",
      "\n",
      "Epoch 00034: val_loss improved from 22.27928 to 22.24439, saving model to embedding_model.h5\n",
      "Epoch 35/150\n",
      "297984/360249 [=======================>......] - ETA: 16s - loss: 22.8493 - acc: 0.4631360249/360249 [==============================] - 97s 268us/step - loss: 22.8523 - acc: 0.4637 - val_loss: 22.2303 - val_acc: 0.5732\n",
      "\n",
      "Epoch 00035: val_loss improved from 22.24439 to 22.23026, saving model to embedding_model.h5\n",
      "Epoch 36/150\n",
      "294912/360249 [=======================>......] - ETA: 16s - loss: 22.8333 - acc: 0.4701360249/360249 [==============================] - 97s 269us/step - loss: 22.8772 - acc: 0.4685 - val_loss: 22.2957 - val_acc: 0.5664\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 22.23026\n",
      "Epoch 37/150\n",
      "336896/360249 [===========================>..] - ETA: 6s - loss: 22.8755 - acc: 0.4686360249/360249 [==============================] - 98s 271us/step - loss: 22.8757 - acc: 0.4677 - val_loss: 22.3084 - val_acc: 0.5907\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 22.23026\n",
      "Epoch 38/150\n",
      "350208/360249 [============================>.] - ETA: 2s - loss: 22.8752 - acc: 0.4795360249/360249 [==============================] - 97s 268us/step - loss: 22.8815 - acc: 0.4800 - val_loss: 22.2879 - val_acc: 0.6218\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 22.23026\n",
      "Epoch 39/150\n",
      "354304/360249 [============================>.] - ETA: 1s - loss: 22.8424 - acc: 0.4905360249/360249 [==============================] - 97s 268us/step - loss: 22.8411 - acc: 0.4905 - val_loss: 22.2671 - val_acc: 0.5996\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 22.23026\n",
      "Epoch 40/150\n",
      "355328/360249 [============================>.] - ETA: 1s - loss: 22.8843 - acc: 0.4830360249/360249 [==============================] - 97s 269us/step - loss: 22.8822 - acc: 0.4829 - val_loss: 22.3156 - val_acc: 0.5789\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 22.23026\n",
      "Epoch 41/150\n",
      "355328/360249 [============================>.] - ETA: 1s - loss: 22.7895 - acc: 0.4887360249/360249 [==============================] - 97s 270us/step - loss: 22.7869 - acc: 0.4884 - val_loss: 22.3050 - val_acc: 0.5893\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 22.23026\n",
      "Epoch 42/150\n",
      "355328/360249 [============================>.] - ETA: 1s - loss: 22.8634 - acc: 0.4916360249/360249 [==============================] - 96s 267us/step - loss: 22.8642 - acc: 0.4917 - val_loss: 22.5072 - val_acc: 0.6082\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 22.23026\n",
      "Epoch 43/150\n",
      "355328/360249 [============================>.] - ETA: 1s - loss: 22.6039 - acc: 0.5027360249/360249 [==============================] - 96s 267us/step - loss: 22.6051 - acc: 0.5026 - val_loss: 22.1581 - val_acc: 0.6068\n",
      "\n",
      "Epoch 00043: val_loss improved from 22.23026 to 22.15812, saving model to embedding_model.h5\n",
      "Epoch 44/150\n",
      "310272/360249 [========================>.....] - ETA: 13s - loss: 22.4483 - acc: 0.4969360249/360249 [==============================] - 97s 270us/step - loss: 22.4313 - acc: 0.4964 - val_loss: 22.1321 - val_acc: 0.6201\n",
      "\n",
      "Epoch 00044: val_loss improved from 22.15812 to 22.13209, saving model to embedding_model.h5\n",
      "Epoch 45/150\n",
      "297984/360249 [=======================>......] - ETA: 16s - loss: 22.3382 - acc: 0.5063360249/360249 [==============================] - 97s 269us/step - loss: 22.3372 - acc: 0.5063 - val_loss: 22.1162 - val_acc: 0.6111\n",
      "\n",
      "Epoch 00045: val_loss improved from 22.13209 to 22.11620, saving model to embedding_model.h5\n",
      "Epoch 46/150\n",
      "294912/360249 [=======================>......] - ETA: 16s - loss: 22.3006 - acc: 0.5097360249/360249 [==============================] - 96s 268us/step - loss: 22.3043 - acc: 0.5099 - val_loss: 22.1009 - val_acc: 0.6182\n",
      "\n",
      "Epoch 00046: val_loss improved from 22.11620 to 22.10094, saving model to embedding_model.h5\n",
      "Epoch 47/150\n",
      "293888/360249 [=======================>......] - ETA: 17s - loss: 22.2733 - acc: 0.5084360249/360249 [==============================] - 97s 269us/step - loss: 22.2809 - acc: 0.5094 - val_loss: 22.0601 - val_acc: 0.6175\n",
      "\n",
      "Epoch 00047: val_loss improved from 22.10094 to 22.06010, saving model to embedding_model.h5\n",
      "Epoch 48/150\n",
      "293888/360249 [=======================>......] - ETA: 17s - loss: 22.3027 - acc: 0.5225360249/360249 [==============================] - 97s 270us/step - loss: 22.3288 - acc: 0.5243 - val_loss: 22.2943 - val_acc: 0.6461\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 22.06010\n",
      "Epoch 49/150\n",
      "336896/360249 [===========================>..] - ETA: 6s - loss: 22.5769 - acc: 0.5282360249/360249 [==============================] - 96s 268us/step - loss: 22.5684 - acc: 0.5279 - val_loss: 22.2249 - val_acc: 0.6283\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 22.06010\n",
      "Epoch 50/150\n",
      "350208/360249 [============================>.] - ETA: 2s - loss: 22.4387 - acc: 0.5320360249/360249 [==============================] - 97s 269us/step - loss: 22.4388 - acc: 0.5316 - val_loss: 22.2467 - val_acc: 0.6268\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 22.06010\n",
      "Epoch 51/150\n",
      "354304/360249 [============================>.] - ETA: 1s - loss: 22.3178 - acc: 0.5379360249/360249 [==============================] - 97s 269us/step - loss: 22.3146 - acc: 0.5376 - val_loss: 21.9965 - val_acc: 0.6323\n",
      "\n",
      "Epoch 00051: val_loss improved from 22.06010 to 21.99651, saving model to embedding_model.h5\n",
      "Epoch 52/150\n",
      "309248/360249 [========================>.....] - ETA: 13s - loss: 22.1787 - acc: 0.5385360249/360249 [==============================] - 97s 268us/step - loss: 22.1730 - acc: 0.5399 - val_loss: 21.9994 - val_acc: 0.6625\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 21.99651\n",
      "Epoch 53/150\n",
      "342016/360249 [===========================>..] - ETA: 4s - loss: 22.1473 - acc: 0.5483360249/360249 [==============================] - 96s 267us/step - loss: 22.1485 - acc: 0.5484 - val_loss: 21.9924 - val_acc: 0.6625\n",
      "\n",
      "Epoch 00053: val_loss improved from 21.99651 to 21.99239, saving model to embedding_model.h5\n",
      "Epoch 54/150\n",
      "306176/360249 [========================>.....] - ETA: 14s - loss: 22.1451 - acc: 0.5414360249/360249 [==============================] - 97s 269us/step - loss: 22.1469 - acc: 0.5423 - val_loss: 21.9893 - val_acc: 0.6380\n",
      "\n",
      "Epoch 00054: val_loss improved from 21.99239 to 21.98932, saving model to embedding_model.h5\n",
      "Epoch 55/150\n",
      "296960/360249 [=======================>......] - ETA: 16s - loss: 22.1447 - acc: 0.5406360249/360249 [==============================] - 97s 269us/step - loss: 22.1374 - acc: 0.5400 - val_loss: 21.9709 - val_acc: 0.6293\n",
      "\n",
      "Epoch 00055: val_loss improved from 21.98932 to 21.97086, saving model to embedding_model.h5\n",
      "Epoch 56/150\n",
      "294912/360249 [=======================>......] - ETA: 16s - loss: 22.1217 - acc: 0.5496360249/360249 [==============================] - 96s 267us/step - loss: 22.1260 - acc: 0.5486 - val_loss: 21.9690 - val_acc: 0.6519\n",
      "\n",
      "Epoch 00056: val_loss improved from 21.97086 to 21.96899, saving model to embedding_model.h5\n",
      "Epoch 57/150\n",
      "293888/360249 [=======================>......] - ETA: 17s - loss: 22.1020 - acc: 0.5447360249/360249 [==============================] - 97s 269us/step - loss: 22.1041 - acc: 0.5439 - val_loss: 21.9538 - val_acc: 0.6509\n",
      "\n",
      "Epoch 00057: val_loss improved from 21.96899 to 21.95379, saving model to embedding_model.h5\n",
      "Epoch 58/150\n",
      "293888/360249 [=======================>......] - ETA: 17s - loss: 22.1153 - acc: 0.5446360249/360249 [==============================] - 97s 270us/step - loss: 22.1411 - acc: 0.5594 - val_loss: 22.0833 - val_acc: 0.7662\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 21.95379\n",
      "Epoch 59/150\n",
      "336896/360249 [===========================>..] - ETA: 6s - loss: 22.2257 - acc: 0.6429360249/360249 [==============================] - 96s 268us/step - loss: 22.2253 - acc: 0.6408 - val_loss: 21.9885 - val_acc: 0.6694\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 21.95379\n",
      "Epoch 60/150\n",
      "350208/360249 [============================>.] - ETA: 2s - loss: 22.1170 - acc: 0.5970360249/360249 [==============================] - 96s 267us/step - loss: 22.1186 - acc: 0.5968 - val_loss: 22.0042 - val_acc: 0.6735\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 21.95379\n",
      "Epoch 61/150\n",
      "354304/360249 [============================>.] - ETA: 1s - loss: 22.0980 - acc: 0.5874360249/360249 [==============================] - 97s 269us/step - loss: 22.0965 - acc: 0.5877 - val_loss: 21.9673 - val_acc: 0.6935\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 21.95379\n",
      "Epoch 62/150\n",
      "355328/360249 [============================>.] - ETA: 1s - loss: 22.0833 - acc: 0.5847360249/360249 [==============================] - 97s 270us/step - loss: 22.0822 - acc: 0.5846 - val_loss: 21.9648 - val_acc: 0.6827\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 21.95379\n",
      "Epoch 63/150\n",
      "355328/360249 [============================>.] - ETA: 1s - loss: 22.1018 - acc: 0.5953360249/360249 [==============================] - 96s 267us/step - loss: 22.1020 - acc: 0.5953 - val_loss: 21.9640 - val_acc: 0.6906\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 21.95379\n",
      "Epoch 64/150\n",
      "355328/360249 [============================>.] - ETA: 1s - loss: 22.1575 - acc: 0.5862360249/360249 [==============================] - 97s 268us/step - loss: 22.1579 - acc: 0.5859 - val_loss: 21.9715 - val_acc: 0.6722\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 21.95379\n",
      "Epoch 65/150\n",
      "355328/360249 [============================>.] - ETA: 1s - loss: 22.0449 - acc: 0.5842360249/360249 [==============================] - 97s 269us/step - loss: 22.0444 - acc: 0.5842 - val_loss: 21.9109 - val_acc: 0.6850\n",
      "\n",
      "Epoch 00065: val_loss improved from 21.95379 to 21.91094, saving model to embedding_model.h5\n",
      "Epoch 66/150\n",
      "310272/360249 [========================>.....] - ETA: 12s - loss: 22.0176 - acc: 0.5846360249/360249 [==============================] - 97s 269us/step - loss: 22.0223 - acc: 0.5830 - val_loss: 21.9050 - val_acc: 0.6785\n",
      "\n",
      "Epoch 00066: val_loss improved from 21.91094 to 21.90501, saving model to embedding_model.h5\n",
      "Epoch 67/150\n",
      "297984/360249 [=======================>......] - ETA: 16s - loss: 22.0075 - acc: 0.5889360249/360249 [==============================] - 96s 267us/step - loss: 22.0154 - acc: 0.5876 - val_loss: 21.8927 - val_acc: 0.6702\n",
      "\n",
      "Epoch 00067: val_loss improved from 21.90501 to 21.89269, saving model to embedding_model.h5\n",
      "Epoch 68/150\n",
      "294912/360249 [=======================>......] - ETA: 17s - loss: 22.0099 - acc: 0.5871360249/360249 [==============================] - 97s 269us/step - loss: 22.0088 - acc: 0.5876 - val_loss: 21.9240 - val_acc: 0.6944\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 21.89269\n",
      "Epoch 69/150\n",
      "336896/360249 [===========================>..] - ETA: 6s - loss: 21.9907 - acc: 0.5945360249/360249 [==============================] - 97s 269us/step - loss: 21.9970 - acc: 0.5935 - val_loss: 21.9102 - val_acc: 0.6860\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 21.89269\n",
      "Epoch 70/150\n",
      "350208/360249 [============================>.] - ETA: 2s - loss: 21.9998 - acc: 0.5871360249/360249 [==============================] - 96s 267us/step - loss: 21.9972 - acc: 0.5872 - val_loss: 21.8833 - val_acc: 0.6876\n",
      "\n",
      "Epoch 00070: val_loss improved from 21.89269 to 21.88331, saving model to embedding_model.h5\n",
      "Epoch 71/150\n",
      "308224/360249 [========================>.....] - ETA: 13s - loss: 21.9831 - acc: 0.5909360249/360249 [==============================] - 97s 270us/step - loss: 21.9757 - acc: 0.5915 - val_loss: 21.9123 - val_acc: 0.6863\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 21.88331\n",
      "Epoch 72/150\n",
      "340992/360249 [===========================>..] - ETA: 5s - loss: 21.9824 - acc: 0.5865360249/360249 [==============================] - 97s 269us/step - loss: 21.9842 - acc: 0.5857 - val_loss: 21.9165 - val_acc: 0.6691\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 21.88331\n",
      "Epoch 73/150\n",
      "351232/360249 [============================>.] - ETA: 2s - loss: 21.9755 - acc: 0.5906360249/360249 [==============================] - 96s 268us/step - loss: 21.9763 - acc: 0.5903 - val_loss: 21.8907 - val_acc: 0.6838\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 21.88331\n",
      "Epoch 74/150\n",
      "354304/360249 [============================>.] - ETA: 1s - loss: 21.9643 - acc: 0.5960360249/360249 [==============================] - 96s 268us/step - loss: 21.9648 - acc: 0.5957 - val_loss: 21.8841 - val_acc: 0.6857\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 21.88331\n",
      "Epoch 75/150\n",
      "355328/360249 [============================>.] - ETA: 1s - loss: 21.9490 - acc: 0.5962360249/360249 [==============================] - 97s 269us/step - loss: 21.9494 - acc: 0.5962 - val_loss: 21.8874 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 21.88331\n",
      "Epoch 76/150\n",
      "355328/360249 [============================>.] - ETA: 1s - loss: 21.9586 - acc: 0.5941360249/360249 [==============================] - 97s 269us/step - loss: 21.9589 - acc: 0.5939 - val_loss: 21.8601 - val_acc: 0.6761\n",
      "\n",
      "Epoch 00076: val_loss improved from 21.88331 to 21.86014, saving model to embedding_model.h5\n",
      "Epoch 77/150\n",
      "310272/360249 [========================>.....] - ETA: 12s - loss: 21.9367 - acc: 0.5917360249/360249 [==============================] - 96s 267us/step - loss: 21.9397 - acc: 0.5907 - val_loss: 21.8559 - val_acc: 0.6782\n",
      "\n",
      "Epoch 00077: val_loss improved from 21.86014 to 21.85592, saving model to embedding_model.h5\n",
      "Epoch 78/150\n",
      "297984/360249 [=======================>......] - ETA: 16s - loss: 21.9345 - acc: 0.5911360249/360249 [==============================] - 97s 269us/step - loss: 21.9247 - acc: 0.5924 - val_loss: 21.7928 - val_acc: 0.6798\n",
      "\n",
      "Epoch 00078: val_loss improved from 21.85592 to 21.79275, saving model to embedding_model.h5\n",
      "Epoch 79/150\n",
      "294912/360249 [=======================>......] - ETA: 17s - loss: 21.9457 - acc: 0.5980360249/360249 [==============================] - 97s 270us/step - loss: 21.9597 - acc: 0.5973 - val_loss: 21.9280 - val_acc: 0.6981\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 21.79275\n",
      "Epoch 80/150\n",
      "336896/360249 [===========================>..] - ETA: 6s - loss: 21.9805 - acc: 0.6069360249/360249 [==============================] - 96s 267us/step - loss: 21.9840 - acc: 0.6062 - val_loss: 21.9229 - val_acc: 0.6953\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 21.79275\n",
      "Epoch 81/150\n",
      "350208/360249 [============================>.] - ETA: 2s - loss: 21.9675 - acc: 0.6011360249/360249 [==============================] - 97s 268us/step - loss: 21.9675 - acc: 0.6009 - val_loss: 21.9158 - val_acc: 0.6915\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 21.79275\n",
      "Epoch 82/150\n",
      "354304/360249 [============================>.] - ETA: 1s - loss: 21.9910 - acc: 0.5918360249/360249 [==============================] - 97s 269us/step - loss: 21.9908 - acc: 0.5918 - val_loss: 21.9717 - val_acc: 0.6858\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 21.79275\n",
      "Epoch 83/150\n",
      "355328/360249 [============================>.] - ETA: 1s - loss: 21.9424 - acc: 0.5949360249/360249 [==============================] - 96s 268us/step - loss: 21.9445 - acc: 0.5948 - val_loss: 21.8776 - val_acc: 0.6879\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 21.79275\n",
      "Epoch 84/150\n",
      "355328/360249 [============================>.] - ETA: 1s - loss: 21.8918 - acc: 0.5948360249/360249 [==============================] - 96s 267us/step - loss: 21.8922 - acc: 0.5949 - val_loss: 21.8191 - val_acc: 0.6861\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 21.79275\n",
      "Epoch 85/150\n",
      "355328/360249 [============================>.] - ETA: 1s - loss: 21.8753 - acc: 0.5927360249/360249 [==============================] - 97s 268us/step - loss: 21.8752 - acc: 0.5928 - val_loss: 21.8338 - val_acc: 0.6992\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 21.79275\n",
      "Epoch 86/150\n",
      "355328/360249 [============================>.] - ETA: 1s - loss: 21.8656 - acc: 0.5993360249/360249 [==============================] - 96s 266us/step - loss: 21.8648 - acc: 0.5993 - val_loss: 21.7860 - val_acc: 0.6882\n",
      "\n",
      "Epoch 00086: val_loss improved from 21.79275 to 21.78597, saving model to embedding_model.h5\n",
      "Epoch 87/150\n",
      "310272/360249 [========================>.....] - ETA: 12s - loss: 21.8462 - acc: 0.6021360249/360249 [==============================] - 96s 267us/step - loss: 21.8542 - acc: 0.6004 - val_loss: 21.7855 - val_acc: 0.6851\n",
      "\n",
      "Epoch 00087: val_loss improved from 21.78597 to 21.78553, saving model to embedding_model.h5\n",
      "Epoch 88/150\n",
      "297984/360249 [=======================>......] - ETA: 16s - loss: 21.8446 - acc: 0.6033360249/360249 [==============================] - 96s 267us/step - loss: 21.8549 - acc: 0.6023 - val_loss: 21.7819 - val_acc: 0.7001\n",
      "\n",
      "Epoch 00088: val_loss improved from 21.78553 to 21.78195, saving model to embedding_model.h5\n",
      "Epoch 89/150\n",
      "294912/360249 [=======================>......] - ETA: 16s - loss: 21.8388 - acc: 0.6048360249/360249 [==============================] - 97s 268us/step - loss: 21.8383 - acc: 0.6028 - val_loss: 21.8299 - val_acc: 0.7055\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 21.78195\n",
      "Epoch 90/150\n",
      "336896/360249 [===========================>..] - ETA: 6s - loss: 21.8895 - acc: 0.6002360249/360249 [==============================] - 96s 267us/step - loss: 21.8837 - acc: 0.6008 - val_loss: 21.8259 - val_acc: 0.7018\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 21.78195\n",
      "Epoch 91/150\n",
      "350208/360249 [============================>.] - ETA: 2s - loss: 21.9013 - acc: 0.5998360249/360249 [==============================] - 96s 267us/step - loss: 21.9002 - acc: 0.5997 - val_loss: 21.8281 - val_acc: 0.6857\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 21.78195\n",
      "Epoch 92/150\n",
      "354304/360249 [============================>.] - ETA: 1s - loss: 21.8829 - acc: 0.5978360249/360249 [==============================] - 96s 267us/step - loss: 21.8831 - acc: 0.5978 - val_loss: 21.8367 - val_acc: 0.6964\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 21.78195\n",
      "Epoch 93/150\n",
      "355328/360249 [============================>.] - ETA: 1s - loss: 21.8522 - acc: 0.6096360249/360249 [==============================] - 96s 267us/step - loss: 21.8530 - acc: 0.6095 - val_loss: 21.7853 - val_acc: 0.6988\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 21.78195\n",
      "Epoch 94/150\n",
      "355328/360249 [============================>.] - ETA: 1s - loss: 21.8208 - acc: 0.6122360249/360249 [==============================] - 96s 267us/step - loss: 21.8206 - acc: 0.6121 - val_loss: 21.7848 - val_acc: 0.7038\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 21.78195\n",
      "Epoch 95/150\n",
      "355328/360249 [============================>.] - ETA: 1s - loss: 21.8175 - acc: 0.6161360249/360249 [==============================] - 96s 267us/step - loss: 21.8184 - acc: 0.6159 - val_loss: 21.7908 - val_acc: 0.6911\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 21.78195\n",
      "Epoch 96/150\n",
      "355328/360249 [============================>.] - ETA: 1s - loss: 21.8375 - acc: 0.6067360249/360249 [==============================] - 96s 267us/step - loss: 21.8376 - acc: 0.6068 - val_loss: 21.7924 - val_acc: 0.7032\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 21.78195\n",
      "Epoch 97/150\n",
      "355328/360249 [============================>.] - ETA: 1s - loss: 21.8309 - acc: 0.6146360249/360249 [==============================] - 96s 268us/step - loss: 21.8304 - acc: 0.6146 - val_loss: 21.7965 - val_acc: 0.7009\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 21.78195\n",
      "Epoch 98/150\n",
      "355328/360249 [============================>.] - ETA: 1s - loss: 21.8447 - acc: 0.6103360249/360249 [==============================] - 96s 267us/step - loss: 21.8446 - acc: 0.6101 - val_loss: 21.8219 - val_acc: 0.6798\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 21.78195\n"
     ]
    }
   ],
   "source": [
    "clf = build_network(X_numeric, X_text, X_numeric_test, X_text_test, y)\n",
    "# clf.load_weights('embedding_model.h5')\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=10)\n",
    "checkpointer = ModelCheckpoint(filepath=\"embedding_model.h5\", verbose=1, save_best_only=True)\n",
    "\n",
    "history_clf = clf.fit([X_numeric, X_text], y, epochs=150, batch_size=1024, validation_split=0.1, callbacks=[early_stopping_monitor, checkpointer])\n",
    "\n",
    "clf.save_weights(\"embedding_model.h5\")\n",
    "model_json = clf.to_json()\n",
    "with open(\"embedding_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1745,
     "status": "ok",
     "timestamp": 1533398382974,
     "user": {
      "displayName": "Albert Lee",
      "photoUrl": "//lh3.googleusercontent.com/-f0u8obCCk_w/AAAAAAAAAAI/AAAAAAAAAxs/83E22s4mtyM/s50-c-k-no/photo.jpg",
      "userId": "109969660738913262150"
     },
     "user_tz": 240
    },
    "id": "eFYCkcxULI3U",
    "outputId": "85cd4264-036e-43b4-f97d-0b0d4d778063"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAFnCAYAAACLnxFFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f849b519710>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_clf.history['loss'])\n",
    "plt.plot(history_clf.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "LQTaxv8Cl4zs"
   },
   "outputs": [],
   "source": [
    "predictions = clf.predict([X_numeric_test, X_text_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "oHKwxY6QLybH"
   },
   "outputs": [],
   "source": [
    "label = ['Function',\n",
    "         'Object_Type',\n",
    "         'Operating_Status',\n",
    "         'Position_Type',\n",
    "         'Pre_K',\n",
    "         'Reporting',\n",
    "         'Sharing',\n",
    "         'Student_Type',\n",
    "         'Use']\n",
    "\n",
    "submission_columns = pd.get_dummies(data_train[label], prefix_sep='__').columns\n",
    "submission = pd.DataFrame(predictions, columns=submission_columns, index=data_test.index)\n",
    "\n",
    "submission.to_csv('embedding_model.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Wy6YXYJgVVFs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Copy of Untitled3.ipynb",
   "provenance": [
    {
     "file_id": "1q7pe5AeFmFhagdFQ2NBqbfEwRo4kl1Pp",
     "timestamp": 1533313246969
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
