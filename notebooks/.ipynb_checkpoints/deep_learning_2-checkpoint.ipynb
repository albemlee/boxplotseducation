{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Working Directory\n",
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/AlbertLee/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Load Requirements\n",
    "import pandas as pd\n",
    "import  numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import Imputer\n",
    "from scipy import sparse\n",
    "\n",
    "import zipfile\n",
    "import re, nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, concatenate, Input, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_train shape: (400277, 25)\n",
      "data_test shape: (50064, 16)\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "data_train, data_test = load_data()\n",
    "print('data_train shape:', data_train.shape)\n",
    "print('data_test shape:', data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_features shape: (450341, 16)\n"
     ]
    }
   ],
   "source": [
    "# Load Features\n",
    "data_features = load_features(data_train, data_test)\n",
    "print('data_features shape:', data_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(phrase):\n",
    "    \"\"\"\n",
    "    Return list processed_phrase: phrase tokens after processing has been completed\n",
    "    \n",
    "    param string phrase: phrase to be processed\n",
    "    \n",
    "    Required Libraries: re, nltk\n",
    "    \"\"\"\n",
    "    \n",
    "    # Case Normalization\n",
    "    processed_phrase = phrase.lower()\n",
    "    \n",
    "    # Remove Punctuations\n",
    "    processed_phrase = re.sub(r\"[^a-z0-9]\", \" \", processed_phrase)\n",
    "    \n",
    "    # Tokenize Phrase\n",
    "    processed_phrase = processed_phrase.split()\n",
    "    \n",
    "    # Remove Stopwords\n",
    "    processed_phrase = [word for word in processed_phrase if word not in stopwords.words(\"english\")]\n",
    "    \n",
    "    # Lemmatization\n",
    "#     processed_phrase = [WordNetLemmatizer().lemmatize(word) for word in processed_phrase]\n",
    "#     processed_phrase = [WordNetLemmatizer().lemmatize(word, pos='v') for word in processed_phrase] # verbs\n",
    "    \n",
    "    # Stemming\n",
    "    processed_phrase = [SnowballStemmer('english').stem(word) for word in processed_phrase]\n",
    "    \n",
    "    # Recombine list into phrase\n",
    "    processed_phrase = ' '.join(processed_phrase)\n",
    "    \n",
    "    return processed_phrase\n",
    "\n",
    "def init_prep(data_train, data_test, data_features, label=None):\n",
    "    \"\"\"\n",
    "    Return numpy array X: feature matrix for classification model fitting\n",
    "    Return numpy array y: labels matrix for classification model fitting\n",
    "    Return numpy array X_test: feature matrix of test set\n",
    "    \n",
    "    Param pandas dataframe data_train: training data (features + labels)\n",
    "    Param pandas dataframe data_test: test data (features)\n",
    "    Param pandas dataframe data_features: data in feature columns of data_train and data_test\n",
    "    \n",
    "    Required Libraries: pandas, numpy, keras\n",
    "    Required helper functions: text_processing\n",
    "    \"\"\"\n",
    "    \n",
    "    # Combined and preprocess text columns\n",
    "    data_train['combined_text'] = (data_train[data_features.columns]\n",
    "                                       .drop(columns=['FTE', 'Total'])\n",
    "                                       .fillna(\"\")\n",
    "                                       .apply(lambda x: \" \".join(x), axis=1)\n",
    "                                       .apply(lambda x: text_processing(x))\n",
    "                                  )\n",
    "    data_test['combined_text'] = (data_test[data_features.columns]\n",
    "                                       .drop(columns=['FTE', 'Total'])\n",
    "                                       .fillna(\"\")\n",
    "                                       .apply(lambda x: \" \".join(x), axis=1)\n",
    "                                       .apply(lambda x: text_processing(x))\n",
    "                                 )\n",
    "    data_features['combined_text'] = (data_features\n",
    "                                          .drop(columns=['FTE', 'Total'])\n",
    "                                          .fillna(\"\")\n",
    "                                          .apply(lambda x: \" \".join(x), axis=1)\n",
    "                                          .apply(lambda x: text_processing(x))\n",
    "                                     )\n",
    "    \n",
    "    # Vectorizer text columns in training data\n",
    "    tokenize = Tokenizer()\n",
    "    tokenize.fit_on_texts(data_features['combined_text'])\n",
    "    \n",
    "    X_text = tokenize.texts_to_matrix(data_train['combined_text'])\n",
    "    X_text_test = tokenize.texts_to_matrix(data_test['combined_text'])\n",
    "    \n",
    "    # Impute missing numerical data\n",
    "    imp_total = Imputer(strategy='median')\n",
    "    \n",
    "    imp_total.fit(data_features['Total'].values.reshape(-1, 1))\n",
    "    \n",
    "    total = imp_total.transform(data_train['Total'].values.reshape(-1, 1))\n",
    "    fte = data_train['FTE'].fillna('0').values.reshape(-1, 1)\n",
    "    \n",
    "    total_test = imp_total.transform(data_test['Total'].values.reshape(-1, 1))\n",
    "    fte_test = data_test['FTE'].fillna('0').values.reshape(-1, 1)\n",
    "    \n",
    "    # Create feature matrix\n",
    "    X_numeric = np.concatenate([total, fte], axis=1)\n",
    "    X_numeric_test = np.concatenate([total_test, fte_test], axis=1)\n",
    "    \n",
    "    # Create labels matrix\n",
    "    if label:\n",
    "        y = pd.get_dummies(data_train[label]).values.astype('float64')\n",
    "    else:\n",
    "        label = ['Function',\n",
    "                 'Object_Type',\n",
    "                 'Operating_Status',\n",
    "                 'Position_Type',\n",
    "                 'Pre_K',\n",
    "                 'Reporting',\n",
    "                 'Sharing',\n",
    "                 'Student_Type',\n",
    "                 'Use']\n",
    "        y = pd.get_dummies(data_train[label]).values.astype('float64')\n",
    "    \n",
    "    return X_numeric, X_text, X_numeric_test, X_text_test, y, tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_numeric shape: (400277, 2)\n",
      "X_numeric_test shape: (50064, 2)\n",
      "X_text shape: (400277, 3363)\n",
      "X_text_test shape: (50064, 3363)\n",
      "y shape: (400277, 104)\n"
     ]
    }
   ],
   "source": [
    "X_numeric, X_text, X_numeric_test, X_text_test, y, tokenize = init_prep(data_train, data_test, data_features, label=None)\n",
    "print('X_numeric shape:', X_numeric.shape)\n",
    "print('X_numeric_test shape:', X_numeric_test.shape)\n",
    "print('X_text shape:', X_text.shape)\n",
    "print('X_text_test shape:', X_text_test.shape)\n",
    "print('y shape:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('X_numeric.csv', X_numeric, fmt='%5s', delimiter=\",\")\n",
    "np.savetxt('X_numeric_test.csv', X_numeric_test, fmt='%5s', delimiter=\",\")\n",
    "np.savetxt('X_text.csv', X_text, fmt='%5s', delimiter=\",\")\n",
    "np.savetxt('X_text_test.csv', X_text_test, fmt='%5s', delimiter=\",\")\n",
    "np.savetxt('y.csv', y, fmt='%5s', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_numeric shape: (400277, 2)\n",
      "X_numeric_test shape: (50064, 2)\n",
      "X_text shape: (400277, 3363)\n",
      "X_text_test shape: (50064, 3363)\n",
      "y shape: (400277, 104)\n"
     ]
    }
   ],
   "source": [
    "X_numeric = pd.read_csv('X_numeric.csv', header=None).values\n",
    "X_text = pd.read_csv('X_text.csv', header=None).values\n",
    "X_numeric_test = pd.read_csv('X_numeric_test.csv', header=None).values\n",
    "X_text_test = pd.read_csv('X_text_test.csv', header=None).values\n",
    "y = pd.read_csv('y.csv', header=None).values\n",
    "\n",
    "print('X_numeric shape:', X_numeric.shape)\n",
    "print('X_numeric_test shape:', X_numeric_test.shape)\n",
    "print('X_text shape:', X_text.shape)\n",
    "print('X_text_test shape:', X_text_test.shape)\n",
    "print('y shape:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400277, 104)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = ['Function',\n",
    "         'Object_Type',\n",
    "         'Operating_Status',\n",
    "         'Position_Type',\n",
    "         'Pre_K',\n",
    "         'Reporting',\n",
    "         'Sharing',\n",
    "         'Student_Type',\n",
    "         'Use']\n",
    "y = pd.get_dummies(data_train[label]).values.astype('float64')\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(X_numeric, X_text, X_numeric_test, X_text_test, y):\n",
    "    \"\"\"\n",
    "    Return compiled keras-model model\n",
    "    \n",
    "    param numpy array X: feature matrix for classification\n",
    "    param numpy array y: labels matrix for classification\n",
    "    \n",
    "    Required Libraries: keras\n",
    "    \"\"\"\n",
    "    \n",
    "    numeric_input = Input(shape=(X_numeric.shape[1],) , name='numeric_input') \n",
    "    text_input = Input(shape=(X_text.shape[1],) , name='text_input')\n",
    "    \n",
    "    # Function\n",
    "    text_function_hidden_layer_1 = Dense(37, activation='relu')(text_input)\n",
    "    text_function_hidden_layer_1_dropout = Dropout(0.2)(text_function_hidden_layer_1)\n",
    "    text_function_hidden_layer_2 = Dense(37, activation='relu')(text_function_hidden_layer_1_dropout)\n",
    "    text_function_hidden_layer_2_dropout = Dropout(0.2)(text_function_hidden_layer_2)\n",
    "    combined_function_layer = concatenate([numeric_input, text_function_hidden_layer_2_dropout])\n",
    "    function_output_layer = Dense(37, activation='softmax')(combined_function_layer)\n",
    "    \n",
    "    # Object_Type\n",
    "    text_object_type_hidden_layer_1 = Dense(37, activation='relu')(text_input)\n",
    "    text_object_type_hidden_layer_1_dropout = Dropout(0.2)(text_object_type_hidden_layer_1)\n",
    "    text_object_type_hidden_layer_2 = Dense(20, activation='relu')(text_object_type_hidden_layer_1_dropout)\n",
    "    text_object_type_hidden_layer_2_dropout = Dropout(0.2)(text_object_type_hidden_layer_2)\n",
    "    combined_object_type_layer = concatenate([numeric_input, text_object_type_hidden_layer_2_dropout])\n",
    "    object_type_output_layer = Dense(11, activation='softmax')(combined_object_type_layer)\n",
    "    \n",
    "    # Operating_Status\n",
    "    text_operating_status_hidden_layer_1 = Dense(30, activation='relu')(text_input)\n",
    "    text_operating_status_hidden_layer_1_dropout = Dropout(0.2)(text_operating_status_hidden_layer_1)\n",
    "    text_operating_status_hidden_layer_2 = Dense(15, activation='relu')(text_operating_status_hidden_layer_1_dropout)\n",
    "    text_operating_status_hidden_layer_2_dropout = Dropout(0.2)(text_operating_status_hidden_layer_2)\n",
    "    text_operating_status_hidden_layer_3 = Dense(7, activation='relu')(text_operating_status_hidden_layer_2_dropout)\n",
    "    text_operating_status_hidden_layer_3_dropout = Dropout(0.2)(text_operating_status_hidden_layer_3)\n",
    "    combined_operating_status_layer = concatenate([numeric_input, text_operating_status_hidden_layer_3_dropout])\n",
    "    operating_status_output_layer = Dense(3, activation='softmax')(combined_operating_status_layer)\n",
    "    \n",
    "    # Position_Type\n",
    "    text_position_type_hidden_layer_1 = Dense(37, activation='relu')(text_input)\n",
    "    text_position_type_hidden_layer_1_dropout = Dropout(0.2)(text_position_type_hidden_layer_1)\n",
    "    text_position_type_hidden_layer_2 = Dense(37, activation='relu')(text_position_type_hidden_layer_1_dropout)\n",
    "    text_position_type_hidden_layer_2_dropout = Dropout(0.2)(text_position_type_hidden_layer_2)\n",
    "    combined_position_type_layer = concatenate([numeric_input, text_position_type_hidden_layer_2_dropout])\n",
    "    position_type_output_layer = Dense(25, activation='softmax')(combined_position_type_layer)\n",
    "    \n",
    "    # Pre_K\n",
    "    text_pre_k_hidden_layer_1 = Dense(30, activation='relu')(text_input)\n",
    "    text_pre_k_hidden_layer_1_dropout = Dropout(0.2)(text_pre_k_hidden_layer_1)\n",
    "    text_pre_k_hidden_layer_2 = Dense(15, activation='relu')(text_pre_k_hidden_layer_1_dropout)\n",
    "    text_pre_k_hidden_layer_2_dropout = Dropout(0.2)(text_pre_k_hidden_layer_2)\n",
    "    text_pre_k_hidden_layer_3 = Dense(7, activation='relu')(text_pre_k_hidden_layer_2_dropout)\n",
    "    text_pre_k_hidden_layer_3_dropout = Dropout(0.2)(text_pre_k_hidden_layer_3)\n",
    "    combined_pre_k_layer = concatenate([numeric_input, text_pre_k_hidden_layer_3_dropout])\n",
    "    pre_k_output_layer = Dense(3, activation='softmax')(combined_pre_k_layer)\n",
    "    \n",
    "    # Reporting\n",
    "    text_reporting_hidden_layer_1 = Dense(30, activation='relu')(text_input)\n",
    "    text_reporting_hidden_layer_1_dropout = Dropout(0.2)(text_reporting_hidden_layer_1)\n",
    "    text_reporting_hidden_layer_2 = Dense(15, activation='relu')(text_reporting_hidden_layer_1_dropout)\n",
    "    text_reporting_hidden_layer_2_dropout = Dropout(0.2)(text_reporting_hidden_layer_2)\n",
    "    text_reporting_hidden_layer_3 = Dense(7, activation='relu')(text_reporting_hidden_layer_2_dropout)\n",
    "    text_reporting_hidden_layer_3_dropout = Dropout(0.2)(text_reporting_hidden_layer_3)\n",
    "    combined_reporting_layer = concatenate([numeric_input, text_reporting_hidden_layer_3_dropout])\n",
    "    reporting_output_layer = Dense(3, activation='softmax')(combined_reporting_layer)\n",
    "    \n",
    "    # Sharing\n",
    "    text_sharing_hidden_layer_1 = Dense(30, activation='relu')(text_input)\n",
    "    text_sharing_hidden_layer_1_dropout = Dropout(0.2)(text_sharing_hidden_layer_1)\n",
    "    text_sharing_hidden_layer_2 = Dense(15, activation='relu')(text_sharing_hidden_layer_1_dropout)\n",
    "    text_sharing_hidden_layer_2_dropout = Dropout(0.2)(text_sharing_hidden_layer_2)\n",
    "    text_sharing_hidden_layer_3 = Dense(7, activation='relu')(text_sharing_hidden_layer_2_dropout)\n",
    "    text_sharing_hidden_layer_3_dropout = Dropout(0.2)(text_sharing_hidden_layer_3)\n",
    "    combined_sharing_layer = concatenate([numeric_input, text_sharing_hidden_layer_3_dropout])\n",
    "    sharing_output_layer = Dense(5, activation='softmax')(combined_sharing_layer)\n",
    "    \n",
    "    # Student_Type\n",
    "    text_student_type_hidden_layer_1 = Dense(30, activation='relu')(text_input)\n",
    "    text_student_type_hidden_layer_1_dropout = Dropout(0.2)(text_student_type_hidden_layer_1)\n",
    "    text_student_type_hidden_layer_2 = Dense(15, activation='relu')(text_student_type_hidden_layer_1_dropout)\n",
    "    text_student_type_hidden_layer_2_dropout = Dropout(0.2)(text_student_type_hidden_layer_2)\n",
    "    combined_student_type_layer = concatenate([numeric_input, text_student_type_hidden_layer_2_dropout])\n",
    "    student_type_output_layer = Dense(9, activation='softmax')(combined_student_type_layer)\n",
    "    \n",
    "    # Use\n",
    "    text_use_hidden_layer_1 = Dense(30, activation='relu')(text_input)\n",
    "    text_use_hidden_layer_1_dropout = Dropout(0.2)(text_use_hidden_layer_1)\n",
    "    text_use_hidden_layer_2 = Dense(15, activation='relu')(text_use_hidden_layer_1_dropout)\n",
    "    text_use_hidden_layer_2_dropout = Dropout(0.2)(text_use_hidden_layer_2)\n",
    "    combined_use_layer = concatenate([numeric_input, text_use_hidden_layer_2_dropout])\n",
    "    use_output_layer = Dense(8, activation='softmax')(combined_use_layer)\n",
    "    \n",
    "    # Output\n",
    "    combined_output_layer = concatenate([function_output_layer, \n",
    "                                         object_type_output_layer,\n",
    "                                         operating_status_output_layer,\n",
    "                                         position_type_output_layer,\n",
    "                                         pre_k_output_layer,\n",
    "                                         reporting_output_layer,\n",
    "                                         sharing_output_layer,\n",
    "                                         student_type_output_layer,\n",
    "                                         use_output_layer])\n",
    "    \n",
    "    model = Model(inputs=[numeric_input, text_input], outputs=[combined_output_layer])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 280193 samples, validate on 120084 samples\n",
      "Epoch 1/30\n",
      "  3072/280193 [..............................] - ETA: 1:07:39 - loss: 108.4906 - acc: 0.0641"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-479883f75a90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcheckpointer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'outputs/functional_model.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_numeric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping_monitor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpointer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'outputs/functional_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    183\u001b[0m                         \u001b[0;31m# Do not slice the training phase flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                         ins_batch = slice_arrays(\n\u001b[0;32m--> 185\u001b[0;31m                             ins[:-1], batch_ids) + [ins[-1]]\n\u001b[0m\u001b[1;32m    186\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                         \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf = build_network(X_numeric, X_text, X_numeric_test, X_text_test, y)\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "checkpointer = ModelCheckpoint(filepath='outputs/functional_model.h5', verbose=1, save_best_only=True)\n",
    "\n",
    "clf.fit([X_numeric, X_text], y, epochs=30, batch_size=1024, validation_split=0.3, callbacks=[early_stopping_monitor, checkpointer])\n",
    "\n",
    "clf.save_weights('outputs/functional_model.h5')\n",
    "model_json = clf.to_json()\n",
    "with open('outputs/functional_model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "json_file = open('outputs/functional_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "loaded_model.load_weights('outputs/functional_model.h5')\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = loaded_model.predict([X_numeric_test, X_text_test])\n",
    "\n",
    "submission_columns = pd.get_dummies(data_train[label], prefix_sep='__').columns\n",
    "submission = pd.DataFrame(predictions, columns=submission_columns, index=data_test.index)\n",
    "\n",
    "submission.to_csv('submissions/functional_model.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
