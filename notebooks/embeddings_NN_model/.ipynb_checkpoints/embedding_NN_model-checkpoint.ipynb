{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gnWCGPLQ5ePG"
   },
   "source": [
    "# Setup Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1283,
     "status": "ok",
     "timestamp": 1534602920138,
     "user": {
      "displayName": "Albert Lee",
      "photoUrl": "//lh3.googleusercontent.com/-f0u8obCCk_w/AAAAAAAAAAI/AAAAAAAAAxs/83E22s4mtyM/s50-c-k-no/photo.jpg",
      "userId": "109969660738913262150"
     },
     "user_tz": 240
    },
    "id": "Nrg9OIDy5ePJ",
    "outputId": "8bc0020b-c24c-41dc-9d9f-5916dc22e32c"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# To load data\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For text processing\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import Imputer\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# To build model\n",
    "import keras\n",
    "from keras.layers import Dense, concatenate, Input, Dropout, Embedding, Flatten\n",
    "from keras.models import Model\n",
    "\n",
    "# To train model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# To evaluate model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# To track time elapsed\n",
    "import time\n",
    "\n",
    "# To save results\n",
    "import dill\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uPLEdpRv5ePM"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YF3nUD615ePM"
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Return pandas dataframe data_train: training data (features + labels)\n",
    "    Return pandas dataframe data_test: test data (only features)\n",
    "    \n",
    "    Required Libraries: zipfile, pandas\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load zipped folder with data files\n",
    "    resource_archive = zipfile.ZipFile('resources.zip', 'r')\n",
    "\n",
    "    # Load testing data\n",
    "    data_test = pd.read_csv(resource_archive.open('TestData.csv'), \n",
    "                            dtype={\n",
    "                                'Object_Description': str, \n",
    "                                'Program_Description': str, \n",
    "                                'SubFund_Description': str, \n",
    "                                'Job_Title_Description': str, \n",
    "                                'Facility_or_Department': str,\n",
    "                                'Sub_Object_Description': str, \n",
    "                                'Location_Description': str, \n",
    "                                'FTE': float,\n",
    "                                'Function_Description': str, \n",
    "                                'Position_Extra': str, \n",
    "                                'Text_4': str, \n",
    "                                'Total': float, \n",
    "                                'Text_2': str,\n",
    "                                'Text_3': str, \n",
    "                                'Fund_Description': str, \n",
    "                                'Text_1': str\n",
    "                            },\n",
    "                            index_col=0)\n",
    "\n",
    "    # Load training data\n",
    "    data_train = pd.read_csv(resource_archive.open('TrainingData.csv'), \n",
    "                            dtype={\n",
    "                                'Object_Description': str, \n",
    "                                'Program_Description': str, \n",
    "                                'SubFund_Description': str, \n",
    "                                'Job_Title_Description': str, \n",
    "                                'Facility_or_Department': str,\n",
    "                                'Sub_Object_Description': str, \n",
    "                                'Location_Description': str, \n",
    "                                'FTE': float,\n",
    "                                'Function_Description': str, \n",
    "                                'Position_Extra': str, \n",
    "                                'Text_4': str, \n",
    "                                'Total': float, \n",
    "                                'Text_2': str,\n",
    "                                'Text_3': str, \n",
    "                                'Fund_Description': str, \n",
    "                                'Text_1': str,\n",
    "                                'Function': 'category',\n",
    "                                'Object_Type': 'category',\n",
    "                                'Operating_Status': 'category',\n",
    "                                'Position_Type': 'category',\n",
    "                                'Pre_K': 'category',\n",
    "                                'Reporting': 'category',\n",
    "                                'Sharing': 'category',\n",
    "                                'Student_Type': 'category',\n",
    "                                'Use': 'category',\n",
    "                            },\n",
    "                             index_col=0)\n",
    "    \n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3055,
     "status": "ok",
     "timestamp": 1534602928847,
     "user": {
      "displayName": "Albert Lee",
      "photoUrl": "//lh3.googleusercontent.com/-f0u8obCCk_w/AAAAAAAAAAI/AAAAAAAAAxs/83E22s4mtyM/s50-c-k-no/photo.jpg",
      "userId": "109969660738913262150"
     },
     "user_tz": 240
    },
    "id": "GVz1rkaX5ePP",
    "outputId": "14444da6-c429-4af3-ec6e-84a6701df46e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_train shape: (400277, 25)\n",
      "data_test shape: (50064, 16)\n"
     ]
    }
   ],
   "source": [
    "data_train, data_test = load_data()\n",
    "print('data_train shape:', data_train.shape)\n",
    "print('data_test shape:', data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i203UxGy5ePT"
   },
   "outputs": [],
   "source": [
    "def load_features(data_train, data_test):\n",
    "    \"\"\"\n",
    "    Return pandas dataframe data_features: data in feature columns of data_train and data_test\n",
    "    \n",
    "    Param pandas dataframe data_train: training data (features + labels)\n",
    "    Param pandas dataframe data_test: test data (only features)\n",
    "    \n",
    "    Required Libraries: pandas\n",
    "    \"\"\"\n",
    "    \n",
    "    feature_columns = data_test.columns # data_test only contains features\n",
    "    \n",
    "    data_features = pd.concat([data_train[feature_columns], data_test])\n",
    "    \n",
    "    return data_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 509,
     "status": "ok",
     "timestamp": 1534602929716,
     "user": {
      "displayName": "Albert Lee",
      "photoUrl": "//lh3.googleusercontent.com/-f0u8obCCk_w/AAAAAAAAAAI/AAAAAAAAAxs/83E22s4mtyM/s50-c-k-no/photo.jpg",
      "userId": "109969660738913262150"
     },
     "user_tz": 240
    },
    "id": "vBZekpVd5ePV",
    "outputId": "b8e53c62-9c42-4e0c-bcab-00734664fc22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_features shape: (450341, 16)\n"
     ]
    }
   ],
   "source": [
    "# Load Features\n",
    "data_features = load_features(data_train, data_test)\n",
    "print('data_features shape:', data_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ITNF9275ePY"
   },
   "source": [
    "# Prepare Data for Classification\n",
    "Run the cells below if prepped data files have not been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ID0Uo8GT5ePY"
   },
   "outputs": [],
   "source": [
    "def text_processing(phrase):\n",
    "    \"\"\"\n",
    "    Return list processed_phrase: phrase tokens after processing has been completed\n",
    "    \n",
    "    param string phrase: phrase to be processed\n",
    "    \n",
    "    Required Libraries: re, nltk\n",
    "    \"\"\"\n",
    "    \n",
    "    # Case Normalization\n",
    "    processed_phrase = phrase.lower()\n",
    "    \n",
    "    # Remove Punctuations\n",
    "    processed_phrase = re.sub(r\"[^a-z0-9-]\", \" \", processed_phrase)\n",
    "    \n",
    "    # Tokenize Phrase\n",
    "    processed_phrase = processed_phrase.split()\n",
    "    \n",
    "    # Remove Stopwords\n",
    "    processed_phrase = [word for word in processed_phrase if word not in stopwords.words(\"english\") and word != '-']\n",
    "    \n",
    "    # Lemmatization\n",
    "    processed_phrase = [WordNetLemmatizer().lemmatize(word) for word in processed_phrase]\n",
    "    \n",
    "    # Recombine list into phrase\n",
    "    processed_phrase = ' '.join(processed_phrase)\n",
    "    \n",
    "    return processed_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2930,
     "status": "ok",
     "timestamp": 1534253366310,
     "user": {
      "displayName": "Albert Lee",
      "photoUrl": "//lh3.googleusercontent.com/-f0u8obCCk_w/AAAAAAAAAAI/AAAAAAAAAxs/83E22s4mtyM/s50-c-k-no/photo.jpg",
      "userId": "109969660738913262150"
     },
     "user_tz": 240
    },
    "id": "_ZapkVld5ePb",
    "outputId": "fc9c6bff-fd23-48d5-b998-698917a8d7a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "past patiently waitin passionately smashin every expectation every action act creation laughin face casualty sorrow first time thinkin past tomorrow\n",
      "2.5214293003082275\n"
     ]
    }
   ],
   "source": [
    "# test text_processing function (with quote from Hamilton: The Musical)\n",
    "start_time = time.time()\n",
    "print(text_processing(\n",
    "    \"I’m past patiently waitin’. I’m passionately smashin’ every expectation. \" + \n",
    "    \"Every action’s an act of creation! \" +\n",
    "    \"I’m laughin’ in the face of casualties and sorrow. \" +\n",
    "    \"For the first time, I’m thinkin’ past tomorrow\"))\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZFQgJuJ_5ePd"
   },
   "outputs": [],
   "source": [
    "def init_prep(data_train, data_test, data_features, label=None):\n",
    "    \"\"\"\n",
    "    Return numpy array X_numeric: numerical feature matrix of test set\n",
    "    Return numpy array X_text: text feature matrix for classification model fitting\n",
    "    Return numpy array X_numeric_test: numerical feature matrix of test set\n",
    "    Return numpy array X_text_test: text feature matrix for classification model fitting\n",
    "    Return numpy array y: labels matrix for classification model fitting\n",
    "    Return keras.Tokenizer() tokenize: contains word to token mapping\n",
    "    \n",
    "    Param pandas dataframe data_train: training data (features + labels)\n",
    "    Param pandas dataframe data_test: test data (features)\n",
    "    Param pandas dataframe data_features: data in feature columns of data_train and data_test\n",
    "    \n",
    "    Required Libraries: pandas, numpy, keras\n",
    "    Required helper functions: text_processing\n",
    "    \"\"\"\n",
    "    \n",
    "    # Combined and preprocess text columns\n",
    "    data_train['combined_text'] = (data_train[data_features.columns]\n",
    "                                       .drop(columns=['FTE', 'Total'])\n",
    "                                       .fillna(\"\")\n",
    "                                       .apply(lambda x: \" \".join(x), axis=1)\n",
    "                                       .apply(lambda x: text_processing(x))\n",
    "                                  )\n",
    "    data_test['combined_text'] = (data_test[data_features.columns]\n",
    "                                       .drop(columns=['FTE', 'Total'])\n",
    "                                       .fillna(\"\")\n",
    "                                       .apply(lambda x: \" \".join(x), axis=1)\n",
    "                                       .apply(lambda x: text_processing(x))\n",
    "                                 )\n",
    "    data_features['combined_text'] = (data_features\n",
    "                                          .drop(columns=['FTE', 'Total'])\n",
    "                                          .fillna(\"\")\n",
    "                                          .apply(lambda x: \" \".join(x), axis=1)\n",
    "                                          .apply(lambda x: text_processing(x))\n",
    "                                     )\n",
    "    \n",
    "    # Vectorizer text columns in training data\n",
    "    tokenize = Tokenizer()\n",
    "    tokenize.fit_on_texts(data_features['combined_text'])\n",
    "    \n",
    "    X_text = tokenize.texts_to_sequences(data_train['combined_text'])\n",
    "    X_text_test = tokenize.texts_to_sequences(data_test['combined_text'])\n",
    "    \n",
    "    X_text = pad_sequences(X_text, padding='post', maxlen=50, truncating='post')\n",
    "    X_text_test = pad_sequences(X_text_test, padding='post', maxlen=50, truncating='post')\n",
    "    \n",
    "    # Impute missing numerical data\n",
    "    imp_total = Imputer(strategy='median')\n",
    "    imp_total.fit(data_features['Total'].values.reshape(-1, 1))\n",
    "\n",
    "    total_not_missing = pd.isnull(data_train['Total']).astype(int).values.reshape(-1, 1)\n",
    "    fte_not_missing = pd.isnull(data_train['FTE']).astype(int).values.reshape(-1, 1)\n",
    "    total = imp_total.transform(data_train['Total'].values.reshape(-1, 1))\n",
    "    fte = data_train['FTE'].fillna('0').values.reshape(-1, 1)\n",
    "\n",
    "    total_not_missing_test = pd.isnull(data_test['Total']).astype(int).values.reshape(-1, 1)\n",
    "    fte_not_missing_test = pd.isnull(data_test['FTE']).astype(int).values.reshape(-1, 1)\n",
    "    total_test = imp_total.transform(data_test['Total'].values.reshape(-1, 1))\n",
    "    fte_test = data_test['FTE'].fillna('0').values.reshape(-1, 1)\n",
    "    \n",
    "    # Create feature matrix\n",
    "    X_numeric = np.concatenate([total, total_not_missing, fte, fte_not_missing], axis=1)\n",
    "    X_numeric_test = np.concatenate([total_test, total_not_missing_test, fte_test, fte_not_missing_test], axis=1)\n",
    "    \n",
    "    # Create labels matrix\n",
    "    if label:\n",
    "        y = pd.get_dummies(data_train[label]).values.astype('float64')\n",
    "    else:\n",
    "        label = ['Function',\n",
    "                 'Object_Type',\n",
    "                 'Operating_Status',\n",
    "                 'Position_Type',\n",
    "                 'Pre_K',\n",
    "                 'Reporting',\n",
    "                 'Sharing',\n",
    "                 'Student_Type',\n",
    "                 'Use']\n",
    "        y = pd.get_dummies(data_train[label]).values.astype('float64')\n",
    "    \n",
    "    return X_numeric, X_text, X_numeric_test, X_text_test, y, tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6IISwk-c5ePf"
   },
   "outputs": [],
   "source": [
    "X_numeric, X_text, X_test_numeric, X_test_text, y, tokenize = init_prep(data_train, data_test, data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YeEwXBk75ePi"
   },
   "outputs": [],
   "source": [
    "# Save X_test_text and X_test_numeric\n",
    "np.savez('X_test_text.npz', X_test_text)\n",
    "np.savez('X_test_numeric.npz', X_test_numeric)\n",
    "\n",
    "# Pickle tokenize\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenize, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g3wYrZqv5ePp"
   },
   "outputs": [],
   "source": [
    "def prep_for_classification(X_numeric, X_text, y, validation_size=50064):\n",
    "    \"\"\"\n",
    "    Split training data into training and validation sets\n",
    "    \n",
    "    Return pandas dataframe X_train_numeric: training data (features)\n",
    "    Return pandas dataframe X_train_text: training data (features)\n",
    "    Return pandas dataframe X_val_numeric: validation data (features)\n",
    "    Return pandas dataframe X_val_text: validation data (features)\n",
    "    Return pandas dataframe y_train: training data (labels)\n",
    "    Return pandas dataframe y_val: validation data (labels)\n",
    "    \n",
    "    param numpy array X_numeric: numerical feature matrix of test set\n",
    "    param numpy array X_text: text feature matrix for classification model fitting\n",
    "    param numpy array X_numeric_test: numerical feature matrix of test set\n",
    "    param numpy array X_text_test: text feature matrix for classification model fitting\n",
    "    param numpy array y: labels matrix for classification model fitting\n",
    "    \n",
    "    Required Libraries: pandas, sklearn.model_selection\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split into training and development sets\n",
    "    X_train_numeric, X_val_numeric, X_train_text, X_val_text, y_train, y_val = train_test_split(X_numeric, X_text, y, test_size=validation_size, random_state=93)\n",
    "    \n",
    "    return X_train_numeric, X_val_numeric, X_train_text, X_val_text, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "drbaQuel5ePr",
    "outputId": "09b43a88-46f1-4b6e-8bd6-4be33d7986d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_numeric (350213, 4)\n",
      "X_train_text (350213, 50)\n",
      "y_train (350213, 104)\n",
      "X_val_numeric (50064, 4)\n",
      "X_val_text (50064, 50)\n",
      "y_val (50064, 104)\n"
     ]
    }
   ],
   "source": [
    "X_train_numeric, X_val_numeric, X_train_text, X_val_text, y_train, y_val = prep_for_classification(X_numeric, X_text, y)\n",
    "print('X_train_numeric', X_train_numeric.shape)\n",
    "print('X_train_text', X_train_text.shape)\n",
    "print('y_train', y_train.shape)\n",
    "print('X_val_numeric', X_val_numeric.shape)\n",
    "print('X_val_text', X_val_text.shape)\n",
    "print('y_val', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZhVYZEmx5ePv"
   },
   "outputs": [],
   "source": [
    "# Save prepped data\n",
    "y_train = sparse.csr_matrix(y_train)\n",
    "y_val = sparse.csr_matrix(y_val)\n",
    "\n",
    "np.savez('X_train_text.npz', X_train_text)\n",
    "np.savez('X_val_text.npz', X_val_text)\n",
    "np.savez('X_train_numeric.npz', X_train_numeric)\n",
    "np.savez('X_val_numeric.npz', X_val_numeric)\n",
    "sparse.save_npz('y_train.npz', y_train)\n",
    "sparse.save_npz('y_val.npz', y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Prepped Data\n",
    "Run these cells below if prepped data files are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 273716,
     "status": "ok",
     "timestamp": 1534603218335,
     "user": {
      "displayName": "Albert Lee",
      "photoUrl": "//lh3.googleusercontent.com/-f0u8obCCk_w/AAAAAAAAAAI/AAAAAAAAAxs/83E22s4mtyM/s50-c-k-no/photo.jpg",
      "userId": "109969660738913262150"
     },
     "user_tz": 240
    },
    "id": "hGz1Dw0S5ePx",
    "outputId": "9f602412-3daa-4472-b64e-72b68a19b8a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_numeric (350213, 4)\n",
      "X_train_text (350213, 50)\n",
      "y_train (350213, 104)\n",
      "X_val_numeric (50064, 4)\n",
      "X_val_text (50064, 50)\n",
      "y_val (50064, 104)\n",
      "X_test_numeric (50064, 4)\n",
      "X_test_text (50064, 50)\n"
     ]
    }
   ],
   "source": [
    "# Load prepped data\n",
    "X_train_text = np.load('X_train_text.npz')['arr_0']\n",
    "X_val_text = np.load('X_val_text.npz')['arr_0']\n",
    "X_train_numeric = np.load('X_train_numeric.npz')['arr_0']\n",
    "X_val_numeric = np.load('X_val_numeric.npz')['arr_0']\n",
    "y_train = sparse.load_npz('y_train.npz')\n",
    "y_val = sparse.load_npz('y_val.npz')\n",
    "X_test_numeric = np.load('X_test_numeric.npz')['arr_0']\n",
    "X_test_text = np.load('X_test_text.npz')['arr_0']\n",
    "\n",
    "print('X_train_numeric', X_train_numeric.shape)\n",
    "print('X_train_text', X_train_text.shape)\n",
    "print('y_train', y_train.shape)\n",
    "print('X_val_numeric', X_val_numeric.shape)\n",
    "print('X_val_text', X_val_text.shape)\n",
    "print('y_val', y_val.shape)\n",
    "print('X_test_numeric', X_test_numeric.shape)\n",
    "print('X_test_text', X_test_text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UfppW7oi5eP0"
   },
   "source": [
    "# Build Model\n",
    "Run cells below if model has not been fitted yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GtnM3_nE5eP0"
   },
   "outputs": [],
   "source": [
    "def build_network(X_numeric=X_train_numeric, X_text=X_train_text, y=y_train):\n",
    "    \"\"\"\n",
    "    Return compiled keras-model model\n",
    "    \n",
    "    param numpy array X: feature matrix for classification\n",
    "    param numpy array y: labels matrix for classification\n",
    "    \n",
    "    Required Libraries: keras\n",
    "    \"\"\"\n",
    "    \n",
    "    dropout_value = 0.5\n",
    "    embedding_vector_length = 8\n",
    "    \n",
    "    numeric_input = Input(shape=(X_numeric.shape[1],) , name='numeric_input') \n",
    "    text_input = Input(shape=(X_text.shape[1],) , name='text_input')\n",
    "    \n",
    "    # Function\n",
    "    word_embedding_function = Embedding(input_dim=3804, output_dim=embedding_vector_length, mask_zero=False, input_length=50)(text_input)\n",
    "    word_embedding_function = Flatten()(word_embedding_function)\n",
    "    text_function_hidden_layer = Dense(256, activation='relu')(word_embedding_function)\n",
    "    text_function_hidden_layer = Dropout(dropout_value)(text_function_hidden_layer)\n",
    "    text_function_hidden_layer = Dense(128, activation='relu')(text_function_hidden_layer)\n",
    "    text_function_hidden_layer = Dropout(dropout_value)(text_function_hidden_layer)\n",
    "    text_function_hidden_layer = Dense(64, activation='relu')(text_function_hidden_layer)\n",
    "    text_function_hidden_layer = Dropout(dropout_value)(text_function_hidden_layer)\n",
    "    numeric_function_hidden_layer = Dense(4, activation='relu')(numeric_input)\n",
    "    numeric_function_hidden_layer = Dropout(dropout_value)(numeric_function_hidden_layer)\n",
    "    combined_function_layer = concatenate([numeric_function_hidden_layer, text_function_hidden_layer])\n",
    "    function_output_layer = Dense(37, activation='softmax')(combined_function_layer)\n",
    "    \n",
    "    # Object_Type\n",
    "    word_embedding_object_type = Embedding(input_dim=3804, output_dim=embedding_vector_length, mask_zero=False, input_length=50)(text_input)\n",
    "    word_embedding_object_type = Flatten()(word_embedding_object_type)\n",
    "    text_object_type_hidden_layer = Dense(256, activation='relu')(word_embedding_object_type)\n",
    "    text_object_type_hidden_layer = Dropout(dropout_value)(text_object_type_hidden_layer)\n",
    "    text_object_type_hidden_layer = Dense(128, activation='relu')(text_object_type_hidden_layer)\n",
    "    text_object_type_hidden_layer = Dropout(dropout_value)(text_object_type_hidden_layer)\n",
    "    text_object_type_hidden_layer = Dense(64, activation='relu')(text_object_type_hidden_layer)\n",
    "    text_object_type_hidden_layer = Dropout(dropout_value)(text_object_type_hidden_layer)\n",
    "    text_object_type_hidden_layer = Dense(32, activation='relu')(text_object_type_hidden_layer)\n",
    "    text_object_type_hidden_layer = Dropout(dropout_value)(text_object_type_hidden_layer)\n",
    "    text_object_type_hidden_layer = Dense(16, activation='relu')(text_object_type_hidden_layer)\n",
    "    text_object_type_hidden_layer = Dropout(dropout_value)(text_object_type_hidden_layer)\n",
    "    numeric_object_type_hidden_layer = Dense(4, activation='relu')(numeric_input)\n",
    "    numeric_object_type_hidden_layer = Dropout(dropout_value)(numeric_object_type_hidden_layer)\n",
    "    combined_object_type_layer = concatenate([numeric_object_type_hidden_layer, text_object_type_hidden_layer])\n",
    "    object_type_output_layer = Dense(11, activation='softmax')(combined_object_type_layer)\n",
    "    \n",
    "    # Operating_Status\n",
    "    word_embedding_operating_status = Embedding(input_dim=3804, output_dim=embedding_vector_length, mask_zero=False, input_length=50)(text_input)\n",
    "    word_embedding_operating_status = Flatten()(word_embedding_operating_status)\n",
    "    text_operating_status_hidden_layer = Dense(256, activation='relu')(word_embedding_operating_status)\n",
    "    text_operating_status_hidden_layer = Dropout(dropout_value)(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dense(128, activation='relu')(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dropout(dropout_value)(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dense(64, activation='relu')(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dropout(dropout_value)(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dense(32, activation='relu')(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dropout(dropout_value)(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dense(16, activation='relu')(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dropout(dropout_value)(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dense(8, activation='relu')(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dropout(dropout_value)(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dense(4, activation='relu')(text_operating_status_hidden_layer)\n",
    "    text_operating_status_hidden_layer = Dropout(dropout_value)(text_operating_status_hidden_layer)\n",
    "    numeric_operating_status_hidden_layer = Dense(4, activation='relu')(numeric_input)\n",
    "    numeric_operating_status_hidden_layer = Dropout(dropout_value)(numeric_operating_status_hidden_layer)\n",
    "    combined_operating_status_layer = concatenate([numeric_operating_status_hidden_layer, text_operating_status_hidden_layer])\n",
    "    operating_status_output_layer = Dense(3, activation='softmax')(combined_operating_status_layer)\n",
    "    \n",
    "    # Position_Type\n",
    "    word_embedding_position_type = Embedding(input_dim=3804, output_dim=embedding_vector_length, mask_zero=False, input_length=50)(text_input)\n",
    "    word_embedding_position_type = Flatten()(word_embedding_position_type)\n",
    "    text_position_type_hidden_layer = Dense(256, activation='relu')(word_embedding_position_type)\n",
    "    text_position_type_hidden_layer = Dropout(dropout_value)(text_position_type_hidden_layer)\n",
    "    text_position_type_hidden_layer = Dense(128, activation='relu')(text_position_type_hidden_layer)\n",
    "    text_position_type_hidden_layer = Dropout(dropout_value)(text_position_type_hidden_layer)\n",
    "    text_position_type_hidden_layer = Dense(64, activation='relu')(text_position_type_hidden_layer)\n",
    "    text_position_type_hidden_layer = Dropout(dropout_value)(text_position_type_hidden_layer)\n",
    "    text_position_type_hidden_layer = Dense(32, activation='relu')(text_position_type_hidden_layer)\n",
    "    text_position_type_hidden_layer = Dropout(dropout_value)(text_position_type_hidden_layer)\n",
    "    numeric_position_type_hidden_layer = Dense(4, activation='relu')(numeric_input)\n",
    "    numeric_position_type_hidden_layer = Dropout(dropout_value)(numeric_position_type_hidden_layer)\n",
    "    combined_position_type_layer = concatenate([numeric_position_type_hidden_layer, text_position_type_hidden_layer])\n",
    "    position_type_output_layer = Dense(25, activation='softmax')(combined_position_type_layer)\n",
    "    \n",
    "    # Pre_K\n",
    "    word_embedding_pre_k = Embedding(input_dim=3804, output_dim=embedding_vector_length, mask_zero=False, input_length=50)(text_input)\n",
    "    word_embedding_pre_k = Flatten()(word_embedding_pre_k)\n",
    "    text_pre_k_hidden_layer = Dense(256, activation='relu')(word_embedding_pre_k)\n",
    "    text_pre_k_hidden_layer = Dropout(dropout_value)(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dense(128, activation='relu')(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dropout(dropout_value)(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dense(64, activation='relu')(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dropout(dropout_value)(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dense(32, activation='relu')(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dropout(dropout_value)(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dense(16, activation='relu')(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dropout(dropout_value)(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dense(8, activation='relu')(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dropout(dropout_value)(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dense(4, activation='relu')(text_pre_k_hidden_layer)\n",
    "    text_pre_k_hidden_layer = Dropout(dropout_value)(text_pre_k_hidden_layer)\n",
    "    numeric_pre_k_hidden_layer = Dense(4, activation='relu')(numeric_input)\n",
    "    numeric_pre_k_hidden_layer = Dropout(dropout_value)(numeric_pre_k_hidden_layer)\n",
    "    combined_pre_k_layer = concatenate([numeric_pre_k_hidden_layer, text_pre_k_hidden_layer])\n",
    "    pre_k_output_layer = Dense(3, activation='softmax')(combined_pre_k_layer)\n",
    "    \n",
    "    # Reporting\n",
    "    word_embedding_reporting = Embedding(input_dim=3804, output_dim=embedding_vector_length, mask_zero=False, input_length=50)(text_input)\n",
    "    word_embedding_reporting = Flatten()(word_embedding_reporting)\n",
    "    text_reporting_hidden_layer = Dense(256, activation='relu')(word_embedding_reporting)\n",
    "    text_reporting_hidden_layer = Dropout(dropout_value)(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dense(128, activation='relu')(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dropout(dropout_value)(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dense(64, activation='relu')(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dropout(dropout_value)(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dense(32, activation='relu')(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dropout(dropout_value)(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dense(16, activation='relu')(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dropout(dropout_value)(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dense(8, activation='relu')(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dropout(dropout_value)(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dense(4, activation='relu')(text_reporting_hidden_layer)\n",
    "    text_reporting_hidden_layer = Dropout(dropout_value)(text_reporting_hidden_layer)\n",
    "    numeric_reporting_hidden_layer = Dense(4, activation='relu')(numeric_input)\n",
    "    numeric_reporting_hidden_layer = Dropout(dropout_value)(numeric_reporting_hidden_layer)\n",
    "    combined_reporting_layer = concatenate([numeric_reporting_hidden_layer, text_reporting_hidden_layer])\n",
    "    reporting_output_layer = Dense(3, activation='softmax')(combined_reporting_layer)\n",
    "    \n",
    "    # Sharing\n",
    "    word_embedding_sharing = Embedding(input_dim=3804, output_dim=embedding_vector_length, mask_zero=False, input_length=50)(text_input)\n",
    "    word_embedding_sharing = Flatten()(word_embedding_sharing)\n",
    "    text_sharing_hidden_layer = Dense(256, activation='relu')(word_embedding_sharing)\n",
    "    text_sharing_hidden_layer = Dropout(dropout_value)(text_sharing_hidden_layer)\n",
    "    text_sharing_hidden_layer = Dense(128, activation='relu')(text_sharing_hidden_layer)\n",
    "    text_sharing_hidden_layer = Dropout(dropout_value)(text_sharing_hidden_layer)\n",
    "    text_sharing_hidden_layer = Dense(64, activation='relu')(text_sharing_hidden_layer)\n",
    "    text_sharing_hidden_layer = Dropout(dropout_value)(text_sharing_hidden_layer)\n",
    "    text_sharing_hidden_layer = Dense(32, activation='relu')(text_sharing_hidden_layer)\n",
    "    text_sharing_hidden_layer = Dropout(dropout_value)(text_sharing_hidden_layer)\n",
    "    text_sharing_hidden_layer = Dense(16, activation='relu')(text_sharing_hidden_layer)\n",
    "    text_sharing_hidden_layer = Dropout(dropout_value)(text_sharing_hidden_layer)\n",
    "    text_sharing_hidden_layer = Dense(8, activation='relu')(text_sharing_hidden_layer)\n",
    "    text_sharing_hidden_layer = Dropout(dropout_value)(text_sharing_hidden_layer)\n",
    "    numeric_sharing_hidden_layer = Dense(4, activation='relu')(numeric_input)\n",
    "    numeric_sharing_hidden_layer = Dropout(dropout_value)(numeric_sharing_hidden_layer)\n",
    "    combined_sharing_layer = concatenate([numeric_sharing_hidden_layer, text_sharing_hidden_layer])\n",
    "    sharing_output_layer = Dense(5, activation='softmax')(combined_sharing_layer)\n",
    "    \n",
    "    # Student_Type\n",
    "    word_embedding_student_type = Embedding(input_dim=3804, output_dim=embedding_vector_length, mask_zero=False, input_length=50)(text_input)\n",
    "    word_embedding_student_type = Flatten()(word_embedding_student_type)\n",
    "    text_student_type_hidden_layer = Dense(256, activation='relu')(word_embedding_student_type)\n",
    "    text_student_type_hidden_layer = Dropout(dropout_value)(text_student_type_hidden_layer)\n",
    "    text_student_type_hidden_layer = Dense(128, activation='relu')(text_student_type_hidden_layer)\n",
    "    text_student_type_hidden_layer = Dropout(dropout_value)(text_student_type_hidden_layer)\n",
    "    text_student_type_hidden_layer = Dense(64, activation='relu')(text_student_type_hidden_layer)\n",
    "    text_student_type_hidden_layer = Dropout(dropout_value)(text_student_type_hidden_layer)\n",
    "    text_student_type_hidden_layer = Dense(32, activation='relu')(text_student_type_hidden_layer)\n",
    "    text_student_type_hidden_layer = Dropout(dropout_value)(text_student_type_hidden_layer)\n",
    "    text_student_type_hidden_layer = Dense(16, activation='relu')(text_student_type_hidden_layer)\n",
    "    text_student_type_hidden_layer = Dropout(dropout_value)(text_student_type_hidden_layer)\n",
    "    numeric_student_type_hidden_layer = Dense(4, activation='relu')(numeric_input)\n",
    "    numeric_student_type_hidden_layer = Dropout(dropout_value)(numeric_student_type_hidden_layer)\n",
    "    combined_student_type_layer = concatenate([numeric_student_type_hidden_layer, text_student_type_hidden_layer])\n",
    "    student_type_output_layer = Dense(9, activation='softmax')(combined_student_type_layer)\n",
    "    \n",
    "    # Use\n",
    "    word_embedding_use = Embedding(input_dim=3804, output_dim=embedding_vector_length, mask_zero=False, input_length=50)(text_input)\n",
    "    word_embedding_use = Flatten()(word_embedding_use)\n",
    "    text_use_hidden_layer = Dense(256, activation='relu')(word_embedding_use)\n",
    "    text_use_hidden_layer = Dropout(dropout_value)(text_use_hidden_layer)\n",
    "    text_use_hidden_layer = Dense(128, activation='relu')(text_use_hidden_layer)\n",
    "    text_use_hidden_layer = Dropout(dropout_value)(text_use_hidden_layer)\n",
    "    text_use_hidden_layer = Dense(64, activation='relu')(text_use_hidden_layer)\n",
    "    text_use_hidden_layer = Dropout(dropout_value)(text_use_hidden_layer)\n",
    "    text_use_hidden_layer = Dense(32, activation='relu')(text_use_hidden_layer)\n",
    "    text_use_hidden_layer = Dropout(dropout_value)(text_use_hidden_layer)\n",
    "    text_use_hidden_layer = Dense(16, activation='relu')(text_use_hidden_layer)\n",
    "    text_use_hidden_layer = Dropout(dropout_value)(text_use_hidden_layer)\n",
    "    text_use_hidden_layer = Dense(8, activation='relu')(text_use_hidden_layer)\n",
    "    text_use_hidden_layer = Dropout(dropout_value)(text_use_hidden_layer)\n",
    "    numeric_use_hidden_layer = Dense(4, activation='relu')(numeric_input)\n",
    "    numeric_use_hidden_layer = Dropout(dropout_value)(numeric_use_hidden_layer)\n",
    "    combined_use_layer = concatenate([numeric_use_hidden_layer, text_use_hidden_layer])\n",
    "    use_output_layer = Dense(8, activation='softmax')(combined_use_layer)\n",
    "    \n",
    "    # Output\n",
    "    combined_output_layer = concatenate([function_output_layer, \n",
    "                                         object_type_output_layer,\n",
    "                                         operating_status_output_layer,\n",
    "                                         position_type_output_layer,\n",
    "                                         pre_k_output_layer,\n",
    "                                         reporting_output_layer,\n",
    "                                         sharing_output_layer,\n",
    "                                         student_type_output_layer,\n",
    "                                         use_output_layer])\n",
    "    \n",
    "    model = Model(inputs=[numeric_input, text_input], outputs=[combined_output_layer])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vkm6VVkF5eP2"
   },
   "outputs": [],
   "source": [
    "embedding_NN = build_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aednKcEsfRMN"
   },
   "outputs": [],
   "source": [
    "model_json = embedding_NN.to_json()\n",
    "with open('embedding_NN_model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "INIHpqTn5eP4"
   },
   "source": [
    "# Train model and generate predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3230
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1040976,
     "status": "ok",
     "timestamp": 1534604424837,
     "user": {
      "displayName": "Albert Lee",
      "photoUrl": "//lh3.googleusercontent.com/-f0u8obCCk_w/AAAAAAAAAAI/AAAAAAAAAxs/83E22s4mtyM/s50-c-k-no/photo.jpg",
      "userId": "109969660738913262150"
     },
     "user_tz": 240
    },
    "id": "h2fWentL1If6",
    "outputId": "b3dea245-9a21-4f9b-88b9-466a55148fc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 315191 samples, validate on 35022 samples\n",
      "Epoch 1/100\n",
      "315191/315191 [==============================] - 26s 82us/step - loss: 81.9879 - acc: 0.0525 - val_loss: 73.8681 - val_acc: 0.2342\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 73.86807, saving model to drive/embedding_NN_model.h5\n",
      "Epoch 2/100\n",
      "315191/315191 [==============================] - 16s 51us/step - loss: 53.5323 - acc: 0.3060 - val_loss: 44.8194 - val_acc: 0.4468\n",
      "\n",
      "Epoch 00002: val_loss improved from 73.86807 to 44.81941, saving model to drive/embedding_NN_model.h5\n",
      "Epoch 3/100\n",
      "315191/315191 [==============================] - 16s 51us/step - loss: 41.9823 - acc: 0.3717 - val_loss: 37.6485 - val_acc: 0.4161\n",
      "\n",
      "Epoch 00003: val_loss improved from 44.81941 to 37.64848, saving model to drive/embedding_NN_model.h5\n",
      "Epoch 4/100\n",
      "315191/315191 [==============================] - 15s 47us/step - loss: 36.4844 - acc: 0.3684 - val_loss: 32.7594 - val_acc: 0.3949\n",
      "\n",
      "Epoch 00004: val_loss improved from 37.64848 to 32.75936, saving model to drive/embedding_NN_model.h5\n",
      "Epoch 5/100\n",
      "315191/315191 [==============================] - 16s 49us/step - loss: 34.2388 - acc: 0.3468 - val_loss: 31.5351 - val_acc: 0.3944\n",
      "\n",
      "Epoch 00005: val_loss improved from 32.75936 to 31.53510, saving model to drive/embedding_NN_model.h5\n",
      "Epoch 6/100\n",
      "315191/315191 [==============================] - 16s 49us/step - loss: 33.3654 - acc: 0.3292 - val_loss: 31.5283 - val_acc: 0.3440\n",
      "\n",
      "Epoch 00006: val_loss improved from 31.53510 to 31.52826, saving model to drive/embedding_NN_model.h5\n",
      "Epoch 7/100\n",
      "315191/315191 [==============================] - 15s 48us/step - loss: 32.4583 - acc: 0.2931 - val_loss: 29.8722 - val_acc: 0.2905\n",
      "\n",
      "Epoch 00007: val_loss improved from 31.52826 to 29.87224, saving model to drive/embedding_NN_model.h5\n",
      "Epoch 8/100\n",
      "315191/315191 [==============================] - 15s 49us/step - loss: 31.7738 - acc: 0.2777 - val_loss: 29.3859 - val_acc: 0.2929\n",
      "\n",
      "Epoch 00008: val_loss improved from 29.87224 to 29.38592, saving model to drive/embedding_NN_model.h5\n",
      "Epoch 9/100\n",
      "315191/315191 [==============================] - 15s 47us/step - loss: 30.9794 - acc: 0.2770 - val_loss: 28.9763 - val_acc: 0.2992\n",
      "\n",
      "Epoch 00009: val_loss improved from 29.38592 to 28.97626, saving model to drive/embedding_NN_model.h5\n",
      "Epoch 10/100\n",
      "315191/315191 [==============================] - 15s 46us/step - loss: 30.3040 - acc: 0.2697 - val_loss: 27.6621 - val_acc: 0.2739\n",
      "\n",
      "Epoch 00010: val_loss improved from 28.97626 to 27.66214, saving model to drive/embedding_NN_model.h5\n",
      "Epoch 11/100\n",
      "315191/315191 [==============================] - 16s 50us/step - loss: 29.8590 - acc: 0.2642 - val_loss: 27.7558 - val_acc: 0.3068\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 27.66214\n",
      "Epoch 12/100\n",
      "315191/315191 [==============================] - 16s 51us/step - loss: 29.3704 - acc: 0.2593 - val_loss: 27.5714 - val_acc: 0.2722\n",
      "\n",
      "Epoch 00012: val_loss improved from 27.66214 to 27.57142, saving model to drive/embedding_NN_model.h5\n",
      "Epoch 13/100\n",
      "315191/315191 [==============================] - 16s 50us/step - loss: 29.0888 - acc: 0.2536 - val_loss: 27.2097 - val_acc: 0.2705\n",
      "\n",
      "Epoch 00013: val_loss improved from 27.57142 to 27.20975, saving model to drive/embedding_NN_model.h5\n",
      "Epoch 14/100\n",
      "315191/315191 [==============================] - 16s 51us/step - loss: 28.6844 - acc: 0.2525 - val_loss: 26.1124 - val_acc: 0.2648\n",
      "\n",
      "Epoch 00014: val_loss improved from 27.20975 to 26.11235, saving model to drive/embedding_NN_model.h5\n",
      "Epoch 15/100\n",
      "315191/315191 [==============================] - 16s 50us/step - loss: 28.4977 - acc: 0.2510 - val_loss: 26.1854 - val_acc: 0.2724\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 26.11235\n",
      "Epoch 16/100\n",
      "315191/315191 [==============================] - 15s 49us/step - loss: 28.3407 - acc: 0.2561 - val_loss: 26.3893 - val_acc: 0.2669\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 26.11235\n",
      "Epoch 17/100\n",
      "315191/315191 [==============================] - 16s 50us/step - loss: 28.0440 - acc: 0.2493 - val_loss: 26.0057 - val_acc: 0.2784\n",
      "\n",
      "Epoch 00017: val_loss improved from 26.11235 to 26.00570, saving model to drive/embedding_NN_model.h5\n",
      "Epoch 18/100\n",
      "315191/315191 [==============================] - 16s 51us/step - loss: 27.8188 - acc: 0.2500 - val_loss: 26.1941 - val_acc: 0.2790\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 26.00570\n",
      "Epoch 19/100\n",
      "315191/315191 [==============================] - 15s 49us/step - loss: 27.6865 - acc: 0.2473 - val_loss: 25.6984 - val_acc: 0.2782\n",
      "\n",
      "Epoch 00019: val_loss improved from 26.00570 to 25.69841, saving model to drive/embedding_NN_model.h5\n",
      "Epoch 20/100\n",
      "315191/315191 [==============================] - 14s 45us/step - loss: 27.2984 - acc: 0.2515 - val_loss: 25.8295 - val_acc: 0.2859\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 25.69841\n",
      "Epoch 21/100\n",
      "315191/315191 [==============================] - 14s 45us/step - loss: 26.8731 - acc: 0.2552 - val_loss: 25.6898 - val_acc: 0.2925\n",
      "\n",
      "Epoch 00021: val_loss improved from 25.69841 to 25.68985, saving model to drive/embedding_NN_model.h5\n",
      "Epoch 22/100\n",
      "315191/315191 [==============================] - 14s 45us/step - loss: 26.6185 - acc: 0.2563 - val_loss: 25.4262 - val_acc: 0.2986\n",
      "\n",
      "Epoch 00022: val_loss improved from 25.68985 to 25.42619, saving model to drive/embedding_NN_model.h5\n",
      "Epoch 23/100\n",
      "315191/315191 [==============================] - 14s 44us/step - loss: 26.4321 - acc: 0.2549 - val_loss: 25.2044 - val_acc: 0.3012\n",
      "\n",
      "Epoch 00023: val_loss improved from 25.42619 to 25.20445, saving model to drive/embedding_NN_model.h5\n",
      "Epoch 24/100\n",
      "315191/315191 [==============================] - 14s 46us/step - loss: 26.2201 - acc: 0.2488 - val_loss: 24.8277 - val_acc: 0.3020\n",
      "\n",
      "Epoch 00024: val_loss improved from 25.20445 to 24.82774, saving model to drive/embedding_NN_model.h5\n",
      "Epoch 25/100\n",
      "315191/315191 [==============================] - 14s 45us/step - loss: 26.0259 - acc: 0.2551 - val_loss: 24.9861 - val_acc: 0.3048\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 24.82774\n",
      "Epoch 26/100\n",
      "315191/315191 [==============================] - 14s 44us/step - loss: 26.0417 - acc: 0.2596 - val_loss: 24.8812 - val_acc: 0.2840\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 24.82774\n",
      "Epoch 27/100\n",
      "315191/315191 [==============================] - 14s 44us/step - loss: 25.8494 - acc: 0.2557 - val_loss: 24.2490 - val_acc: 0.2890\n",
      "\n",
      "Epoch 00027: val_loss improved from 24.82774 to 24.24901, saving model to drive/embedding_NN_model.h5\n",
      "Epoch 28/100\n",
      "315191/315191 [==============================] - 14s 45us/step - loss: 25.7719 - acc: 0.2582 - val_loss: 24.4787 - val_acc: 0.3078\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 24.24901\n",
      "Epoch 29/100\n",
      "315191/315191 [==============================] - 15s 46us/step - loss: 25.7294 - acc: 0.2572 - val_loss: 24.4909 - val_acc: 0.2988\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 24.24901\n",
      "Epoch 30/100\n",
      "315191/315191 [==============================] - 14s 44us/step - loss: 25.6334 - acc: 0.2587 - val_loss: 24.3734 - val_acc: 0.3058\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 24.24901\n",
      "Epoch 31/100\n",
      "315191/315191 [==============================] - 14s 45us/step - loss: 25.4835 - acc: 0.2651 - val_loss: 24.3358 - val_acc: 0.3062\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 24.24901\n",
      "Epoch 32/100\n",
      "315191/315191 [==============================] - 14s 45us/step - loss: 25.4097 - acc: 0.2611 - val_loss: 24.5425 - val_acc: 0.3123\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 24.24901\n",
      "Epoch 33/100\n",
      "315191/315191 [==============================] - 14s 46us/step - loss: 25.3165 - acc: 0.2623 - val_loss: 24.2646 - val_acc: 0.3067\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 24.24901\n",
      "Epoch 34/100\n",
      "315191/315191 [==============================] - 16s 51us/step - loss: 25.3362 - acc: 0.2699 - val_loss: 24.5516 - val_acc: 0.3191\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 24.24901\n",
      "Epoch 35/100\n",
      "315191/315191 [==============================] - 16s 52us/step - loss: 25.3831 - acc: 0.2690 - val_loss: 24.5160 - val_acc: 0.3156\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 24.24901\n",
      "Epoch 36/100\n",
      "315191/315191 [==============================] - 16s 52us/step - loss: 25.2591 - acc: 0.2724 - val_loss: 24.4128 - val_acc: 0.3211\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 24.24901\n",
      "Epoch 37/100\n",
      "315191/315191 [==============================] - 16s 52us/step - loss: 25.1995 - acc: 0.2712 - val_loss: 23.8783 - val_acc: 0.3219\n",
      "\n",
      "Epoch 00037: val_loss improved from 24.24901 to 23.87827, saving model to drive/embedding_NN_model.h5\n",
      "Epoch 38/100\n",
      "315191/315191 [==============================] - 15s 49us/step - loss: 25.0374 - acc: 0.2750 - val_loss: 24.1088 - val_acc: 0.3315\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 23.87827\n",
      "Epoch 39/100\n",
      "315191/315191 [==============================] - 16s 49us/step - loss: 25.0982 - acc: 0.2740 - val_loss: 24.0983 - val_acc: 0.3218\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 23.87827\n",
      "Epoch 40/100\n",
      "315191/315191 [==============================] - 15s 47us/step - loss: 25.0009 - acc: 0.2759 - val_loss: 24.0801 - val_acc: 0.3136\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 23.87827\n",
      "Epoch 41/100\n",
      "315191/315191 [==============================] - 15s 48us/step - loss: 25.0053 - acc: 0.2760 - val_loss: 24.0903 - val_acc: 0.3281\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 23.87827\n",
      "Epoch 42/100\n",
      "315191/315191 [==============================] - 16s 49us/step - loss: 24.9579 - acc: 0.2839 - val_loss: 23.9595 - val_acc: 0.3309\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 23.87827\n",
      "Epoch 43/100\n",
      "315191/315191 [==============================] - 16s 50us/step - loss: 24.9064 - acc: 0.2857 - val_loss: 24.0569 - val_acc: 0.3251\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 23.87827\n",
      "Epoch 44/100\n",
      "315191/315191 [==============================] - 15s 49us/step - loss: 25.0176 - acc: 0.2889 - val_loss: 23.8921 - val_acc: 0.3233\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 23.87827\n",
      "Epoch 45/100\n",
      "315191/315191 [==============================] - 15s 49us/step - loss: 24.9433 - acc: 0.2901 - val_loss: 24.1671 - val_acc: 0.3652\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 23.87827\n",
      "Epoch 46/100\n",
      "315191/315191 [==============================] - 16s 50us/step - loss: 24.8912 - acc: 0.2887 - val_loss: 23.9316 - val_acc: 0.3291\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 23.87827\n",
      "Epoch 47/100\n",
      "315191/315191 [==============================] - 16s 49us/step - loss: 25.0795 - acc: 0.2913 - val_loss: 23.8959 - val_acc: 0.3261\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 23.87827\n"
     ]
    }
   ],
   "source": [
    "early_stopping_monitor = EarlyStopping(patience=10)\n",
    "checkpointer = ModelCheckpoint(filepath=\"embedding_NN_model.h5\", verbose=1, save_best_only=True)\n",
    "\n",
    "history_embedding_NN = embedding_NN.fit([X_train_numeric, X_train_text], y_train, epochs=100, batch_size=2048, validation_split=0.1, callbacks=[early_stopping_monitor, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 511,
     "status": "ok",
     "timestamp": 1534604425467,
     "user": {
      "displayName": "Albert Lee",
      "photoUrl": "//lh3.googleusercontent.com/-f0u8obCCk_w/AAAAAAAAAAI/AAAAAAAAAxs/83E22s4mtyM/s50-c-k-no/photo.jpg",
      "userId": "109969660738913262150"
     },
     "user_tz": 240
    },
    "id": "Aqfq_Q9VDf5B",
    "outputId": "ccda960c-81ac-4c97-9989-881988d489a7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAFnCAYAAACLnxFFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb9e2048d0>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_embedding_NN.history['loss'])\n",
    "plt.plot(history_embedding_NN.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pre-trained model\n",
    "Run cell below if model has already been trained, and json and h5 files are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MzyY-6uQNQCT"
   },
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "with open('embedding_NN_model.json', 'r') as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "\n",
    "embedding_NN = model_from_json(loaded_model_json)\n",
    "embedding_NN.load_weights(\"embedding_NN_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create predictions on test and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AlXYzPrd5eP4"
   },
   "outputs": [],
   "source": [
    "label_class_counts = {\n",
    "    'Function': [0, 37], \n",
    "    'Object_Type': [37, 48], \n",
    "    'Operating_Status': [48, 51], \n",
    "    'Position_Type': [51, 76], \n",
    "    'Pre_K': [76, 79], \n",
    "    'Reporting': [79, 82], \n",
    "    'Sharing': [82, 87], \n",
    "    'Student_Type': [87, 96], \n",
    "    'Use': [96, 104]\n",
    "}\n",
    "\n",
    "scores = {}\n",
    "val_predictions = embedding_NN.predict([X_val_numeric, X_val_text])\n",
    "\n",
    "# Calculate validation logloss for each label\n",
    "for label, indices in label_class_counts.items():\n",
    "    # Get values for specific label\n",
    "    start_idx = indices[0]\n",
    "    end_idx = indices[1]\n",
    "    y_val_label = y_val[:, start_idx:end_idx]\n",
    "    val_predictions_label = val_predictions[:, start_idx:end_idx]\n",
    "    \n",
    "    \n",
    "    # Get logloss score of mode\n",
    "    scores[label] = log_loss(y_val_label, val_predictions_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c6d9EwoxGX3z"
   },
   "outputs": [],
   "source": [
    "# Generate predictions for test set\n",
    "predictions = embedding_NN.predict([X_test_numeric, X_test_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_-pJMVIr5eP7"
   },
   "source": [
    "# Save score and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ngyDlM8A5eP7"
   },
   "outputs": [],
   "source": [
    "# Save predictions for test set\n",
    "label = ['Function',\n",
    "         'Object_Type',\n",
    "         'Operating_Status',\n",
    "         'Position_Type',\n",
    "         'Pre_K',\n",
    "         'Reporting',\n",
    "         'Sharing',\n",
    "         'Student_Type',\n",
    "         'Use']\n",
    "y = pd.get_dummies(data_train[label]).astype('float64')\n",
    "\n",
    "submission = pd.DataFrame(predictions, index=data_test.index, columns=y.columns)\n",
    "submission.to_csv('embedding_NN_model.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 281,
     "status": "ok",
     "timestamp": 1534605069543,
     "user": {
      "displayName": "Albert Lee",
      "photoUrl": "//lh3.googleusercontent.com/-f0u8obCCk_w/AAAAAAAAAAI/AAAAAAAAAxs/83E22s4mtyM/s50-c-k-no/photo.jpg",
      "userId": "109969660738913262150"
     },
     "user_tz": 240
    },
    "id": "uRHjIso15eP8",
    "outputId": "808f842f-daa8-4ce1-c6f5-37a84fd440f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Function': 0.26831351530628883,\n",
       " 'Object_Type': 2.6317392260097945,\n",
       " 'Operating_Status': 0.2097481580680103,\n",
       " 'Position_Type': 0.3932253361166463,\n",
       " 'Pre_K': 0.1892173416770578,\n",
       " 'Reporting': 0.14214244610230797,\n",
       " 'Sharing': 0.563543908566012,\n",
       " 'Student_Type': 0.22320706113364697,\n",
       " 'Use': 1.15466418557569}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 326,
     "status": "ok",
     "timestamp": 1534605069942,
     "user": {
      "displayName": "Albert Lee",
      "photoUrl": "//lh3.googleusercontent.com/-f0u8obCCk_w/AAAAAAAAAAI/AAAAAAAAAxs/83E22s4mtyM/s50-c-k-no/photo.jpg",
      "userId": "109969660738913262150"
     },
     "user_tz": 240
    },
    "id": "yQd8s_q85eP_",
    "outputId": "8d86a07c-2592-4679-ecd2-f8b497d3bdcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.654825683905677"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([score for score in scores.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DQxW3L5o5eQC"
   },
   "outputs": [],
   "source": [
    "# Save scores\n",
    "with open('embedding_NN_score.json', 'w') as file:\n",
    "     file.write(json.dumps(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gASvI8D9lcD-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "embedding_NN_model.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
