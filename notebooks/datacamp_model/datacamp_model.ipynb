{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# To load data\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "# To build model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer, Imputer, MaxAbsScaler\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# To evaluate model\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# To track time elapsed\n",
    "import time\n",
    "\n",
    "# To save results\n",
    "import dill\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://github.com/drivendataorg/box-plots-sklearn/blob/master/src/features/SparseInteractions.py\n",
    "# Use SparseInteractions with sparse matrices\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class SparseInteractions(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, degree=2, feature_name_separator=\"_\"):\n",
    "        self.degree = degree\n",
    "        self.feature_name_separator = feature_name_separator\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if not sparse.isspmatrix_csc(X):\n",
    "            X = sparse.csc_matrix(X)\n",
    "\n",
    "        if hasattr(X, \"columns\"):\n",
    "            self.orig_col_names = X.columns\n",
    "        else:\n",
    "            self.orig_col_names = np.array([str(i) for i in range(X.shape[1])])\n",
    "\n",
    "        spi = self._create_sparse_interactions(X)\n",
    "        return spi\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self.feature_names\n",
    "\n",
    "    def _create_sparse_interactions(self, X):\n",
    "        out_mat = []\n",
    "        self.feature_names = self.orig_col_names.tolist()\n",
    "\n",
    "        for sub_degree in range(2, self.degree + 1):\n",
    "            for col_ixs in combinations(range(X.shape[1]), sub_degree):\n",
    "                # add name for new column\n",
    "                name = self.feature_name_separator.join(self.orig_col_names[list(col_ixs)])\n",
    "                self.feature_names.append(name)\n",
    "\n",
    "                # get column multiplications value\n",
    "                out = X[:, col_ixs[0]]\n",
    "                for j in col_ixs[1:]:\n",
    "                    out = out.multiply(X[:, j])\n",
    "\n",
    "                out_mat.append(out)\n",
    "\n",
    "        return sparse.hstack([X] + out_mat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Return pandas dataframe data_train: training data (features + labels)\n",
    "    Return pandas dataframe data_test: test data (only features)\n",
    "    \n",
    "    Required Libraries: zipfile, pandas\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load zipped folder with data files\n",
    "    resource_archive = zipfile.ZipFile('resources.zip', 'r')\n",
    "\n",
    "    # Load testing data\n",
    "    data_test = pd.read_csv(resource_archive.open('TestData.csv'), \n",
    "                            dtype={\n",
    "                                'Object_Description': str, \n",
    "                                'Program_Description': str, \n",
    "                                'SubFund_Description': str, \n",
    "                                'Job_Title_Description': str, \n",
    "                                'Facility_or_Department': str,\n",
    "                                'Sub_Object_Description': str, \n",
    "                                'Location_Description': str, \n",
    "                                'FTE': float,\n",
    "                                'Function_Description': str, \n",
    "                                'Position_Extra': str, \n",
    "                                'Text_4': str, \n",
    "                                'Total': float, \n",
    "                                'Text_2': str,\n",
    "                                'Text_3': str, \n",
    "                                'Fund_Description': str, \n",
    "                                'Text_1': str\n",
    "                            },\n",
    "                            index_col=0)\n",
    "\n",
    "    # Load training data\n",
    "    data_train = pd.read_csv(resource_archive.open('TrainingData.csv'), \n",
    "                            dtype={\n",
    "                                'Object_Description': str, \n",
    "                                'Program_Description': str, \n",
    "                                'SubFund_Description': str, \n",
    "                                'Job_Title_Description': str, \n",
    "                                'Facility_or_Department': str,\n",
    "                                'Sub_Object_Description': str, \n",
    "                                'Location_Description': str, \n",
    "                                'FTE': float,\n",
    "                                'Function_Description': str, \n",
    "                                'Position_Extra': str, \n",
    "                                'Text_4': str, \n",
    "                                'Total': float, \n",
    "                                'Text_2': str,\n",
    "                                'Text_3': str, \n",
    "                                'Fund_Description': str, \n",
    "                                'Text_1': str,\n",
    "                                'Function': 'category',\n",
    "                                'Object_Type': 'category',\n",
    "                                'Operating_Status': 'category',\n",
    "                                'Position_Type': 'category',\n",
    "                                'Pre_K': 'category',\n",
    "                                'Reporting': 'category',\n",
    "                                'Sharing': 'category',\n",
    "                                'Student_Type': 'category',\n",
    "                                'Use': 'category',\n",
    "                            },\n",
    "                             index_col=0)\n",
    "    \n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_train shape: (400277, 25)\n",
      "data_test shape: (50064, 16)\n"
     ]
    }
   ],
   "source": [
    "data_train, data_test = load_data()\n",
    "print('data_train shape:', data_train.shape)\n",
    "print('data_test shape:', data_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_for_classification(data_train, data_test, validation_size=50064):\n",
    "    \"\"\"\n",
    "    Split training data into training and validation sets\n",
    "    Split data into features and labels\n",
    "    \n",
    "    Return pandas dataframe X_train: training data (features)\n",
    "    Return pandas dataframe X_val: validation data (features)\n",
    "    Return pandas dataframe X_test: test data (only features)\n",
    "    Return pandas dataframe y_train: training data (labels)\n",
    "    Return pandas dataframe y_val: validation data (labels)\n",
    "    \n",
    "    param pandas dataframe data_train: training data (features + labels)\n",
    "    param pandas dataframe data_test: test data (features)\n",
    "    param numerical validation_size: size of validation set\n",
    "    \n",
    "    Required Libraries: pandas, sklearn.model_selection\n",
    "    \"\"\"\n",
    "    \n",
    "    # List features and labels\n",
    "    features = [\n",
    "        'FTE', \n",
    "        'Facility_or_Department', \n",
    "        'Function_Description', \n",
    "        'Fund_Description', \n",
    "        'Job_Title_Description', \n",
    "        'Location_Description', \n",
    "        'Object_Description', \n",
    "        'Position_Extra', \n",
    "        'Program_Description', \n",
    "        'SubFund_Description', \n",
    "        'Sub_Object_Description', \n",
    "        'Text_1', \n",
    "        'Text_2', \n",
    "        'Text_3', \n",
    "        'Text_4', \n",
    "        'Total']\n",
    "\n",
    "    labels = [\n",
    "        'Function', \n",
    "        'Object_Type', \n",
    "        'Operating_Status', \n",
    "        'Position_Type', \n",
    "        'Pre_K', \n",
    "        'Reporting', \n",
    "        'Sharing', \n",
    "        'Student_Type', \n",
    "        'Use']\n",
    "    \n",
    "    # Separate features (X) and labels (y)\n",
    "    X = data_train[features]\n",
    "    X_test = data_test[features]\n",
    "    y = pd.get_dummies(data_train[labels], prefix_sep='__')\n",
    "    \n",
    "    # Split into training and development sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=validation_size, random_state=93)\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val = prep_for_classification(data_train, data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \"\"\"\n",
    "    Return sklearn pipeline clf_pipeline: \n",
    "    \n",
    "        \n",
    "    Required Libraries:\n",
    "    pandas\n",
    "    sklearn.preprocessing\n",
    "    sklearn.pipeline\n",
    "    sklearn.feature_extraction.text\n",
    "    sklearn.feature_selection\n",
    "    sklearn.multiclass\n",
    "    sklearn.linear_model\n",
    "    \"\"\"\n",
    "    # Numeric feature preprocessing\n",
    "    select_numeric_features = FunctionTransformer(lambda x: x[['FTE', 'Total']], validate=False)\n",
    "    numeric_preprocess_pipeline = Pipeline([\n",
    "        ('selector', select_numeric_features),\n",
    "        ('handle_missing_values', Imputer())\n",
    "    ])\n",
    "\n",
    "    # Text feature preprocessing\n",
    "    def combine_text_columns(df_train):\n",
    "        return df_train.drop(columns=['FTE', 'Total']).fillna(\"\").apply(lambda x: \" \".join(x), axis=1)\n",
    "\n",
    "    prepare_text_features = FunctionTransformer(lambda x: combine_text_columns(x), validate=False)\n",
    "    text_preprocess_pipeline = Pipeline([\n",
    "        ('combine_text', prepare_text_features),\n",
    "        ('vectorize', HashingVectorizer(token_pattern='[A-Za-z0-9]+(?=\\\\s+)',\n",
    "                                        non_negative=True,\n",
    "                                        norm=None,\n",
    "                                        binary=False, \n",
    "                                        ngram_range=(1, 2))),\n",
    "        ('dim_red', SelectKBest(chi2, 300))\n",
    "    ])\n",
    "\n",
    "    # Combine numeric and text feature preprocessing\n",
    "    preprocess_pipeline = FeatureUnion(transformer_list = [\n",
    "        ('numeric_preprocess', numeric_preprocess_pipeline),\n",
    "        ('text_preprocess', text_preprocess_pipeline)\n",
    "    ])\n",
    "\n",
    "    # Build model\n",
    "    clf_pipeline = Pipeline([\n",
    "        ('preprocess', preprocess_pipeline),\n",
    "        ('feature_interactions', SparseInteractions(degree=2)),\n",
    "        ('scale', MaxAbsScaler()), \n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "    \n",
    "    return clf_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacamp_model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model on each label and generate predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completing model for Function ...\n",
      "Model for Function trained... time elapsed 690.8775169849396\n",
      "Predictions for Function generated... time elapsed 755.75226521492\n",
      "Completing model for Object_Type ...\n",
      "Model for Object_Type trained... time elapsed 928.5800788402557\n",
      "Predictions for Object_Type generated... time elapsed 999.149539232254\n",
      "Completing model for Operating_Status ...\n",
      "Model for Operating_Status trained... time elapsed 222.13770008087158\n",
      "Predictions for Operating_Status generated... time elapsed 294.9519290924072\n",
      "Completing model for Position_Type ...\n",
      "Model for Position_Type trained... time elapsed 606.0556590557098\n",
      "Predictions for Position_Type generated... time elapsed 687.3139142990112\n",
      "Completing model for Pre_K ...\n",
      "Model for Pre_K trained... time elapsed 561.3521637916565\n",
      "Predictions for Pre_K generated... time elapsed 625.9415218830109\n",
      "Completing model for Reporting ...\n",
      "Model for Reporting trained... time elapsed 366.9106650352478\n",
      "Predictions for Reporting generated... time elapsed 433.5815749168396\n",
      "Completing model for Sharing ...\n",
      "Model for Sharing trained... time elapsed 487.18365001678467\n",
      "Predictions for Sharing generated... time elapsed 544.4538197517395\n",
      "Completing model for Student_Type ...\n",
      "Model for Student_Type trained... time elapsed 629.547858953476\n",
      "Predictions for Student_Type generated... time elapsed 697.0471351146698\n",
      "Completing model for Use ...\n",
      "Model for Use trained... time elapsed 696.596241235733\n",
      "Predictions for Use generated... time elapsed 764.2819709777832\n"
     ]
    }
   ],
   "source": [
    "label_class_counts = {\n",
    "    'Function': [0, 37], \n",
    "    'Object_Type': [37, 48], \n",
    "    'Operating_Status': [48, 51], \n",
    "    'Position_Type': [51, 76], \n",
    "    'Pre_K': [76, 79], \n",
    "    'Reporting': [79, 82], \n",
    "    'Sharing': [82, 87], \n",
    "    'Student_Type': [87, 96], \n",
    "    'Use': [96, 104]\n",
    "}\n",
    "\n",
    "scores = {}\n",
    "predictions = np.zeros(shape=(X_test.shape[0], 104))\n",
    "\n",
    "for label, indices in label_class_counts.items():\n",
    "    start_time = time.time()\n",
    "    print(\"Completing model for\", label, \"...\")\n",
    "    # Get values for specific label\n",
    "    start_idx = indices[0]\n",
    "    end_idx = indices[1]\n",
    "    y_train_label = y_train.values[:, start_idx:end_idx]\n",
    "    y_val_label = y_val.values[:, start_idx:end_idx]\n",
    "    \n",
    "    # Train model\n",
    "    datacamp_model.fit(X_train, y_train_label)\n",
    "    print(\"Model for\", label, \"trained...\", \"time elapsed\", time.time() - start_time)\n",
    "    \n",
    "    # Get logloss score of mode\n",
    "    validation_predictions = datacamp_model.predict_proba(X_val)\n",
    "    scores[label] = log_loss(y_val_label, validation_predictions)\n",
    "    \n",
    "    # Generate predictions on test set\n",
    "    predictions[:, start_idx:end_idx] = datacamp_model.predict_proba(X_test)\n",
    "    print(\"Predictions for\", label, \"generated...\", \"time elapsed\", time.time() - start_time)\n",
    "    \n",
    "    # Save model\n",
    "    with open('datacamp_' + label + '.pkl', 'wb') as fid:\n",
    "        dill.dump(datacamp_model, fid) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save score and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions for test set\n",
    "submission = pd.DataFrame(predictions, index=X_test.index, columns=y_train.columns)\n",
    "submission.to_csv('datacamp_model.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Function': 0.5444172744127479,\n",
       " 'Object_Type': 0.12562586497405154,\n",
       " 'Operating_Status': 0.06815703676624803,\n",
       " 'Position_Type': 0.32486287814541626,\n",
       " 'Pre_K': 0.0456704909623151,\n",
       " 'Reporting': 0.11312726754702154,\n",
       " 'Sharing': 0.16476345311088206,\n",
       " 'Student_Type': 0.1604987108904824,\n",
       " 'Use': 0.2176736817876597}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19608851762186935"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([score for score in scores.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datacamp_score.json', 'w') as file:\n",
    "     file.write(json.dumps(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_class_counts = {\n",
    "    'Function': [0, 37], \n",
    "    'Object_Type': [37, 48], \n",
    "    'Operating_Status': [48, 51], \n",
    "    'Position_Type': [51, 76], \n",
    "    'Pre_K': [76, 79], \n",
    "    'Reporting': [79, 82], \n",
    "    'Sharing': [82, 87], \n",
    "    'Student_Type': [87, 96], \n",
    "    'Use': [96, 104]\n",
    "}\n",
    "\n",
    "for label, indices in label_class_counts.items():\n",
    "    files.download('datacamp_' + label + '.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
